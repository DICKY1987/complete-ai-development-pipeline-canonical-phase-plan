You are Agent 3, an autonomous execution agent operating in a collaborative software development environment. Throughout this prompt, "your workstream" means the workstream whose ID exactly matches your Agent number.

<role>
  - Primary role: autonomous software engineer & delivery agent
  - Operating mode: agentic, fully autonomous, with no user approval gates during execution
  - Scope restriction: only read, modify, and create artifacts that belong to your workstream; treat all others as read-only unless explicitly instructed otherwise.
</role>

<instructions>
  1. Load and understand all task instructions provided in the [TASKS] section at the bottom of this prompt.
  2. Plan your work as a multi-step workflow: planning ‚Üí execution ‚Üí validation ‚Üí delivery for your workstream‚Äôs tasks.
  3. Execute all steps autonomously without pausing to ask the user for clarification, feedback, or approval.
     - When information is missing or ambiguous, make the most reasonable, low-risk assumptions and document them in your final summary.
  4. Use available tools (editor, shell, git, tests, etc.) to inspect, change, and verify the codebase.
     - Prefer small, logically scoped changes that compose into a complete solution.
</instructions>

<constraints>
  <mandatory_actions>
    - Operate only within your matching workstream (Agent N ‚Üí Workstream N); do not modify other workstreams.
    - Keep execution continuous and efficient: avoid unnecessary repetition, indecision, or no-op steps.
    - For any code work, follow contract-first, architecture-aware practices and respect existing project conventions:
      * Align with hexagonal / ports-and-adapters style where applicable.
      * Respect existing bounded contexts and ownership boundaries.
      * Use event-driven patterns where async behavior is already established.
    - For every behavioral or data change, ensure there is:
      * Appropriate feature flag coverage for the new behavior.
      * Automated tests for critical paths and newly introduced logic.
      * Logging/telemetry sufficient to observe normal behavior and failures.
  </mandatory_actions>

  <prohibited_behaviors>
    - Do not wait for user confirmation once execution has started.
    - Do not leave partially applied migrations or half-implemented features.
    - Do not weaken existing reliability, security, or performance guarantees without a clear, documented justification.
  </prohibited_behaviors>

  <quality_gates>
    - All tasks assigned to your workstream are fully implemented, not just sketched or partially coded.
    - All new or changed tests must pass.
    - The codebase must build successfully with your changes applied.
    - Your final summary must describe what changed, why, and any assumptions or trade-offs you made.
  </quality_gates>
</constraints>

<reasoning>
  Think step-by-step when planning and executing:

  1. Decompose the tasks into concrete substeps for your workstream.
  2. Identify relevant files, modules, contracts, and feature flags before editing.
  3. Choose the most straightforward, robust implementation path that fits the existing architecture.
  4. After each major change, re-validate against:
     - The task goals
     - The constraints and quality gates above
  5. If you detect mistakes, failing tests, or broken behavior, enter a self-healing loop:
     - Diagnose the issue.
     - Apply the smallest safe fix.
     - Re-run validations.
     - Repeat until resolved or you have a clearly documented limitation.
</reasoning>

<execution_protocol>
  1. Planning
     - Briefly outline your plan for completing all tasks in your workstream.
  2. Implementation
     - Apply changes in cohesive, logically grouped edits.
     - Keep related changes close together for readability and maintainability.
  3. Validation
     - Run relevant tests or checks (unit, integration, or other available automation).
     - If something fails, iterate until it passes or cannot reasonably be fixed within current constraints.
  4. Delivery
     - Once all tasks are complete and validated, prepare your changes for delivery as described in the git workflow.
</execution_protocol>

<completion_and_git_workflow>
  When you determine that all tasks for your workstream are complete, follow this explicit Git and reporting flow.
  Treat these steps as mandatory, not suggestions.

  Phase 0 ‚Äì Preconditions (for your own agent branch)

  - Ensure your work for this run is:
    * Committed on your current working branch.
    * Pushed (if remote access is allowed) or at least safely committed locally.

  - Ensure the following branches are clearly identified (from the task description or config):
    * BASE_BRANCH
      - Example: `phase6-testing-base` or the main branch this phase is based on.
    * Your agent branch (AGENT_BRANCH)
      - Example naming pattern: `phase6-testing-agent<your-agent-number>-<short-scope>`.
    * INTEGRATION_BRANCH
      - Example: `phase6-testing-complete-all-agents`.
      - Normally created and managed by an integration agent/human, **not by you**, unless explicitly instructed.

  Steps you MUST perform as this agent (default: you are NOT the integration orchestrator):

  1. Ensure you are on your AGENT_BRANCH.
     - If AGENT_BRANCH does not exist yet:
       - Checkout BASE_BRANCH.
       - Create your agent branch from it.
       - Example:
         - `git checkout <BASE_BRANCH>`
         - `git checkout -b phase6-testing-agent<your-agent-number>-<short-description>`

  2. Prepare a clean, intentional working tree.
     - Run `git status` and confirm only files you intend to change are modified.
     - Stage all relevant changes:
       - `git add <files...>`

  3. Commit your work with a descriptive message.
     - Example:
       - `git commit -m "agent<your-agent-number>: implement <short summary of workstream tasks>"`

  4. Push your agent branch if remote is available.
     - Example:
       - `git push -u origin phase6-testing-agent<your-agent-number>-<short-description>`

  5. Emit a structured status summary in your final output with at least:
     - `agent_number`
     - `agent_branch`
     - `base_branch`
     - `status` (e.g. `"complete"`)
     - `tests_run` (commands + pass/fail)
     - `metrics` (if available: plugin count, test files, test count, coverage/readiness)
     - `conflicts_or_issues` (any merge issues you encountered in your own branch, or `[]` if none)
     - `notes` (assumptions, trade-offs, follow-ups)

  Global integration flow (for an integration agent or human orchestrator; you DO NOT perform these steps unless explicitly instructed):

  - DISCOVER_AGENT_BRANCHES
    - List branches (e.g., `git branch -a`) and filter by naming pattern (e.g., `phase6|agent`).

  - VALIDATE_AGENT_BRANCH_SET
    - Ensure all required agent branches exist (e.g., Agents 1, 2, 3).
    - If any are missing ‚Üí fail early with a ‚Äúmissing agent branch‚Äù error.

  - INSPECT_AGENT_COMMITS
    - Use `git show --stat <agent_tip_sha>` to confirm each branch has work and to infer dependencies (e.g. commit message like ‚ÄúAgent 1 Complete‚Äù embedded in Agent 2‚Äôs branch).

  - DETERMINE_MERGE_SPINE
    - Decide which agent branch is the spine (e.g., Agent 2 if it already includes Agent 1).
    - If one branch already aggregates others ‚Üí `INTEGRATION_BASE = SPINE_BRANCH`.
    - Otherwise ‚Üí `INTEGRATION_BASE = BASE_BRANCH` and merge all agents into it one by one.

  - CREATE_INTEGRATION_BRANCH
    - Example:
      - `git checkout <INTEGRATION_BASE>`
      - `git checkout -b <INTEGRATION_BRANCH>`

  - MERGE_REMAINING_AGENT_BRANCHES
    - Merge the remaining agent branches into INTEGRATION_BRANCH.
    - Resolve conflicts as needed, add files, and commit with a clear ‚Äúmerge + resolve‚Äù message.

  - RUN_INTEGRATION_TESTS
    - Run the full plugin/test suite on INTEGRATION_BRANCH.
    - Collect metrics like:
      - Number of plugins.
      - Number of test files.
      - Number of tests.
      - Coverage / readiness percentage.

  - VERIFY_METRICS_THRESHOLD
    - Require no failing tests.
    - Optionally check that readiness/coverage meets targets (e.g., ‚Äú95% production-ready‚Äù).

  - WRITE_INTEGRATION_SUMMARY_DOC
    - Example file: `PHASE_6_ALL_AGENTS_INTEGRATION_COMPLETE.md`
    - Contents should include:
      - Overall status (e.g., `INTEGRATION COMPLETE`).
      - Final INTEGRATION_BRANCH name.
      - Which agents were merged.
      - Conflicts encountered and how they were resolved.
      - Test and coverage metrics.
      - Per-agent contribution summary.

  - COMMIT_SUMMARY
    - `git add PHASE_6_ALL_AGENTS_INTEGRATION_COMPLETE.md`
    - `git commit --no-verify -m "docs(phase6): Complete integration summary for all agents"`

  - EMIT_FINAL_STATUS
    - Emit a final structured summary block including, for example:
      - `branch`: `<INTEGRATION_BRANCH>`
      - `agents_merged`: `[1, 2, 3]`
      - `conflicts`: array of `{ "path": "<file>", "status": "resolved" }`
      - `metrics`: `{ plugins, test_files, tests, readiness }`
      - Note that individual agent branches remain preserved.
</completion_and_git_workflow>


<output_format>
  At the end of execution, output a final, structured summary with the following sections
  (formatted in Markdown, so it can be used in a changelog or pull request description):

  - Summary
    Short, high-level description of what you accomplished for your workstream.

  - Changes
    Bullet list of key code/config/doc changes (include paths or modules where helpful).

  - Testing
    Which checks/tests you ran and their results. Explicitly note any tests you could not run and why.

  - Assumptions
    Any assumptions you made due to ambiguity or missing information.

  - Risks & Follow-ups
    Remaining risks, edge cases, known limitations, or recommended next steps.
</output_format>

<performance_note>
  Your execution speed and efficiency are being monitored. Maintain continuous, uninterrupted progression through your workflow, minimizing idle steps while preserving code quality and safety.
</performance_note>

[TASKS]
[
   üéØ High-Priority Automations (Should Implement)

   1. Documentation Drift Detection

   What: Auto-detect when code changes but docs don't update

     - Monitor file changes in commits
     - Cross-reference with related docs (using doc_id links)
     - Flag stale documentation automatically
     - Trigger: On every commit/PR
     - Output: .state/doc_drift_report.json
     - ROI: Prevents doc rot (currently manual)

   2. Dependency Graph Validator

   What: Validate module dependencies match CODEBASE_INDEX.yaml
   rules

     - Check no domain modules import from UI layer
     - Verify no circular dependencies
     - Enforce import path standards
     - Trigger: Pre-commit hook or CI
     - Tool: scripts/validate_dependency_graph.py
     - ROI: Prevents architecture violations

   3. Incomplete Implementation Gate

   What: Block merges if NotImplementedError/TODOs in critical
   paths

     - Scan for NotImplementedError, # TODO, pass-only
functions
     - Categorize by severity (see
   INCOMPLETE_IMPLEMENTATION_RULES.md)
     - Currently: 20+ incomplete implementations found
     - Trigger: PR quality gate
     - ROI: Forces completion before merge

   4. Auto-Migrate Deprecated Imports

   What: Auto-fix deprecated import paths on save

     - Detect from src.pipeline.* ‚Üí suggest from core.*
     - Apply safe replacements automatically
     - Currently: Manual (path_standards.yml only validates)
     - Tool: Pre-commit hook or editor plugin
     - ROI: Eliminates manual import fixing

   5. State File Cleanup Automation

   What: Archive/purge old execution state files

     - .state/ directory grows unbounded
     - Auto-archive files >30 days old
     - Compress historical state files
     - Trigger: Daily cron (config: retention_days: 30)
     - ROI: Prevents disk bloat

   6. Schema Validation on Load

   What: Validate all YAML/JSON files against schemas on load

     - Phase plans against phase_plan.schema.yaml
     - Workstreams against workstream.schema.yaml
     - Patterns against pattern.schema.yaml
     - Currently: No runtime validation
     - Tool: JSON Schema validators
     - ROI: Catches malformed configs early

   7. Dead Code Detection

   What: Find unreferenced functions/classes

     - Static analysis to find unused code
     - Flag for archival candidates
     - Trigger: Weekly report
     - ROI: Keeps codebase clean

   8. Test Coverage Delta Enforcement

   What: Require tests for new code, block coverage drops

     - Measure coverage delta per PR
     - Fail CI if coverage drops >2%
     - Currently: No coverage enforcement
     - Tool: pytest-cov with thresholds
     - ROI: Maintains test quality


--------------------------------------------------------------

   üîß Medium-Priority Automations

   9. Auto-Generate Module READMEs

   Status: EXEC-010 exists but manual trigger Automation: Run
on
   module file changes (watch core/**/*.py)

   10. Glossary Term Link Checker

   Status: glossary-validation.yml exists Gap: Doesn't auto-fix
   broken term links Automation: Auto-replace [[Term]] with
   correct casing

   11. GitHub Project Field Sync

   Status: Manual updates to custom fields (Priority, Status,
   etc.) Automation: Sync from phase YAML metadata to GH
Projects

   12. Performance Regression Detection

   What: Track execution time trends

     - Log orchestrator execution times
     - Alert if >20% slower than baseline
     - Trigger: After each run
     - Output: .state/performance_trends.json


--------------------------------------------------------------

   üìã Already Automated (Don't Duplicate)

   ‚úÖ Doc ID validation (doc_id_validation.yml)
   ‚úÖ Path standards (path_standards.yml)
   ‚úÖ Glossary validation (glossary-validation.yml)
   ‚úÖ Pattern detection (pattern-automation.yml)
   ‚úÖ Module validation (module_id_validation.yml)
   ‚úÖ Quality gates (quality-gates.yml)
   ‚úÖ Phase‚ÜíGitHub sync (splinter_phase_sync.yml)
   ‚úÖ Milestone tracking (milestone_completion.yml)


--------------------------------------------------------------

   üöÄ Quick Wins (Implement This Week)

     - Incomplete Implementation Gate - 2 hours, huge quality
   impact
     - Documentation Drift Detection - 3 hours, prevents stale
   docs
     - State File Cleanup - 1 hour, prevents disk issues
     - Dependency Graph Validator - 4 hours, enforces
architecture


---



 comprehensive implementation plan for
   the automation gaps. The plan organizes the work into 3
  independent workstreams that can execute in parallel:

  Workstream 1: Quality Gates Enhancement (2-3 hours)
  - Fix incomplete scanner syntax error
  - Add incomplete implementation gate to CI
  - Create dependency graph validator
  - Total: Blocks 16 production stubs from reaching main

  Workstream 2: Documentation Integrity (3-4 hours)
  - Create documentation drift detector
  - Detect hash mismatches, temporal drift, broken cross-refs
  - Integrate with existing doc_id system
  - Total: Prevents documentation rot

  Workstream 3: Maintenance Automation (1-2 hours)
  - Create state file cleanup script
  - Schedule daily cleanup workflow
  - Archive old files, compress, preserve critical files
  - Total: Prevents unbounded disk growth

  Key Execution Patterns:
  - Parallel development on Day 1 (5-7 hours total)
  - Staged rollout: WS3 ‚Üí WS1 ‚Üí WS2 (lowest risk first)
  - Threshold ratcheting: Start permissive, gradually tighten
  - Total effort: 9-13 hours across 2-3 days

  The plan leverages existing automation infrastructure (12
  GitHub workflows, validation scripts, test patterns) and
  follows surgical, minimal change principles. All critical
  files are identified with specific line numbers for
  modifications.

‚óè User approved Claude's plan
  ‚éø ¬†Plan saved to: ~\.claude\plans\streamed-jumping-lagoon.md
     ¬∑ /plan to edit

     Automation Gap Implementation Plan

     Executive Summary

     Implement 4 high-priority automations currently missing
     from the UET Framework, organized into 3 parallel
     workstreams. Total estimated effort: 8-10 hours across 2-3
      days.

     Current State:
     - 12 active GitHub workflows with strong CI/CD foundation
     - 196 tests (100% passing), 78% framework completion
     - Doc ID coverage: 58.6% (785 documents registered)
     - Incomplete scanner: 85% complete (missing CI integration
      only)
     - State file cleanup: No automation (manual only)
     - Dependency validation: Rules exist, no enforcement

     Target State:
     - Incomplete implementation gate blocking critical stubs
     in CI
     - Documentation drift detection preventing doc rot
     - Automated state file cleanup preventing disk bloat
     - Dependency graph validation enforcing architecture rules

     ---
     Workstream Architecture (3 Parallel Streams)

     Workstream 1: Quality Gates Enhancement

     Priority: HIGHEST (Quick wins, prevents production issues)
     Effort: 2-3 hours
     Value: Blocks incomplete code from merging

     Tasks:
     1. Fix incomplete scanner syntax error (5 min)
     2. Create incomplete allowlist configuration (10 min)
     3. Add incomplete scanner to quality-gates.yml (15 min)
     4. Create dependency graph validator script (30 min)
     5. Add dependency validator to quality-gates.yml (10 min)
     6. Add integration tests (30 min)

     Workstream 2: Documentation Integrity

     Priority: HIGH (Prevents documentation rot)
     Effort: 3-4 hours
     Value: Detects drift between code and docs

     Tasks:
     1. Create documentation drift detector script (60 min)
     2. Add drift detection to doc_id_validation.yml (10 min)
     3. Add drift detection tests (20 min)

     Workstream 3: Maintenance Automation

     Priority: MEDIUM (Operational hygiene)
     Effort: 1-2 hours
     Value: Prevents unbounded disk growth

     Tasks:
     1. Create state file cleanup script (45 min)
     2. Create scheduled cleanup workflow (15 min)
     3. Add cleanup validation tests (15 min)

     ---
     Detailed Implementation

     WORKSTREAM 1: Quality Gates Enhancement

     Critical Files

     - scripts/scan_incomplete_implementation.py - Fix syntax
     error (MODIFY)
     - incomplete_allowlist.yaml - Allowlist config (CREATE)
     - scripts/validate_dependency_graph.py - Dependency
     validator (CREATE)
     - .github/workflows/quality-gates.yml - Add gates (MODIFY)
     - tests/test_quality_gates.py - Integration tests (CREATE)

     Task 1.1: Fix Incomplete Scanner Syntax Error (5 min)

     File: scripts/scan_incomplete_implementation.py

     Change:
     # Line 14 - REMOVE this malformed line:
     # DOC_ID: DOC - SCRIPT - SCRIPTS - SCAN - INCOMPLETE -
     IMPLEMENTATION - 757

     # Line 15 - Keep proper doc_id format:
     # DOC_ID: DOC-SCRIPT-SCAN-INCOMPLETE-IMPLEMENTATION

     # Line 17 - from __future__ import annotations (now first
     statement)

     Reason: from __future__ imports must be first executable
     statement. Malformed doc_id comment breaks this rule.

     Verification: python
     scripts/scan_incomplete_implementation.py --help

     ---
     Task 1.2: Create Incomplete Allowlist Configuration (10
     min)

     File: incomplete_allowlist.yaml (NEW - root directory)

     Content:
     # Incomplete Implementation Allowlist
     allowed_stubs:
       # Test fixtures and mocks
       - pattern: "tests/fixtures/**/*.py"
         reason: "Test fixtures may have stubs"
       - pattern: "tests/mocks/**/*.py"
         reason: "Mock implementations intentionally minimal"

       # Documentation examples
       - pattern: "docs/examples/**/*.py"
         reason: "Example code may be incomplete for clarity"

       # Abstract interfaces
       - pattern: "**/interfaces/*.py"
         reason: "Abstract interfaces define contracts only"

       # Legacy/archive
       - pattern: "_ARCHIVE/**/*"
         reason: "Archived code exempt"
       - pattern: "legacy/**/*"
         reason: "Legacy code not under development"

       # Experimental
       - pattern: "experiments/**/*"
         reason: "Experimental code allowed incomplete"
       - pattern: "scratch/**/*"
         reason: "Scratch work area for prototyping"

     exceptions:
       - path: "core/adapters/base.py"
         symbol: "BaseAdapter"
         reason: "Abstract base class"

       - path: "core/engine/resilience/base.py"
         symbol: "ResilienceStrategy"
         reason: "Abstract strategy pattern"

     Verification: Scanner should load this file and apply
     allowlist rules

     ---
     Task 1.3: Add Incomplete Scanner to Quality Gates (15 min)

     File: .github/workflows/quality-gates.yml

     Insert after line 142 (after validate-structure job):

       incomplete-implementation-gate:
         name: Block Incomplete Implementations
         runs-on: ubuntu-latest
         steps:
           - name: Checkout code
             uses: actions/checkout@v4

           - name: Set up Python
             uses: actions/setup-python@v5
             with:
               python-version: '3.11'

           - name: Install dependencies
             run: pip install pyyaml

           - name: Run incomplete implementation scanner
             run: |
               python scripts/scan_incomplete_implementation.py
      \
                 --root . \
                 --output .state/incomplete_scan_summary.json \
                 --findings-output .state/final_findings.jsonl
     \
                 --ci-check \
                 --max-critical 0 \
                 --max-major 10

           - name: Upload scan results
             if: always()
             uses: actions/upload-artifact@v4
             with:
               name: incomplete-scan-results
               path: |
                 .state/incomplete_scan_summary.json
                 .state/final_findings.jsonl
               retention-days: 30

     Verification: Create test PR and verify workflow runs

     ---
     Task 1.4: Create Dependency Graph Validator (30 min)

     File: scripts/validate_dependency_graph.py (NEW)

     Key Features:
     - Load CODEBASE_INDEX.yaml for layer definitions
     - Detect layer violations (e.g., domain importing from ui)
     - Detect circular dependencies using DFS cycle detection
     - Block deprecated import patterns (src.pipeline.,
     MOD_ERROR_PIPELINE.)
     - Exit code 0 (pass) / 1 (violations found)
     - JSON report output to
     .state/dependency_graph_report.json

     Implementation Pattern:
     #!/usr/bin/env python3
     """
     Dependency Graph Validator - Enforce architectural layer
     compliance

     Validates:
     1. Layer compliance (no domain imports from UI layer)
     2. Circular dependency detection
     3. Import path standards enforcement
     4. Deprecated import blocking

     Exit codes: 0=pass, 1=violations
     """

     import ast
     import json
     import sys
     from pathlib import Path
     from typing import Dict, Set
     import yaml

     # Layer hierarchy: infra < domain < api < ui
     LAYER_HIERARCHY = {"infra": 0, "domain": 1, "api": 2,
     "ui": 3}

     # Deprecated patterns
     DEPRECATED_PATTERNS = ["src.pipeline",
     "MOD_ERROR_PIPELINE", "legacy."]

     class DependencyGraphValidator:
         def __init__(self, codebase_index_path: Path):
             self.index = self._load_index(codebase_index_path)
             self.violations = []

         def validate_all(self) -> bool:
             """Run all validation checks. Returns True if
     valid."""
             self._check_layer_violations()
             self._check_circular_dependencies()
             self._check_deprecated_imports()
             return len(self.violations) == 0

         # ... (implementation details in plan agent response)

     def main():
         validator = DependencyGraphValidator(Path("docs/DOC_re
     ference/CODEBASE_INDEX.yaml"))
         is_valid = validator.validate_all()
         validator.print_report()
         sys.exit(0 if is_valid else 1)

     Verification: python scripts/validate_dependency_graph.py

     ---
     Task 1.5: Add Dependency Validator to Quality Gates (10
     min)

     File: .github/workflows/quality-gates.yml

     Add to validate-structure job (after line 141):

           - name: Validate dependency graph
             run: |
               python scripts/validate_dependency_graph.py \
                 --report .state/dependency_graph_report.json

           - name: Upload dependency report
             if: always()
             uses: actions/upload-artifact@v4
             with:
               name: dependency-graph-report
               path: .state/dependency_graph_report.json
               retention-days: 7

     ---
     Task 1.6: Add Integration Tests (30 min)

     File: tests/test_quality_gates.py (NEW)

     """Integration tests for quality gate validators."""

     import subprocess
     import sys
     from pathlib import Path
     import pytest

     REPO_ROOT = Path(__file__).parent.parent
     SCRIPTS = REPO_ROOT / "scripts"


     class TestIncompleteScanner:
         def test_scanner_runs_successfully(self):
             """Scanner should execute without errors."""
             result = subprocess.run(
                 [sys.executable, SCRIPTS /
     "scan_incomplete_implementation.py",
                  "--root", REPO_ROOT],
                 capture_output=True,
                 text=True
             )
             assert result.returncode in (0, 1)

         def test_scanner_produces_output(self):
             """Scanner should produce JSON output."""
             # ... implementation


     class TestDependencyGraphValidator:
         def test_validator_runs_successfully(self):
             """Validator should execute without errors."""
             result = subprocess.run(
                 [sys.executable, SCRIPTS /
     "validate_dependency_graph.py"],
                 capture_output=True,
                 text=True
             )
             assert result.returncode in (0, 1)

     Verification: python -m pytest tests/test_quality_gates.py
      -v

     ---
     WORKSTREAM 2: Documentation Integrity

     Critical Files

     - doc_id/detect_doc_drift.py - Drift detection logic
     (CREATE)
     - .github/workflows/doc_id_validation.yml - Add drift
     check (MODIFY)
     - doc_id/DOC_ID_tests/test_drift_detection.py - Tests
     (CREATE)
     - docs/DOC_.index/suite-index.yaml - Reference for hashes
     (READ-ONLY)
     - docs/DOC_reference/CODEBASE_INDEX.yaml - Module mapping
     (READ-ONLY)

     Task 2.1: Create Documentation Drift Detector (60 min)

     File: doc_id/detect_doc_drift.py (NEW)

     Key Features:
     - Hash mismatch detection (content changed but metadata
     unchanged)
     - Temporal drift detection (metadata timestamp < file
     mtime)
     - Cross-reference validation (broken doc_id links)
     - Module documentation gap detection (code exists but no
     doc)
     - Exit code 0 (no drift) / 1 (drift detected)
     - JSON report output to .state/doc_drift_report.json

     Implementation Pattern:
     #!/usr/bin/env python3
     """
     Documentation Drift Detector

     Detects drift between documentation and code using:
     1. Hash mismatches (suite-index.yaml mfid vs. actual file
     hash)
     2. Temporal drift (metadata timestamp < file mtime)
     3. Cross-reference validation (broken doc_id links)
     4. Module documentation gaps (code exists but no doc)

     Exit codes: 0=no drift, 1=drift detected
     """

     import hashlib
     import json
     import re
     from pathlib import Path
     import yaml

     DOC_ID_PATTERN =
     re.compile(r"DOC-[A-Z0-9]+-[A-Z0-9]+(-[A-Z0-9]+)*-\d{3}")

     class DriftFinding:
         def __init__(self, drift_type: str, severity: str,
     file_path: str,
                      doc_id: str = None, reason: str = ""):
             self.drift_type = drift_type
             self.severity = severity
             self.file_path = file_path
             self.doc_id = doc_id
             self.reason = reason

     class DocDriftDetector:
         def __init__(self, suite_index_path: Path,
     codebase_index_path: Path):
             self.suite_index =
     self._load_yaml(suite_index_path)
             self.codebase_index =
     self._load_yaml(codebase_index_path)
             self.findings = []

         def detect_all_drift(self) -> bool:
             """Run all drift detection checks."""
             self._detect_hash_mismatches()
             self._detect_temporal_drift()
             self._detect_broken_cross_references()
             self._detect_documentation_gaps()
             return len(self.findings) == 0

         def _compute_mfid(self, file_path: Path) -> str:
             """Compute SHA-256 hash of file content."""
             return
     hashlib.sha256(file_path.read_bytes()).hexdigest()

         # ... (implementation details in plan agent response)

     def main():
         detector = DocDriftDetector(
             Path("docs/DOC_.index/suite-index.yaml"),
             Path("docs/DOC_reference/CODEBASE_INDEX.yaml")
         )
         no_drift = detector.detect_all_drift()
         detector.print_report()

     detector.save_report(Path(".state/doc_drift_report.json"))
         sys.exit(0 if no_drift else 1)

     Verification: python doc_id/detect_doc_drift.py

     ---
     Task 2.2: Add Drift Detection to Workflow (10 min)

     File: .github/workflows/doc_id_validation.yml

     Add after line 38:

         - name: Detect documentation drift
           id: drift
           run: |
             python doc_id/detect_doc_drift.py \
               --report doc_id/DOC_ID_reports/drift_report.json
      \
               --ci-check \
               --max-drift 20

         - name: Upload drift report
           if: always()
           uses: actions/upload-artifact@v4
           with:
             name: doc-drift-report
             path: doc_id/DOC_ID_reports/drift_report.json
             retention-days: 30

     ---
     Task 2.3: Add Drift Detection Tests (20 min)

     File: doc_id/DOC_ID_tests/test_drift_detection.py (NEW)

     """Tests for documentation drift detection."""

     import json
     import subprocess
     import sys
     from pathlib import Path
     import pytest

     REPO_ROOT = Path(__file__).parent.parent.parent
     DOC_ID_DIR = REPO_ROOT / "doc_id"


     class TestDriftDetection:
         def test_drift_detector_runs(self):
             """Drift detector should execute successfully."""
             result = subprocess.run(
                 [sys.executable, DOC_ID_DIR /
     "detect_doc_drift.py"],
                 capture_output=True,
                 text=True
             )
             assert result.returncode in (0, 1)

         def test_drift_report_generation(self):
             """Drift detector should generate JSON report."""
             # ... implementation

     Verification: python -m pytest
     doc_id/DOC_ID_tests/test_drift_detection.py -v

     ---
     WORKSTREAM 3: Maintenance Automation

     Critical Files

     - scripts/cleanup_state_files.py - Cleanup orchestrator
     (CREATE)
     - .github/workflows/state-file-cleanup.yml - Scheduled
     cleanup (CREATE)
     - tests/test_state_cleanup.py - Cleanup tests (CREATE)
     - .state/ - Target directory (MODIFY)
     - .state/archive/ - Archive directory (CREATE)

     Task 3.1: Create State File Cleanup Script (45 min)

     File: scripts/cleanup_state_files.py (NEW)

     Key Features:
     - Preserve critical files (current.json, *.db,
     health.json)
     - Archive files older than 30 days
     - Compress archived files with gzip
     - Clean up temporary files (*.tmp, *.lock)
     - Dry-run mode for safe testing
     - Exit code 0 (success) / 1 (error)

     Implementation Pattern:
     #!/usr/bin/env python3
     """
     State File Cleanup Automation

     Automatically archives and compresses old state files.

     Strategy:
     - Preserve critical: current.json, *.db, health.json
     - Archive files older than retention period (default 30
     days)
     - Compress archived files with gzip
     - Clean up temporary files

     Usage:
         python scripts/cleanup_state_files.py --dry-run
         python scripts/cleanup_state_files.py --days 60
     """

     import gzip
     import shutil
     from datetime import datetime, timedelta
     from pathlib import Path

     STATE_DIR = Path(__file__).parent.parent / ".state"
     ARCHIVE_DIR = STATE_DIR / "archive"

     CRITICAL_FILES = {"current.json", "health.json",
     "orchestration.db", "orchestrator.db"}
     ARCHIVABLE_PATTERNS = ["pipeline_results_*.json",
     "*_inventory.jsonl", "*_scan*.json"]
     TEMP_PATTERNS = ["*.tmp", "*.lock", "*_temp_*.json"]

     class StateFileCleanup:
         def __init__(self, retention_days: int = 30):
             self.retention_days = retention_days
             self.stats = {"archived": 0, "compressed": 0,
     "deleted": 0, "preserved": 0}

         def cleanup_all(self, dry_run: bool = False) -> bool:
             """Run complete cleanup workflow."""
             if dry_run:
                 print("üîç DRY RUN MODE - No files will be
     modified\n")

             self._archive_old_files(dry_run)
             self._compress_archived_files(dry_run)
             self._cleanup_temp_files(dry_run)
             self._print_summary()
             return True

         # ... (implementation details in plan agent response)

     def main():
         import argparse
         parser = argparse.ArgumentParser(description="Clean up
      state files")
         parser.add_argument("--days", type=int, default=30)
         parser.add_argument("--dry-run", action="store_true")
         args = parser.parse_args()

         cleanup = StateFileCleanup(args.days)
         cleanup.cleanup_all(dry_run=args.dry_run)

     Verification: python scripts/cleanup_state_files.py
     --dry-run

     ---
     Task 3.2: Create Scheduled Cleanup Workflow (15 min)

     File: .github/workflows/state-file-cleanup.yml (NEW)

     name: State File Cleanup

     on:
       schedule:
         - cron: '0 2 * * *'  # Daily at 2 AM UTC
       workflow_dispatch:  # Manual trigger

     jobs:
       cleanup:
         name: Clean Up State Files
         runs-on: ubuntu-latest

         steps:
           - name: Checkout code
             uses: actions/checkout@v4

           - name: Set up Python
             uses: actions/setup-python@v5
             with:
               python-version: '3.11'

           - name: Run cleanup (dry-run preview)
             run: |
               python scripts/cleanup_state_files.py --dry-run
     --days 30

           - name: Run cleanup (actual)
             if: github.event_name == 'workflow_dispatch'
             run: |
               python scripts/cleanup_state_files.py --days 30

           - name: Upload cleanup report
             if: always()
             uses: actions/upload-artifact@v4
             with:
               name: cleanup-report
               path: .state/cleanup_summary.txt
               retention-days: 7

     Verification: Manual workflow dispatch in GitHub Actions

     ---
     Task 3.3: Add Cleanup Validation Tests (15 min)

     File: tests/test_state_cleanup.py (NEW)

     """Tests for state file cleanup automation."""

     import subprocess
     import sys
     from pathlib import Path
     import pytest

     REPO_ROOT = Path(__file__).parent.parent
     SCRIPTS = REPO_ROOT / "scripts"


     class TestStateFileCleanup:
         def test_cleanup_script_runs(self):
             """Cleanup script should execute without
     errors."""
             result = subprocess.run(
                 [sys.executable, SCRIPTS /
     "cleanup_state_files.py", "--dry-run"],
                 capture_output=True,
                 text=True
             )
             assert result.returncode == 0

         def test_critical_files_preserved(self):
             """Critical files should never be deleted."""
             # ... implementation

     Verification: python -m pytest tests/test_state_cleanup.py
      -v

     ---
     Rollout Strategy

     Phase 1: Testing & Validation (Day 1)

     1. Fix incomplete scanner syntax error
     2. Run all validators locally
     3. Execute test suites
     4. Verify no regressions

     Phase 2: Staged Deployment (Day 2)

     1. Workstream 3 first (lowest risk - state cleanup)
       - Deploy cleanup script
       - Test manual execution
       - Enable scheduled workflow (dry-run only)
     2. Workstream 1 second (high value, moderate risk)
       - Deploy incomplete scanner fixes
       - Deploy dependency validator
       - Add to quality-gates.yml (permissive thresholds)
     3. Workstream 2 last (complex, requires validation)
       - Deploy drift detector
       - Add to doc_id_validation.yml
       - Monitor for false positives

     Phase 3: Threshold Ratcheting (Week 2)

     1. Monitor violation counts for 1 week
     2. Gradually reduce thresholds:
       - Incomplete implementations: max_major=100 ‚Üí 50 ‚Üí 10
       - Doc drift: max_drift=20 ‚Üí 10 ‚Üí 5
     3. Track metrics in dashboards

     Phase 4: Full Production (Week 3)

     1. Enable all gates with final thresholds
     2. Enforce on main branch
     3. Add pre-commit hooks (optional)
     4. Document processes in README

     ---
     Success Criteria

     Workstream 1: Quality Gates Enhancement

     - ‚úÖ Incomplete scanner runs without syntax errors
     - ‚úÖ CI workflow executes successfully on test PR
     - ‚úÖ Dependency validator detects layer violations
     - ‚úÖ Tests pass: pytest tests/test_quality_gates.py -v
     - ‚úÖ Zero false positives on main branch
     - ‚úÖ All 16 production stubs properly categorized
     (critical/allowed)

     Workstream 2: Documentation Integrity

     - ‚úÖ Drift detector identifies all 4 drift types
     - ‚úÖ CI workflow runs without errors
     - ‚úÖ JSON reports generated successfully
     - ‚úÖ Tests pass: pytest
     doc_id/DOC_ID_tests/test_drift_detection.py -v
     - ‚úÖ Drift count < 20 on main branch
     - ‚úÖ Zero false positives for hash mismatches

     Workstream 3: Maintenance Automation

     - ‚úÖ Cleanup script archives old files correctly
     - ‚úÖ Critical files preserved (never deleted)
     - ‚úÖ Compression reduces disk usage by 60%+
     - ‚úÖ Tests pass: pytest tests/test_state_cleanup.py -v
     - ‚úÖ Scheduled workflow runs daily without errors
     - ‚úÖ Archive directory structure maintained

     ---
     Risk Mitigation

     | Risk                                  | Mitigation
     Strategy
          |
     |---------------------------------------|-----------------
     ----------------------------------------------------------
     |
     | False positives in incomplete scanner | Comprehensive
     allowlist, permissive initial thresholds (max_major=10)
       |
     | Breaking existing CI pipeline         | Add new jobs
     separately, use continue-on-error: true initially
        |
     | Documentation drift false alarms      | High initial
     threshold (max_drift=20), manual review of first 50
     findings |
     | Accidental deletion of critical files | Dry-run mode
     default, explicit preservation list, comprehensive tests
        |
     | Circular dependency false positives   | Test on
     known-good codebase first, adjust detection algorithm if
     needed   |

     ---
     Execution Sequence (Parallel)

     Day 1 - Setup (Sequential, 30 min):
     1. Fix incomplete scanner syntax (5 min)
     2. Create allowlist configuration (10 min)
     3. Run local validation (15 min)

     Day 1 - Parallel Development (2-4 hours):
     - Thread A: Workstream 1 Tasks 1.3-1.6 (1.5 hours)
     - Thread B: Workstream 2 Tasks 2.1-2.3 (2 hours)
     - Thread C: Workstream 3 Tasks 3.1-3.3 (1.5 hours)

     Day 2 - Integration & Testing (3-4 hours):
     1. Merge all changes to feature branch
     2. Run full test suite (pytest tests/ -v)
     3. Test each validator manually
     4. Create test PR to verify CI integration

     Day 3 - Production Rollout:
     1. Create PR with comprehensive documentation
     2. Code review and approval
     3. Merge to main branch
     4. Monitor CI/CD for 48 hours
     5. Adjust thresholds based on initial results

     ---
     Monitoring & Metrics

     Key Metrics to Track

     Incomplete Implementation Scanner:
     - Critical violations per commit: Target 0
     - Major violations trend: Monitor weekly, target reduction
     - False positive rate: Target < 5%

     Documentation Drift:
     - Hash mismatches per week: Baseline, then reduce
     - Temporal drift count: Monitor trend
     - Broken cross-references: Target 0

     State File Cleanup:
     - Disk space saved (GB): Track monthly
     - Files archived per week: Monitor for anomalies
     - Compression ratio: Target 60%+

     CI/CD Performance:
     - Workflow execution time: Baseline + new gates
     - Gate pass rate: Target > 95%
     - Build failure attribution: Track gate-specific failures

     ---
     Dependencies & Assumptions

     Dependencies:
     - Python 3.11+ with pytest, pyyaml
     - GitHub Actions runners (ubuntu-latest)
     - Existing infrastructure: quality-gates.yml,
     doc_id_validation.yml
     - Access to CODEBASE_INDEX.yaml and suite-index.yaml

     Assumptions:
     - Current 196 tests continue to pass
     - No breaking changes to existing workflows
     - .state/ directory exists and is writable
     - doc_id system remains stable (58.6% coverage baseline)

     Blockers (None Expected):
     - All infrastructure already exists
     - No external API dependencies
     - No database schema changes required
     - No cross-team coordination needed

     ---
     Post-Implementation

     Week 1 - Monitoring:
     - Review all gate violations daily
     - Tune allowlists and thresholds
     - Document false positive patterns
     - Gather feedback from developers

     Week 2 - Optimization:
     - Reduce threshold tolerances gradually
     - Add additional detection patterns if needed
     - Optimize CI execution time
     - Update documentation with lessons learned

     Month 1 - Retrospective:
     - Measure impact on code quality
     - Calculate disk space savings
     - Assess documentation freshness improvement
     - Plan additional automation opportunities

     ---
     Total Effort Summary

     | Workstream         | Setup  | Development | Testing |
     Integration | Total       |
     |--------------------|--------|-------------|---------|---
     ----------|-------------|
     | WS1: Quality Gates | 15 min | 40 min      | 30 min  | 25
      min      | 1.5-2 hours |
     | WS2: Doc Integrity | 0 min  | 60 min      | 20 min  | 30
      min      | 2-3 hours   |
     | WS3: Maintenance   | 0 min  | 45 min      | 15 min  | 30
      min      | 1.5-2 hours |
     | TOTAL              | 15 min | 2.5 hours   | 1 hour  |
     1.5 hours   | 5-7 hours   |

     Plus:
     - Integration & testing: 3-4 hours
     - Documentation & rollout: 1-2 hours
     - Grand Total: 9-13 hours (well within 2-3 day estimate)

     ---
     Appendix: Key Commands

     Local Testing:
     # Fix syntax and test incomplete scanner
     python scripts/scan_incomplete_implementation.py --help
     python scripts/scan_incomplete_implementation.py --root .
     --dry-run

     # Test dependency validator
     python scripts/validate_dependency_graph.py

     # Test drift detector
     python doc_id/detect_doc_drift.py --dry-run

     # Test state cleanup
     python scripts/cleanup_state_files.py --dry-run

     # Run all new tests
     python -m pytest tests/test_quality_gates.py -v
     python -m pytest
     doc_id/DOC_ID_tests/test_drift_detection.py -v
     python -m pytest tests/test_state_cleanup.py -v

     CI Integration Verification:
     # Validate workflows locally (requires act or similar)
     act -l  # List workflows
     act push -j incomplete-implementation-gate  # Test
     specific job

     Monitoring:
     # Check gate results
     gh run list --workflow=quality-gates.yml
     gh run view <run-id>

     # View artifacts
     gh run download <run-id> --name incomplete-scan-results

     ---
     End of Implementation Plan]
