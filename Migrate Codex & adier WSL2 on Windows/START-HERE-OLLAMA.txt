================================================================================
                    LOCAL OLLAMA SETUP - START HERE
================================================================================

Your WSL + Aider + Ollama (LOCAL ONLY) setup is ready!

NO CLOUD API KEYS NEEDED - All AI processing happens locally.


CRITICAL NEXT STEP: Enable WSL Access to Ollama
================================================================================

Step 1: Configure Ollama to Listen on Network Interface
--------------------------------------------------------------------------------
Run this script:

    cd C:\Users\richg\WSL_CODEX_AIDER
    .\Configure-Ollama-For-WSL.ps1

Or manually set environment variable:
    Variable: OLLAMA_HOST
    Value:    0.0.0.0:11434

Then restart Ollama or reboot.


Step 2: Test Connection from WSL
--------------------------------------------------------------------------------
    wsl -d Ubuntu
    curl http://172.27.16.1:11434/api/tags
    exit

Expected: JSON response with your model list

If this fails, Ollama isn't configured yet (go back to Step 1)


Step 3: Pull a Model (If You Haven't Already)
--------------------------------------------------------------------------------
    ollama pull deepseek-coder

Or another coding model:
    ollama pull qwen2.5-coder
    ollama pull codellama


Step 4: Test Aider
--------------------------------------------------------------------------------
    # Create test repo
    wsl -d Ubuntu
    cd ~/code
    mkdir test-repo && cd test-repo
    git init
    echo "# Test" > README.md
    git add . && git commit -m "init"

    # Launch Aider
    source ~/.venvs/aider/bin/activate
    aider

    # Try a command:
    > Add a Python hello world script

    exit


Step 5: Use PowerShell Launcher (After Step 1 Complete)
--------------------------------------------------------------------------------
    # Reload profile
    . $PROFILE

    # Launch Aider for any repo
    aider-wsl <repo-name>


================================================================================
                        YOUR CONFIGURATION
================================================================================

Location:        Windows + WSL Ubuntu
AI Provider:     Ollama (local, no cloud)
Aider Version:   0.86.1
Default Model:   ollama_chat/deepseek-coder
Ollama Host:     http://172.27.16.1:11434 (Windows host from WSL)

WSL Configuration (~/.bashrc):
    export OLLAMA_API_BASE="http://172.27.16.1:11434"
    export AIDER_MODEL="ollama_chat/deepseek-coder"
    unset OPENAI_API_KEY  # No cloud APIs
    unset ANTHROPIC_API_KEY
    unset DEEPSEEK_API_KEY


================================================================================
                        AVAILABLE COMMANDS
================================================================================

After reloading profile (. $PROFILE):

    aider-wsl <repo>    - Launch Aider with Ollama
    wsl-info            - Show WSL environment
    wsl-repos           - List repos in ~/code
    wsl-code [repo]     - Open WSL terminal


================================================================================
                        TROUBLESHOOTING
================================================================================

Problem: "Connection refused" or "Cannot connect"
Solution: Run .\Configure-Ollama-For-WSL.ps1
          Ollama needs to listen on 0.0.0.0, not just 127.0.0.1

Problem: "Model not found"
Solution: ollama pull deepseek-coder

Problem: Slow responses
Solution: Use smaller model (qwen2.5-coder:7b)

Problem: Aider asks for API key
Solution: Make sure ~/.bashrc has OLLAMA_API_BASE set
          Or launch with: aider --model ollama_chat/deepseek-coder


================================================================================
                        DOCUMENTATION
================================================================================

üìÑ OLLAMA-LOCAL-SETUP-COMPLETE.md  - Complete setup guide (READ THIS!)
üìÑ OLLAMA-SETUP-GUIDE.md            - Technical reference
üìÑ Configure-Ollama-For-WSL.ps1     - Configuration script

These files are in:
    C:\Users\richg\WSL_CODEX_AIDER\


================================================================================
                        WHAT'S DIFFERENT FROM CLOUD
================================================================================

‚úÖ No API keys needed
‚úÖ No API costs
‚úÖ Code stays on your machine (privacy)
‚úÖ Works offline
‚úÖ LAN-speed responses

‚ö†Ô∏è Requires one-time Ollama configuration (Step 1 above)
‚ö†Ô∏è Requires downloaded models (takes disk space)
‚ö†Ô∏è Performance depends on your GPU


================================================================================
                        QUICK START SUMMARY
================================================================================

1. .\Configure-Ollama-For-WSL.ps1   (enable WSL access)
2. Restart Ollama or reboot
3. ollama pull deepseek-coder       (download model)
4. Test: curl http://172.27.16.1:11434/api/tags
5. aider-wsl <your-repo>            (start coding!)


================================================================================
Ready to start? Run:
    .\Configure-Ollama-For-WSL.ps1

For detailed instructions, see:
    OLLAMA-LOCAL-SETUP-COMPLETE.md
================================================================================
