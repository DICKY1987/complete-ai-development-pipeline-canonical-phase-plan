
 â–â–›â–ˆâ–ˆâ–ˆâ–œâ–Œ   Claude Code v2.0.55
â–â–œâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–›â–˜  Sonnet 4.5 Â· Claude Pro
  â–˜â–˜ â–â–    ~\ALL_AI\Complete AI Development Pipeline â€“ Canonical Phâ€¦

> i need to clean up the repo by finding old code that has been
improved somewhere eles in the repo. i need find old unused files
and folders. ultrathink and deterimne how to identify these files in
 the codebase that can be archived. exclude .md and .txt files from
your anaylsis.

â— I'll help you identify old, unused, and superseded code in your
  repository that can be archived. Let me start by exploring the
  codebase to understand its structure and identify patterns.

â— 2 Explore agents finished (ctrl+o to expand)
   â”œâ”€ Explore repository structure Â· 28 tool uses Â· 36.6k tokens
   â”‚  âŽ¿ Done
   â””â”€ Find cleanup scripts and patterns Â· 24 tool uses Â· 47.9k tokens
      âŽ¿ Done

â— Excellent! I've completed my exploration of the codebase. I found
  that your repository already has a sophisticated cleanup
  automation system with multiple analysis tools. Now let me develop
   a comprehensive plan for identifying archival candidates.

â— Plan(Develop cleanup strategy)
  âŽ¿ Â Done (26 tool uses Â· 60.2k tokens Â· 5m 22s)

â— Now I'm in Phase 3: Synthesis. Let me read the critical files
  identified in the plan to better understand the existing
  infrastructure before asking you key questions.

â— Read(scripts\analyze_cleanup_candidates.py)
  âŽ¿ Â Read 827 lines

â— Read(scripts\analyze_imports.py)
  âŽ¿ Â Read 286 lines

â— Read(docs\CODEBASE_INDEX.yaml)
  âŽ¿ Â Read 100 lines

â— Read(FOLDER_CLASSIFICATION.yaml)
  âŽ¿ Â Read 100 lines

â— Read(conftest.py)
  âŽ¿ Â Read 32 lines

â— Perfect! I've reviewed the existing cleanup infrastructure. Now
  let me ask you some key questions to finalize the plan:

â—Â User answered Claude's questions:
  âŽ¿ Â Â· What confidence threshold should we use for automated
     archival? The existing tool uses 85% by default. â†’ Conservative
      (90%+)
     Â· The existing tool uses a 6-month staleness threshold (files
     not modified in 180 days). Should we adjust this? â†’ More
     aggressive (3 months)
     Â· How should we handle the parallel implementations found
     (e.g., ./engine/ vs ./modules/core-engine/, ./error/engine/ vs
     ./modules/error-engine/)? â†’ Detailed analysis first

     Â· Should we enhance the existing analyze_cleanup_candidates.py
     tool with additional signals (entry point reachability, test
     coverage analysis), or use it as-is? â†’ Add new signals


â— Excellent! Based on your preferences, I'll now create the final
  plan. Let me write it to the plan file:

â— Updated plan
  âŽ¿ Â /plan to preview Â· ~\.claude\plans\calm-spinning-cosmos.md

â— Perfect! I've created a comprehensive plan for cleaning up your
  Python codebase. The plan:

  Key Features:
  - âœ… Enhances your existing analyze_cleanup_candidates.py with 2
  new signals (entry point reachability + test coverage)
  - âœ… Creates detailed parallel implementation analyzer for
  competing modules
  - âœ… Uses your preferences: 90% confidence threshold, 90-day
  staleness, excludes .md/.txt files
  - âœ… Conservative 6-signal approach with comprehensive validation
  - âœ… Tiered execution (Tier 1: automated, Tier 2-3: manual review)
  - âœ… Full rollback capability and git history preservation

  What's in the Plan:
  1. Phase 1: Enhance cleanup analyzer with reachability and test
  coverage scoring
  2. Phase 2: Create parallel implementation detector with 200-point
   comparison system
  3. Phase 3: Master orchestration script to run all analyzers
  4. Phase 4: Comprehensive validation framework (pre/post archive)
  5. Phase 5: Detailed execution workflow with dry-run, validation
  gates, and commit guidance

  Estimated Results:
  - 80-100 files in Tier 1 (90%+ confidence) - safe for automated
  archival
  - 100-150 files in Tier 2 (75-89%) - review recommended
  - Detailed reports on parallel implementations (engine/ vs
  modules/core-engine/, etc.)

  The plan is ready for your review and approval!
  âŽ¿ Â User rejected Claude's plan:
     â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
     â”‚ Python Code Cleanup & Archival Plan                             â”‚
     â”‚                                                                 â”‚
     â”‚ Executive Summary                                               â”‚
     â”‚                                                                 â”‚
     â”‚ This plan enhances your existing analyze_cleanup_candidates.py  â”‚
     â”‚ tool with additional forensic signals to identify old, unused,  â”‚
     â”‚ and superseded Python code for archival. Based on exploration ofâ”‚
     â”‚  929 Python files across your repository, we'll use a           â”‚
     â”‚ conservative, multi-signal approach that excludes .md and .txt  â”‚
     â”‚ files.                                                          â”‚
     â”‚                                                                 â”‚
     â”‚ User Preferences Applied:                                       â”‚
     â”‚ - Confidence threshold: 90%+ (conservative - very safe archival)â”‚
     â”‚ - Staleness threshold: 90 days (3 months - aggressive flagging) â”‚
     â”‚ - Parallel implementations: Detailed analysis before archival   â”‚
     â”‚ decisions                                                       â”‚
     â”‚ - Tool enhancement: Add entry point reachability + test coverageâ”‚
     â”‚  signals                                                        â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Current State Analysis                                          â”‚
     â”‚                                                                 â”‚
     â”‚ Existing Infrastructure (Strengths)                             â”‚
     â”‚                                                                 â”‚
     â”‚ Your repository already has sophisticated cleanup tools:        â”‚
     â”‚                                                                 â”‚
     â”‚ 1. scripts/analyze_cleanup_candidates.py (827 lines)            â”‚
     â”‚   - SHA-256 duplication detection                               â”‚
     â”‚   - Git history staleness scoring                               â”‚
     â”‚   - Obsolescence detection (version suffixes, deprecated        â”‚
     â”‚ patterns)                                                       â”‚
     â”‚   - Import isolation analysis                                   â”‚
     â”‚   - Canonical path awareness (CODEBASE_INDEX.yaml)              â”‚
     â”‚ 2. scripts/analyze_imports.py (286 lines)                       â”‚
     â”‚   - AST-based import analysis                                   â”‚
     â”‚   - Deprecated import detection                                 â”‚
     â”‚   - Cross-module reference tracking                             â”‚
     â”‚ 3. Archive system: 14 timestamped archives following            â”‚
     â”‚ ./archive/YYYY-MM-DD_HHMMSS_description/ pattern                â”‚
     â”‚                                                                 â”‚
     â”‚ Identified Gaps                                                 â”‚
     â”‚                                                                 â”‚
     â”‚ 1. No entry point reachability analysis (orphaned code          â”‚
     â”‚ unreachable from __main__, tests, or CLI)                       â”‚
     â”‚ 2. No test coverage integration for archival decisions          â”‚
     â”‚ 3. No detailed parallel implementation comparison               â”‚
     â”‚ 4. Configuration uses 180-day staleness (you want 90 days)      â”‚
     â”‚ 5. Default 85% confidence threshold (you want 90%)              â”‚
     â”‚                                                                 â”‚
     â”‚ Repository Structure                                            â”‚
     â”‚                                                                 â”‚
     â”‚ - 929 Python files across multiple directories                  â”‚
     â”‚ - Primary areas: ./modules/ (37 subdirectories with             â”‚
     â”‚ ULID-prefixed files), ./engine/, ./scripts/,                    â”‚
     â”‚ ./UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/                      â”‚
     â”‚ - Known overlaps:                                               â”‚
     â”‚   - Orchestration: ./modules/core-engine/ vs ./engine/          â”‚
     â”‚   - Error systems: ./modules/error-engine/ vs                   â”‚
     â”‚ ./UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/error/                â”‚
     â”‚   - State management: multiple implementations                  â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Implementation Plan                                             â”‚
     â”‚                                                                 â”‚
     â”‚ Phase 1: Enhance Cleanup Analyzer (New Signals)                 â”‚
     â”‚                                                                 â”‚
     â”‚ Step 1.1: Add Entry Point Reachability Analyzer                 â”‚
     â”‚                                                                 â”‚
     â”‚ Create: scripts/entry_point_reachability.py                     â”‚
     â”‚                                                                 â”‚
     â”‚ Purpose: Identify code unreachable from any entry point         â”‚
     â”‚ (orphaned code)                                                 â”‚
     â”‚                                                                 â”‚
     â”‚ Entry Point Categories:                                         â”‚
     â”‚ 1. Files with if __name__ == "__main__": (30+ found)            â”‚
     â”‚ 2. Test files in ./tests/ (491 tests collected)                 â”‚
     â”‚ 3. CLI entry points (e.g., ./modules/aim-cli/m01001A_main.py)   â”‚
     â”‚ 4. Script files in ./scripts/ directory                         â”‚
     â”‚ 5. Pytest fixtures (conftest.py)                                â”‚
     â”‚                                                                 â”‚
     â”‚ Algorithm:                                                      â”‚
     â”‚ def analyze_entry_point_reachability():                         â”‚
     â”‚     """                                                         â”‚
     â”‚     1. Identify all entry points in repository                  â”‚
     â”‚     2. Build complete import graph (reuse analyze_imports.py    â”‚
     â”‚ logic)                                                          â”‚
     â”‚     3. BFS traversal from each entry point marking reachable    â”‚
     â”‚ modules                                                         â”‚
     â”‚     4. Identify unreachable modules (orphans)                   â”‚
     â”‚     5. Cross-reference with test imports                        â”‚
     â”‚     6. Score by reachability distance                           â”‚
     â”‚                                                                 â”‚
     â”‚     Scoring:                                                    â”‚
     â”‚     - 100: Not reachable from any entry point + no test         â”‚
     â”‚ references                                                      â”‚
     â”‚     - 85: Only reachable from other orphans                     â”‚
     â”‚     - 70: Only reachable via deprecated import paths            â”‚
     â”‚     - 0: Reachable from active entry points                     â”‚
     â”‚     """                                                         â”‚
     â”‚                                                                 â”‚
     â”‚ Output: entry_point_reachability_report.json                    â”‚
     â”‚ - List of orphaned files with confidence scores                 â”‚
     â”‚ - Reachability tree showing paths from entry points             â”‚
     â”‚ - Files only reachable via deprecated imports                   â”‚
     â”‚                                                                 â”‚
     â”‚ Integration: Provide reachability_score (0-100) for each file toâ”‚
     â”‚  master analyzer                                                â”‚
     â”‚                                                                 â”‚
     â”‚ Step 1.2: Add Test Coverage Analyzer                            â”‚
     â”‚                                                                 â”‚
     â”‚ Create: scripts/test_coverage_archival.py                       â”‚
     â”‚                                                                 â”‚
     â”‚ Purpose: Identify untested files (often indicates abandoned     â”‚
     â”‚ code)                                                           â”‚
     â”‚                                                                 â”‚
     â”‚ Algorithm:                                                      â”‚
     â”‚ def analyze_test_coverage_for_archival():                       â”‚
     â”‚     """                                                         â”‚
     â”‚     1. Scan ./tests/ and                                        â”‚
     â”‚ UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/tests/                  â”‚
     â”‚     2. Extract imports and test targets from each test file     â”‚
     â”‚ using AST                                                       â”‚
     â”‚     3. Build mapping: module_path -> [test_files_covering_it]   â”‚
     â”‚     4. Identify modules with no test coverage                   â”‚
     â”‚     5. Cross-reference with:                                    â”‚
     â”‚        - Last modified date (no tests + old = abandoned)        â”‚
     â”‚        - Import isolation (no tests + not imported = dead)      â”‚
     â”‚        - Deprecation patterns (no tests + deprecated naming =   â”‚
     â”‚ obsolete)                                                       â”‚
     â”‚                                                                 â”‚
     â”‚     Scoring:                                                    â”‚
     â”‚     - 95: No tests + 90+ days stale + not imported              â”‚
     â”‚     - 80: No tests + deprecated naming pattern                  â”‚
     â”‚     - 70: No tests + only imported by untested code             â”‚
     â”‚     - 0: Has test coverage                                      â”‚
     â”‚     """                                                         â”‚
     â”‚                                                                 â”‚
     â”‚ Output: test_coverage_archival_report.json                      â”‚
     â”‚ - Files with no test coverage                                   â”‚
     â”‚ - Files with only indirect test coverage                        â”‚
     â”‚ - Coverage gaps by module                                       â”‚
     â”‚                                                                 â”‚
     â”‚ Integration: Provide test_coverage_score (0-100) for each file  â”‚
     â”‚ to master analyzer                                              â”‚
     â”‚                                                                 â”‚
     â”‚ Step 1.3: Enhance Main Cleanup Analyzer                         â”‚
     â”‚                                                                 â”‚
     â”‚ Modify: scripts/analyze_cleanup_candidates.py                   â”‚
     â”‚                                                                 â”‚
     â”‚ Changes:                                                        â”‚
     â”‚ 1. Update configuration constants:                              â”‚
     â”‚ STALENESS_DAYS = 90  # Changed from 180 to 90 (3 months)        â”‚
     â”‚ CONFIDENCE_THRESHOLD = 90  # Changed from 85 to 90              â”‚
     â”‚ 2. Add new scoring dimensions to FileScore dataclass:           â”‚
     â”‚ @dataclass                                                      â”‚
     â”‚ class FileScore:                                                â”‚
     â”‚     # Existing fields...                                        â”‚
     â”‚     reachability_score: int = 0  # 0-100 (NEW)                  â”‚
     â”‚     test_coverage_score: int = 0  # 0-100 (NEW)                 â”‚
     â”‚ 3. Update score_file() to integrate new signals:                â”‚
     â”‚ def score_file(self, filepath: Path) -> FileScore:              â”‚
     â”‚     # Existing scores                                           â”‚
     â”‚     dup_score, dup_reasons = self.score_duplication(filepath)   â”‚
     â”‚     stale_score, stale_reasons = self.score_staleness(filepath) â”‚
     â”‚     obs_score, obs_reasons = self.score_obsolescence(filepath)  â”‚
     â”‚     iso_score, iso_reasons = self.score_isolation(filepath)     â”‚
     â”‚                                                                 â”‚
     â”‚     # NEW: Get reachability and test coverage scores            â”‚
     â”‚     reach_score, reach_reasons =                                â”‚
     â”‚ self.score_reachability(filepath)                               â”‚
     â”‚     test_score, test_reasons =                                  â”‚
     â”‚ self.score_test_coverage(filepath)                              â”‚
     â”‚                                                                 â”‚
     â”‚     # Weighted composite score                                  â”‚
     â”‚     total = (                                                   â”‚
     â”‚         dup_score * 0.25 +      # Duplication: 25%              â”‚
     â”‚         stale_score * 0.15 +    # Staleness: 15%                â”‚
     â”‚         obs_score * 0.20 +      # Obsolescence: 20%             â”‚
     â”‚         iso_score * 0.15 +      # Isolation: 15%                â”‚
     â”‚         reach_score * 0.15 +    # Reachability: 15% (NEW)       â”‚
     â”‚         test_score * 0.10       # Test coverage: 10% (NEW)      â”‚
     â”‚     )                                                           â”‚
     â”‚                                                                 â”‚
     â”‚     # Confidence boost for signal convergence                   â”‚
     â”‚     high_signal_count = sum([                                   â”‚
     â”‚         dup_score >= 80,                                        â”‚
     â”‚         stale_score >= 80,                                      â”‚
     â”‚         obs_score >= 80,                                        â”‚
     â”‚         iso_score >= 80,                                        â”‚
     â”‚         reach_score >= 80,                                      â”‚
     â”‚         test_score >= 80                                        â”‚
     â”‚     ])                                                          â”‚
     â”‚     if high_signal_count >= 3:                                  â”‚
     â”‚         confidence += 10  # Boost when multiple signals agree   â”‚
     â”‚ 4. Add file type filtering:                                     â”‚
     â”‚ def should_analyze_file(self, filepath: Path) -> bool:          â”‚
     â”‚     """Filter out .md and .txt files per user requirement."""   â”‚
     â”‚     ext = filepath.suffix.lower()                               â”‚
     â”‚     if ext in {'.md', '.txt'}:                                  â”‚
     â”‚         return False                                            â”‚
     â”‚     return ext in CODE_EXTENSIONS | CONFIG_EXTENSIONS           â”‚
     â”‚                                                                 â”‚
     â”‚ Phase 2: Parallel Implementation Analyzer                       â”‚
     â”‚                                                                 â”‚
     â”‚ Step 2.1: Create Parallel Implementation Detector               â”‚
     â”‚                                                                 â”‚
     â”‚ Create: scripts/detect_parallel_implementations.py              â”‚
     â”‚                                                                 â”‚
     â”‚ Purpose: Detailed comparison of competing implementations (e.g.,â”‚
     â”‚  ./engine/ vs ./modules/core-engine/)                           â”‚
     â”‚                                                                 â”‚
     â”‚ Algorithm:                                                      â”‚
     â”‚ def detect_parallel_implementations():                          â”‚
     â”‚     """                                                         â”‚
     â”‚     1. Load FOLDER_CLASSIFICATION.yaml for MODULE_KIND taxonomy â”‚
     â”‚     2. Identify overlap groups by functional similarity:        â”‚
     â”‚        - Orchestration: ./modules/core-engine/ vs ./engine/ vs  â”‚
     â”‚          ./UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/core/engine/ â”‚
     â”‚        - Error: ./modules/error-engine/ vs                      â”‚
     â”‚ ./UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/error/                â”‚
     â”‚        - State: ./modules/core-state/ vs ./state/ vs            â”‚
     â”‚ ./engine/state_store/                                           â”‚
     â”‚                                                                 â”‚
     â”‚     3. For each overlap group, score implementations:           â”‚
     â”‚        Scoring criteria:                                        â”‚
     â”‚        - CODEBASE_INDEX.yaml canonical path: +50 points         â”‚
     â”‚        - ULID-prefixed files (m010001_*): +40 points            â”‚
     â”‚        - Most recent git activity: +30 points                   â”‚
     â”‚        - Higher import reference count: +25 points              â”‚
     â”‚        - Test coverage percentage: +25 points                   â”‚
     â”‚        - Documentation quality: +15 points                      â”‚
     â”‚        - Code size (prefer smaller/cleaner): +15 points         â”‚
     â”‚                                                                 â”‚
     â”‚        Total: 200 points maximum per implementation             â”‚
     â”‚                                                                 â”‚
     â”‚     4. Rank implementations within each group                   â”‚
     â”‚     5. Generate detailed comparison report                      â”‚
     â”‚     6. Recommend primary implementation (keep) vs secondary     â”‚
     â”‚ (archive candidates)                                            â”‚
     â”‚     """                                                         â”‚
     â”‚                                                                 â”‚
     â”‚ Output: parallel_implementations_analysis.json                  â”‚
     â”‚                                                                 â”‚
     â”‚ Report Structure:                                               â”‚
     â”‚ {                                                               â”‚
     â”‚   "overlap_groups": [                                           â”‚
     â”‚     {                                                           â”‚
     â”‚       "group_id": "orchestration_engines",                      â”‚
     â”‚       "purpose": "Workstream orchestration and execution",      â”‚
     â”‚       "implementations": [                                      â”‚
     â”‚         {                                                       â”‚
     â”‚           "path": "modules/core-engine/",                       â”‚
     â”‚           "score": 185,                                         â”‚
     â”‚           "ranking": 1,                                         â”‚
     â”‚           "status": "PRIMARY - KEEP",                           â”‚
     â”‚           "strengths": [                                        â”‚
     â”‚             "Canonical path in CODEBASE_INDEX.yaml",            â”‚
     â”‚             "ULID-prefixed files (m010001_*)",                  â”‚
     â”‚             "Most recent commits (last 7 days)",                â”‚
     â”‚             "30 files with DAG orchestration",                  â”‚
     â”‚             "Full test coverage (15 test files)"                â”‚
     â”‚           ],                                                    â”‚
     â”‚           "characteristics": {                                  â”‚
     â”‚             "files": 30,                                        â”‚
     â”‚             "lines_of_code": 5234,                              â”‚
     â”‚             "last_modified": "2025-12-01",                      â”‚
     â”‚             "import_references": 47,                            â”‚
     â”‚             "test_files": 15,                                   â”‚
     â”‚             "documentation": "High"                             â”‚
     â”‚           }                                                     â”‚
     â”‚         },                                                      â”‚
     â”‚         {                                                       â”‚
     â”‚           "path": "engine/",                                    â”‚
     â”‚           "score": 120,                                         â”‚
     â”‚           "ranking": 2,                                         â”‚
     â”‚           "status": "SECONDARY - REVIEW FOR ARCHIVAL",          â”‚
     â”‚           "strengths": [                                        â”‚
     â”‚             "Job queue specialization",                         â”‚
     â”‚             "Worker pool implementation",                       â”‚
     â”‚             "Escalation management"                             â”‚
     â”‚           ],                                                    â”‚
     â”‚           "concerns": [                                         â”‚
     â”‚             "Not in CODEBASE_INDEX canonical paths",            â”‚
     â”‚             "No ULID prefixes",                                 â”‚
     â”‚             "Older codebase (6 months since major update)",     â”‚
     â”‚             "Lower import reference count (12 vs 47)",          â”‚
     â”‚             "Partial test coverage"                             â”‚
     â”‚           ],                                                    â”‚
     â”‚           "characteristics": {                                  â”‚
     â”‚             "files": 21,                                        â”‚
     â”‚             "lines_of_code": 3456,                              â”‚
     â”‚             "last_modified": "2025-06-15",                      â”‚
     â”‚             "import_references": 12,                            â”‚
     â”‚             "test_files": 5,                                    â”‚
     â”‚             "documentation": "Medium"                           â”‚
     â”‚           },                                                    â”‚
     â”‚           "recommendation": "ANALYZE: Determine if job queue    â”‚
     â”‚ functionality is unique or redundant with modules/core-engine/" â”‚
     â”‚         }                                                       â”‚
     â”‚       ],                                                        â”‚
     â”‚       "decision_needed": true,                                  â”‚
     â”‚       "question": "Does engine/ provide unique job queue        â”‚
     â”‚ functionality not in modules/core-engine/?"                     â”‚
     â”‚     }                                                           â”‚
     â”‚   ]                                                             â”‚
     â”‚ }                                                               â”‚
     â”‚                                                                 â”‚
     â”‚ Manual Review Aid: Generate checklist for user:                 â”‚
     â”‚ [ ] Orchestration engines: modules/core-engine/ (PRIMARY) vs    â”‚
     â”‚ engine/ (SECONDARY)                                             â”‚
     â”‚     - Review: Does engine/ have unique job queue features?      â”‚
     â”‚     - Action if redundant: Archive engine/ to                   â”‚
     â”‚ archive/2025-12-02_HHMMSS_engine-superseded/                    â”‚
     â”‚     - Action if unique: Keep both, add clarifying README to     â”‚
     â”‚ engine/                                                         â”‚
     â”‚                                                                 â”‚
     â”‚ [ ] Error systems: modules/error-engine/ (PRIMARY) vs           â”‚
     â”‚ UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/error/ (SECONDARY)      â”‚
     â”‚     - Review: Framework vs application-level separation?        â”‚
     â”‚     - Decision: [Keep both / Archive framework version / Merge] â”‚
     â”‚                                                                 â”‚
     â”‚ [ ] State management: modules/core-state/ (PRIMARY) vs multiple â”‚
     â”‚ others                                                          â”‚
     â”‚     - Review: Different persistence strategies?                 â”‚
     â”‚     - Decision: [Consolidate / Keep specialized versions]       â”‚
     â”‚                                                                 â”‚
     â”‚ Phase 3: Master Orchestration Script                            â”‚
     â”‚                                                                 â”‚
     â”‚ Step 3.1: Create Comprehensive Analyzer                         â”‚
     â”‚                                                                 â”‚
     â”‚ Create: scripts/comprehensive_archival_analyzer.py              â”‚
     â”‚                                                                 â”‚
     â”‚ Purpose: Orchestrate all analysis tools and generate unified    â”‚
     â”‚ report                                                          â”‚
     â”‚                                                                 â”‚
     â”‚ Execution Flow:                                                 â”‚
     â”‚ def run_comprehensive_analysis():                               â”‚
     â”‚     """                                                         â”‚
     â”‚     Step 1: Run individual analyzers in sequence                â”‚
     â”‚     - analyze_cleanup_candidates.py (enhanced with 6 signals)   â”‚
     â”‚     - entry_point_reachability.py                               â”‚
     â”‚     - test_coverage_archival.py                                 â”‚
     â”‚     - detect_parallel_implementations.py                        â”‚
     â”‚     - analyze_imports.py (for deprecated import tracking)       â”‚
     â”‚                                                                 â”‚
     â”‚     Step 2: Aggregate results                                   â”‚
     â”‚     - Combine all signals into unified scoring                  â”‚
     â”‚     - Apply user preferences (90% threshold, 90-day staleness)  â”‚
     â”‚     - Filter out .md and .txt files                             â”‚
     â”‚                                                                 â”‚
     â”‚     Step 3: Generate tiered reports                             â”‚
     â”‚     - TIER_1 (90-100%): High confidence archival candidates     â”‚
     â”‚     - TIER_2 (75-89%): Review recommended                       â”‚
     â”‚     - TIER_3 (60-74%): Manual expert review needed              â”‚
     â”‚     - TIER_4 (<60%): Keep                                       â”‚
     â”‚                                                                 â”‚
     â”‚     Step 4: Generate execution artifacts                        â”‚
     â”‚     - archival_plan_tier1_automated.ps1 (90%+ confidence)       â”‚
     â”‚     - archival_plan_tier2_review.json (75-89%)                  â”‚
     â”‚     - parallel_implementations_decision_checklist.md            â”‚
     â”‚     - validation_checklist.md                                   â”‚
     â”‚     """                                                         â”‚
     â”‚                                                                 â”‚
     â”‚ Command Line:                                                   â”‚
     â”‚ python scripts/comprehensive_archival_analyzer.py \             â”‚
     â”‚   --confidence-threshold 90 \                                   â”‚
     â”‚   --staleness-days 90 \                                         â”‚
     â”‚   --exclude-extensions .md,.txt \                               â”‚
     â”‚   --output-dir cleanup_reports/                                 â”‚
     â”‚                                                                 â”‚
     â”‚ Outputs:                                                        â”‚
     â”‚ 1. comprehensive_archival_report.json - Full analysis           â”‚
     â”‚ 2. archival_plan_tier1_automated.ps1 - 90%+ confidence script   â”‚
     â”‚ 3. archival_plan_tier2_review.json - 75-89% review list         â”‚
     â”‚ 4. archival_plan_tier3_manual.json - 60-74% expert review       â”‚
     â”‚ 5. parallel_implementations_decision_checklist.md - Human       â”‚
     â”‚ decision aid                                                    â”‚
     â”‚ 6. validation_checklist.md - Pre/post archive validation steps  â”‚
     â”‚                                                                 â”‚
     â”‚ Phase 4: Validation Framework                                   â”‚
     â”‚                                                                 â”‚
     â”‚ Step 4.1: Create Validation Script                              â”‚
     â”‚                                                                 â”‚
     â”‚ Create: scripts/validate_archival_safety.py                     â”‚
     â”‚                                                                 â”‚
     â”‚ Purpose: Ensure archival is safe before and after execution     â”‚
     â”‚                                                                 â”‚
     â”‚ Pre-Archive Validation Checks:                                  â”‚
     â”‚ def validate_pre_archive(files_to_archive: List[Path]):         â”‚
     â”‚     """                                                         â”‚
     â”‚     1. Import Validation                                        â”‚
     â”‚        - Ensure no active code imports files to be archived     â”‚
     â”‚        - Check for dynamic imports (importlib.import_module)    â”‚
     â”‚        - Verify no UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK      â”‚
     â”‚ pattern references                                              â”‚
     â”‚                                                                 â”‚
     â”‚     2. Test Suite Validation                                    â”‚
     â”‚        - Run pytest with files temporarily moved (dry-run)      â”‚
     â”‚        - Ensure all valid tests pass (current: 458 valid tests) â”‚
     â”‚        - Check for import errors                                â”‚
     â”‚                                                                 â”‚
     â”‚     3. Entry Point Validation                                   â”‚
     â”‚        - Verify all __main__ blocks still accessible            â”‚
     â”‚        - Check CLI tools remain functional                      â”‚
     â”‚        - Validate scripts/ executables                          â”‚
     â”‚                                                                 â”‚
     â”‚     4. Git Status Check                                         â”‚
     â”‚        - No uncommitted changes                                 â”‚
     â”‚        - Not in active worktrees                                â”‚
     â”‚        - Files not in .gitignore                                â”‚
     â”‚                                                                 â”‚
     â”‚     5. Canonical Path Validation                                â”‚
     â”‚        - Ensure CODEBASE_INDEX.yaml canonical paths unaffected  â”‚
     â”‚        - Verify PIPELINE_STAGE_MODULE integrity                 â”‚
     â”‚        - Check FEATURE_SERVICE_MODULE not broken                â”‚
     â”‚                                                                 â”‚
     â”‚     Return: (is_safe: bool, blockers: List[str], warnings:      â”‚
     â”‚ List[str])                                                      â”‚
     â”‚     """                                                         â”‚
     â”‚                                                                 â”‚
     â”‚ Post-Archive Validation:                                        â”‚
     â”‚ def validate_post_archive():                                    â”‚
     â”‚     """                                                         â”‚
     â”‚     1. Test Suite: All 458 valid tests pass                     â”‚
     â”‚     2. Import Check: No new import errors                       â”‚
     â”‚     3. Entry Points: All functional                             â”‚
     â”‚     4. Functionality: No regression detected                    â”‚
     â”‚     5. Archive Recovery: Archive can be restored                â”‚
     â”‚                                                                 â”‚
     â”‚     Return: (is_valid: bool, failures: List[str])               â”‚
     â”‚     """                                                         â”‚
     â”‚                                                                 â”‚
     â”‚ Usage:                                                          â”‚
     â”‚ # Before archival                                               â”‚
     â”‚ python scripts/validate_archival_safety.py --mode pre-archive   â”‚
     â”‚ --files-list tier1_files.json                                   â”‚
     â”‚                                                                 â”‚
     â”‚ # After archival                                                â”‚
     â”‚ python scripts/validate_archival_safety.py --mode post-archive  â”‚
     â”‚                                                                 â”‚
     â”‚ Phase 5: Execution Workflow                                     â”‚
     â”‚                                                                 â”‚
     â”‚ Recommended Execution Steps                                     â”‚
     â”‚                                                                 â”‚
     â”‚ Step 1: Run Comprehensive Analysis (15-20 minutes)              â”‚
     â”‚ # Run full analysis                                             â”‚
     â”‚ python scripts/comprehensive_archival_analyzer.py `             â”‚
     â”‚   --confidence-threshold 90 `                                   â”‚
     â”‚   --staleness-days 90 `                                         â”‚
     â”‚   --exclude-extensions .md,.txt `                               â”‚
     â”‚   --output-dir cleanup_reports/                                 â”‚
     â”‚                                                                 â”‚
     â”‚ # Review outputs                                                â”‚
     â”‚ ls cleanup_reports/                                             â”‚
     â”‚                                                                 â”‚
     â”‚ Step 2: Review Parallel Implementations (30-60 minutes)         â”‚
     â”‚ # Open decision checklist                                       â”‚
     â”‚ code                                                            â”‚
     â”‚ cleanup_reports/parallel_implementations_decision_checklist.md  â”‚
     â”‚                                                                 â”‚
     â”‚ # For each overlap group:                                       â”‚
     â”‚ # 1. Read comparison details                                    â”‚
     â”‚ # 2. Decide: Archive secondary / Keep both / Needs investigationâ”‚
     â”‚ # 3. Document decision in checklist                             â”‚
     â”‚                                                                 â”‚
     â”‚ Step 3: Review Tier 1 Candidates (10-15 minutes)                â”‚
     â”‚ # Review high-confidence archival list                          â”‚
     â”‚ python -c "import json;                                         â”‚
     â”‚ print(json.dumps(json.load(open('cleanup_reports/comprehensive_aâ”‚
     â”‚ rchival_report.json'))['tier_1_summary'], indent=2))"           â”‚
     â”‚                                                                 â”‚
     â”‚ # Check automated script                                        â”‚
     â”‚ code cleanup_reports/archival_plan_tier1_automated.ps1          â”‚
     â”‚                                                                 â”‚
     â”‚ Step 4: Pre-Archive Validation (5 minutes)                      â”‚
     â”‚ # Validate safety                                               â”‚
     â”‚ python scripts/validate_archival_safety.py --mode pre-archive   â”‚
     â”‚                                                                 â”‚
     â”‚ # Review validation report                                      â”‚
     â”‚ code cleanup_reports/validation_pre_archive.json                â”‚
     â”‚                                                                 â”‚
     â”‚ # If blockers exist, resolve them before proceeding             â”‚
     â”‚                                                                 â”‚
     â”‚ Step 5: Execute Tier 1 Archival (DRY RUN FIRST)                 â”‚
     â”‚ # DRY RUN - See what would happen                               â”‚
     â”‚ cd cleanup_reports                                              â”‚
     â”‚ .\archival_plan_tier1_automated.ps1  # $DryRun = $true by       â”‚
     â”‚ default                                                         â”‚
     â”‚                                                                 â”‚
     â”‚ # Review dry run output carefully                               â”‚
     â”‚ # If satisfied, execute for real:                               â”‚
     â”‚ # Edit script: Set $DryRun = $false                             â”‚
     â”‚ .\archival_plan_tier1_automated.ps1  # Execute archival         â”‚
     â”‚                                                                 â”‚
     â”‚ Step 6: Post-Archive Validation (5 minutes)                     â”‚
     â”‚ # Validate archival was safe                                    â”‚
     â”‚ python scripts/validate_archival_safety.py --mode post-archive  â”‚
     â”‚                                                                 â”‚
     â”‚ # Run test suite                                                â”‚
     â”‚ python -m pytest                                                â”‚
     â”‚                                                                 â”‚
     â”‚ # Check for issues                                              â”‚
     â”‚ git status                                                      â”‚
     â”‚                                                                 â”‚
     â”‚ Step 7: Commit Archival (if validation passes)                  â”‚
     â”‚ $Timestamp = Get-Date -Format 'yyyy-MM-dd_HHmmss'               â”‚
     â”‚                                                                 â”‚
     â”‚ git add .                                                       â”‚
     â”‚ git commit -m "chore: Archive obsolete Python code (Tier 1 -    â”‚
     â”‚ 90%+ confidence)                                                â”‚
     â”‚                                                                 â”‚
     â”‚ Archived N files based on comprehensive 6-signal analysis:      â”‚
     â”‚ - Duplication: SHA-256 exact matches                            â”‚
     â”‚ - Staleness: 90+ days without modification                      â”‚
     â”‚ - Obsolescence: Superseded by canonical modules                 â”‚
     â”‚ - Isolation: Not imported by active code                        â”‚
     â”‚ - Reachability: Unreachable from entry points                   â”‚
     â”‚ - Test coverage: No test coverage                               â”‚
     â”‚                                                                 â”‚
     â”‚ Archive: archive/${Timestamp}_python-code-cleanup/              â”‚
     â”‚ Confidence: 90-100% safe to archive                             â”‚
     â”‚ Configuration: 90-day staleness, 90% threshold                  â”‚
     â”‚                                                                 â”‚
     â”‚ All validation checks passed:                                   â”‚
     â”‚ âœ“ Tests passing (458/458)                                       â”‚
     â”‚ âœ“ No import errors                                              â”‚
     â”‚ âœ“ Entry points functional                                       â”‚
     â”‚ âœ“ Git history preserved                                         â”‚
     â”‚                                                                 â”‚
     â”‚ ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code) â”‚
     â”‚                                                                 â”‚
     â”‚ Co-Authored-By: Claude <noreply@anthropic.com>"                 â”‚
     â”‚                                                                 â”‚
     â”‚ Step 8: Process Tier 2 (Optional - Manual Review)               â”‚
     â”‚ # Review Tier 2 candidates (75-89% confidence)                  â”‚
     â”‚ code cleanup_reports/archival_plan_tier2_review.json            â”‚
     â”‚                                                                 â”‚
     â”‚ # For each file:                                                â”‚
     â”‚ # 1. Review evidence                                            â”‚
     â”‚ # 2. Approve or reject                                          â”‚
     â”‚ # 3. Create custom script for approved files                    â”‚
     â”‚ # 4. Execute with validation                                    â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Archive Structure                                               â”‚
     â”‚                                                                 â”‚
     â”‚ Standard Archive Format                                         â”‚
     â”‚                                                                 â”‚
     â”‚ ./archive/2025-12-02_HHMMSS_python-code-cleanup/                â”‚
     â”‚ â”œâ”€â”€ ARCHIVE_MANIFEST.json           # Metadata and restoration  â”‚
     â”‚ instructions                                                    â”‚
     â”‚ â”œâ”€â”€ ARCHIVAL_ANALYSIS_REPORT.json   # Full analysis with all    â”‚
     â”‚ signals                                                         â”‚
     â”‚ â”œâ”€â”€ README.md                        # Human-readable summary   â”‚
     â”‚ â”œâ”€â”€ modules/                         # Preserved directory      â”‚
     â”‚ structure                                                       â”‚
     â”‚ â”‚   â”œâ”€â”€ old-module/                                             â”‚
     â”‚ â”‚   â””â”€â”€ ...                                                     â”‚
     â”‚ â”œâ”€â”€ engine/                          # If engine/ is archived   â”‚
     â”‚ â”‚   â””â”€â”€ ...                                                     â”‚
     â”‚ â””â”€â”€ scripts/                                                    â”‚
     â”‚     â””â”€â”€ obsolete_scripts/                                       â”‚
     â”‚                                                                 â”‚
     â”‚ Manifest Format                                                 â”‚
     â”‚                                                                 â”‚
     â”‚ {                                                               â”‚
     â”‚   "archive_metadata": {                                         â”‚
     â”‚     "timestamp": "2025-12-02T14:30:22Z",                        â”‚
     â”‚     "confidence_tier": "TIER_1_SAFE_ARCHIVE",                   â”‚
     â”‚     "created_by": "comprehensive_archival_analyzer.py v1.0",    â”‚
     â”‚     "user_config": {                                            â”‚
     â”‚       "confidence_threshold": 90,                               â”‚
     â”‚       "staleness_days": 90,                                     â”‚
     â”‚       "excluded_extensions": [".md", ".txt"]                    â”‚
     â”‚     }                                                           â”‚
     â”‚   },                                                            â”‚
     â”‚   "statistics": {                                               â”‚
     â”‚     "files_archived": 87,                                       â”‚
     â”‚     "total_size_bytes": 245760,                                 â”‚
     â”‚     "space_saved_mb": 0.23                                      â”‚
     â”‚   },                                                            â”‚
     â”‚   "validation": {                                               â”‚
     â”‚     "pre_archive_tests_passed": true,                           â”‚
     â”‚     "post_archive_tests_passed": true,                          â”‚
     â”‚     "import_validation": "PASSED",                              â”‚
     â”‚     "entry_point_validation": "PASSED"                          â”‚
     â”‚   },                                                            â”‚
     â”‚   "signals_summary": {                                          â”‚
     â”‚     "avg_duplication_score": 45,                                â”‚
     â”‚     "avg_staleness_score": 92,                                  â”‚
     â”‚     "avg_obsolescence_score": 78,                               â”‚
     â”‚     "avg_isolation_score": 85,                                  â”‚
     â”‚     "avg_reachability_score": 88,                               â”‚
     â”‚     "avg_test_coverage_score": 90                               â”‚
     â”‚   },                                                            â”‚
     â”‚   "files": [                                                    â”‚
     â”‚     {                                                           â”‚
     â”‚       "original_path": "engine/old_orchestrator.py",            â”‚
     â”‚       "confidence": 95,                                         â”‚
     â”‚       "composite_score": 91,                                    â”‚
     â”‚       "signals": {                                              â”‚
     â”‚         "duplication": 0,                                       â”‚
     â”‚         "staleness": 95,                                        â”‚
     â”‚         "obsolescence": 90,                                     â”‚
     â”‚         "isolation": 100,                                       â”‚
     â”‚         "reachability": 100,                                    â”‚
     â”‚         "test_coverage": 95                                     â”‚
     â”‚       },                                                        â”‚
     â”‚       "reasons": [                                              â”‚
     â”‚         "Superseded by                                          â”‚
     â”‚ modules/core-engine/m010001_orchestrator.py",                   â”‚
     â”‚         "Not modified in 120 days (threshold: 90 days)",        â”‚
     â”‚         "Not imported by active code",                          â”‚
     â”‚         "Unreachable from any entry point",                     â”‚
     â”‚         "No test coverage"                                      â”‚
     â”‚       ],                                                        â”‚
     â”‚       "superseded_by":                                          â”‚
     â”‚ "modules/core-engine/m010001_orchestrator.py"                   â”‚
     â”‚     }                                                           â”‚
     â”‚   ],                                                            â”‚
     â”‚   "parallel_implementations_decisions": [                       â”‚
     â”‚     {                                                           â”‚
     â”‚       "overlap_group": "orchestration_engines",                 â”‚
     â”‚       "primary": "modules/core-engine/",                        â”‚
     â”‚       "archived": ["engine/"],                                  â”‚
     â”‚       "rationale": "engine/ job queue functionality merged into â”‚
     â”‚ modules/core-engine/scheduler.py"                               â”‚
     â”‚     }                                                           â”‚
     â”‚   ],                                                            â”‚
     â”‚   "restoration_instructions": {                                 â”‚
     â”‚     "command": "python scripts/restore_from_archive.py          â”‚
     â”‚ --archive-id 2025-12-02_143022_python-code-cleanup",            â”‚
     â”‚     "notes": "Restoration will move files back to original      â”‚
     â”‚ locations and run validation"                                   â”‚
     â”‚   }                                                             â”‚
     â”‚ }                                                               â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Expected Outcomes                                               â”‚
     â”‚                                                                 â”‚
     â”‚ Signal Effectiveness (Estimated)                                â”‚
     â”‚                                                                 â”‚
     â”‚ Based on repository analysis, expected archival candidates by   â”‚
     â”‚ signal:                                                         â”‚
     â”‚                                                                 â”‚
     â”‚ | Signal        | Threshold                     | Est. Files    â”‚
     â”‚ Flagged | Rationale                            |                â”‚
     â”‚ |---------------|-------------------------------|---------------â”‚
     â”‚ -----|--------------------------------------|                   â”‚
     â”‚ | Duplication   | SHA-256 match                 | 10-15         â”‚
     â”‚      | Exact duplicates across archive dirs |                   â”‚
     â”‚ | Staleness     | 90+ days                      | 200-250       â”‚
     â”‚      | Aggressive 3-month threshold         |                   â”‚
     â”‚ | Obsolescence  | Deprecated patterns           | 60-70         â”‚
     â”‚      | Archive dirs, version suffixes       |                   â”‚
     â”‚ | Isolation     | Not imported                  | 40-50         â”‚
     â”‚      | Orphaned modules                     |                   â”‚
     â”‚ | Reachability  | Unreachable from entry points | 50-60         â”‚
     â”‚      | Dead code paths                      |                   â”‚
     â”‚ | Test Coverage | No tests + stale              | 120-150       â”‚
     â”‚      | Untested + old = abandoned           |                   â”‚
     â”‚                                                                 â”‚
     â”‚ Overlap (Signal Convergence):                                   â”‚
     â”‚ - Expected 80-100 files with 3+ high signals (90%+ confidence)  â”‚
     â”‚ - Expected 100-150 files with 2 signals (75-89% confidence)     â”‚
     â”‚                                                                 â”‚
     â”‚ By Tier (Estimated)                                             â”‚
     â”‚                                                                 â”‚
     â”‚ - Tier 1 (90-100%): 80-100 files - Automated archival safe      â”‚
     â”‚   - Exact duplicates: ~15 files                                 â”‚
     â”‚   - Stale + not imported + no tests: ~40 files                  â”‚
     â”‚   - In archive directories already: ~10 files                   â”‚
     â”‚   - Superseded by canonical modules: ~25 files                  â”‚
     â”‚ - Tier 2 (75-89%): 100-150 files - Review recommended           â”‚
     â”‚   - Stale + deprecated patterns: ~50 files                      â”‚
     â”‚   - Parallel implementation secondaries: ~30 files              â”‚
     â”‚   - Orphaned with unclear purpose: ~40 files                    â”‚
     â”‚ - Tier 3 (60-74%): 80-100 files - Manual expert review          â”‚
     â”‚   - Recent but isolated: ~30 files                              â”‚
     â”‚   - Low test coverage but used: ~40 files                       â”‚
     â”‚   - Unclear parallel impl status: ~20 files                     â”‚
     â”‚ - Tier 4 (<60%): 600-700 files - Keep                           â”‚
     â”‚   - Active, tested, well-integrated code                        â”‚
     â”‚                                                                 â”‚
     â”‚ Space Savings                                                   â”‚
     â”‚                                                                 â”‚
     â”‚ - Estimated 0.3-0.5 MB from Tier 1 archival                     â”‚
     â”‚ - Potential 1.0-1.5 MB if Tier 2 is processed                   â”‚
     â”‚ - Primary benefit: Clarity and reduced cognitive load for       â”‚
     â”‚ developers                                                      â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Critical Files to Modify                                        â”‚
     â”‚                                                                 â”‚
     â”‚ Implementation requires changes to these files:                 â”‚
     â”‚                                                                 â”‚
     â”‚ 1. scripts/analyze_cleanup_candidates.py (MODIFY)               â”‚
     â”‚   - Update STALENESS_DAYS = 90 (line 54)                        â”‚
     â”‚   - Update CONFIDENCE_THRESHOLD = 90 in main() (line 732)       â”‚
     â”‚   - Add FileScore.reachability_score field (line 85)            â”‚
     â”‚   - Add FileScore.test_coverage_score field (line 86)           â”‚
     â”‚   - Modify score_file() to integrate new signals (line 396)     â”‚
     â”‚   - Add should_analyze_file() filter for .md/.txt (new method)  â”‚
     â”‚ 2. scripts/entry_point_reachability.py (CREATE NEW)             â”‚
     â”‚   - Implement entry point discovery                             â”‚
     â”‚   - Build import graph with BFS traversal                       â”‚
     â”‚   - Generate reachability scores                                â”‚
     â”‚   - Output: entry_point_reachability_report.json                â”‚
     â”‚ 3. scripts/test_coverage_archival.py (CREATE NEW)               â”‚
     â”‚   - Scan test directories                                       â”‚
     â”‚   - Build module -> tests mapping                               â”‚
     â”‚   - Score untested files                                        â”‚
     â”‚   - Output: test_coverage_archival_report.json                  â”‚
     â”‚ 4. scripts/detect_parallel_implementations.py (CREATE NEW)      â”‚
     â”‚   - Load FOLDER_CLASSIFICATION.yaml                             â”‚
     â”‚   - Identify overlap groups                                     â”‚
     â”‚   - Score implementations with 200-point system                 â”‚
     â”‚   - Generate comparison report                                  â”‚
     â”‚   - Output: parallel_implementations_analysis.json              â”‚
     â”‚ 5. scripts/comprehensive_archival_analyzer.py (CREATE NEW)      â”‚
     â”‚   - Orchestrate all analyzers                                   â”‚
     â”‚   - Aggregate signals                                           â”‚
     â”‚   - Generate tiered reports                                     â”‚
     â”‚   - Create execution scripts                                    â”‚
     â”‚ 6. scripts/validate_archival_safety.py (CREATE NEW)             â”‚
     â”‚   - Pre-archive validation (import checks, test runs, entry     â”‚
     â”‚ points)                                                         â”‚
     â”‚   - Post-archive validation                                     â”‚
     â”‚   - Output: validation_*.json reports                           â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Risk Mitigation                                                 â”‚
     â”‚                                                                 â”‚
     â”‚ Safety Mechanisms                                               â”‚
     â”‚                                                                 â”‚
     â”‚ 1. Conservative Threshold (90%+)                                â”‚
     â”‚   - Only archive files with overwhelming evidence               â”‚
     â”‚   - Virtually no risk of archiving active code                  â”‚
     â”‚ 2. Multi-Signal Convergence                                     â”‚
     â”‚   - Require 3+ signals at 80+ for high confidence               â”‚
     â”‚   - No single signal can trigger archival alone                 â”‚
     â”‚ 3. Dry-Run Mode                                                 â”‚
     â”‚   - All scripts default to $DryRun = $true                      â”‚
     â”‚   - Review before execution                                     â”‚
     â”‚ 4. Validation Gates                                             â”‚
     â”‚   - Pre-archive: Block if tests fail, imports break, entry      â”‚
     â”‚ points affected                                                 â”‚
     â”‚   - Post-archive: Automatic validation with rollback option     â”‚
     â”‚ 5. Git Preservation                                             â”‚
     â”‚   - Files moved (not deleted) - full git history intact         â”‚
     â”‚   - Archive directory timestamped and documented                â”‚
     â”‚ 6. Tiered Approach                                              â”‚
     â”‚   - Process Tier 1 first, validate, then decide on Tier 2       â”‚
     â”‚   - Never auto-execute uncertain cases                          â”‚
     â”‚ 7. Parallel Implementation Analysis                             â”‚
     â”‚   - Detailed comparison before any archival                     â”‚
     â”‚   - User decision required for competing implementations        â”‚
     â”‚                                                                 â”‚
     â”‚ Rollback Procedure                                              â”‚
     â”‚                                                                 â”‚
     â”‚ If issues arise after archival:                                 â”‚
     â”‚                                                                 â”‚
     â”‚ # Option 1: Git revert                                          â”‚
     â”‚ git revert HEAD  # Undo the archival commit                     â”‚
     â”‚                                                                 â”‚
     â”‚ # Option 2: Restore from archive                                â”‚
     â”‚ python scripts/restore_from_archive.py --archive-id             â”‚
     â”‚ 2025-12-02_143022_python-code-cleanup                           â”‚
     â”‚                                                                 â”‚
     â”‚ # Option 3: Manual restore (if needed)                          â”‚
     â”‚ $ArchiveDir = "archive/2025-12-02_143022_python-code-cleanup"   â”‚
     â”‚ $OriginalRoot = "."                                             â”‚
     â”‚                                                                 â”‚
     â”‚ # Move files back (preserves structure)                         â”‚
     â”‚ Get-ChildItem -Path $ArchiveDir -Recurse -File | ForEach-Object â”‚
     â”‚ {                                                               â”‚
     â”‚     $RelativePath = $_.FullName.Substring($ArchiveDir.Length +  â”‚
     â”‚ 1)                                                              â”‚
     â”‚     $DestPath = Join-Path $OriginalRoot $RelativePath           â”‚
     â”‚     New-Item -ItemType Directory -Path (Split-Path $DestPath)   â”‚
     â”‚ -Force                                                          â”‚
     â”‚     Move-Item -Path $_.FullName -Destination $DestPath -Force   â”‚
     â”‚ }                                                               â”‚
     â”‚                                                                 â”‚
     â”‚ # Validate restoration                                          â”‚
     â”‚ python -m pytest                                                â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Success Criteria                                                â”‚
     â”‚                                                                 â”‚
     â”‚ Quantitative Metrics                                            â”‚
     â”‚                                                                 â”‚
     â”‚ - 90%+ of Tier 1 candidates safely archived                     â”‚
     â”‚ - 0 test failures post-archive                                  â”‚
     â”‚ - 0 import errors post-archive                                  â”‚
     â”‚ - All 458 valid tests passing                                   â”‚
     â”‚ - Archive restoration tested successfully                       â”‚
     â”‚ - Git history fully preserved                                   â”‚
     â”‚ - Parallel implementations documented with clear decisions      â”‚
     â”‚                                                                 â”‚
     â”‚ Qualitative Outcomes                                            â”‚
     â”‚                                                                 â”‚
     â”‚ - Repository structure clearer (canonical paths obvious)        â”‚
     â”‚ - No confusion about which implementation to use                â”‚
     â”‚ - Developer confidence in cleanup process                       â”‚
     â”‚ - Comprehensive audit trail for all archival decisions          â”‚
     â”‚ - Reusable framework for future cleanups                        â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Summary of Enhancements Over Existing Tool                      â”‚
     â”‚                                                                 â”‚
     â”‚ | Aspect         | Existing Tool                                â”‚
     â”‚        | Enhanced Plan                            |             â”‚
     â”‚ |----------------|----------------------------------------------â”‚
     â”‚ -------|------------------------------------------|             â”‚
     â”‚ | Signals        | 4 (duplication, staleness, obsolescence,     â”‚
     â”‚ isolation) | 6 (+ reachability, test coverage)        |         â”‚
     â”‚ | Staleness      | 180 days                                     â”‚
     â”‚        | 90 days (user preference)                |             â”‚
     â”‚ | Confidence     | 85% threshold                                â”‚
     â”‚        | 90% threshold (conservative)             |             â”‚
     â”‚ | File Filtering | All code files                               â”‚
     â”‚        | Excludes .md and .txt (user requirement) |             â”‚
     â”‚ | Parallel Impl  | Not analyzed                                 â”‚
     â”‚        | Detailed comparison with scoring         |             â”‚
     â”‚ | Validation     | Basic                                        â”‚
     â”‚        | Comprehensive pre/post validation        |             â”‚
     â”‚ | Execution      | Single-tier                                  â”‚
     â”‚        | Tiered (3 levels) with manual review aid |             â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Timeline Estimate                                               â”‚
     â”‚                                                                 â”‚
     â”‚ - Phase 1 (Enhance analyzer): 6-8 hours                         â”‚
     â”‚   - Entry point reachability: 2-3 hours                         â”‚
     â”‚   - Test coverage analysis: 2-3 hours                           â”‚
     â”‚   - Modify main analyzer: 2 hours                               â”‚
     â”‚ - Phase 2 (Parallel impl detector): 3-4 hours                   â”‚
     â”‚   - Implementation comparison: 2-3 hours                        â”‚
     â”‚   - Report generation: 1 hour                                   â”‚
     â”‚ - Phase 3 (Master orchestrator): 2-3 hours                      â”‚
     â”‚   - Aggregate analyzers: 1-2 hours                              â”‚
     â”‚   - Generate tiered reports: 1 hour                             â”‚
     â”‚ - Phase 4 (Validation): 2-3 hours                               â”‚
     â”‚   - Pre-archive validation: 1-2 hours                           â”‚
     â”‚   - Post-archive validation: 1 hour                             â”‚
     â”‚ - Phase 5 (Execution & Review): 1-2 hours                       â”‚
     â”‚   - Run analysis: 20 minutes                                    â”‚
     â”‚   - Review reports: 30 minutes                                  â”‚
     â”‚   - Execute Tier 1: 10-20 minutes                               â”‚
     â”‚   - Validation: 10 minutes                                      â”‚
     â”‚                                                                 â”‚
     â”‚ Total Effort: 14-20 hours of implementation + 1-2 hours         â”‚
     â”‚ execution                                                       â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Next Steps After Plan Approval                                  â”‚
     â”‚                                                                 â”‚
     â”‚ 1. Modify scripts/analyze_cleanup_candidates.py with new        â”‚
     â”‚ configuration                                                   â”‚
     â”‚ 2. Create scripts/entry_point_reachability.py                   â”‚
     â”‚ 3. Create scripts/test_coverage_archival.py                     â”‚
     â”‚ 4. Create scripts/detect_parallel_implementations.py            â”‚
     â”‚ 5. Create scripts/comprehensive_archival_analyzer.py            â”‚
     â”‚ 6. Create scripts/validate_archival_safety.py                   â”‚
     â”‚ 7. Run comprehensive analysis                                   â”‚
     â”‚ 8. Review parallel implementation decisions                     â”‚
     â”‚ 9. Execute Tier 1 archival with validation                      â”‚
     â”‚ 10. Commit results with detailed commit message                 â”‚
     â”‚                                                                 â”‚
     â”‚ ---                                                             â”‚
     â”‚ Questions Before Implementation                                 â”‚
     â”‚                                                                 â”‚
     â”‚ None - all clarifications received from user:                   â”‚
     â”‚ - âœ… Confidence threshold: 90%+ (conservative)                  â”‚
     â”‚ - âœ… Staleness threshold: 90 days (aggressive)                  â”‚
     â”‚ - âœ… Parallel implementations: Detailed analysis first          â”‚
     â”‚ - âœ… Tool enhancement: Add new signals                          â”‚
     â”‚                                                                 â”‚
     â”‚ Ready to proceed with implementation upon approval.             â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â”‚                                                                 â”‚
     â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
>
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â¸ plan mode on (shift+tab to cycle)
