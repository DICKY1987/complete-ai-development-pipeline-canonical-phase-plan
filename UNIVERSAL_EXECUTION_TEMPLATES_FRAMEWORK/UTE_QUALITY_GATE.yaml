# QUALITY_GATE.yaml
# Quality gate definitions for the Universal Execution Templates Framework
# Purpose: Define all validation checkpoints and their pass criteria

version: "1.0.0"
generated: "2025-11-23"
framework: "Universal Execution Templates (UET)"

# ==============================================================================
# TEST GATES
# Core test suites that must pass
# ==============================================================================

test_gates:
  # Schema validation tests
  schema_tests:
    name: "Schema Validation Tests"
    description: "Validate all JSON schemas and artifact conformance"
    command: "pytest tests/schema/ -v"
    timeout_seconds: 10
    expected_tests: 22
    pass_criteria: "22/22 passing"
    coverage_target: 100
    
    validates:
      - "All 17 JSON schemas have valid syntax"
      - "Example artifacts conform to schemas"
      - "Schema references resolve correctly"
    
    on_failure:
      - "Check schema/*.json for syntax errors"
      - "Validate example artifacts against schemas"
      - "Review schema/README.md for validation guide"
    
    tags: ["schema", "validation", "foundation"]
  
  # Bootstrap system tests
  bootstrap_tests:
    name: "Bootstrap System Tests"
    description: "Validate project discovery and configuration generation"
    command: "pytest tests/bootstrap/ -v"
    timeout_seconds: 30
    expected_tests: 8
    pass_criteria: "8/8 passing"
    coverage_target: 80
    
    validates:
      - "ProjectScanner discovers project structure correctly"
      - "ProfileSelector chooses appropriate profile"
      - "ArtifactGenerator creates valid PROJECT_PROFILE.yaml"
      - "BootstrapOrchestrator runs full workflow"
    
    on_failure:
      - "Check core/bootstrap/*.py for logic errors"
      - "Verify profiles/ templates are valid"
      - "Review bootstrap_discovery.v1.json schema"
    
    tags: ["bootstrap", "discovery", "domain"]
  
  # Execution engine tests
  engine_tests:
    name: "Execution Engine Tests"
    description: "Validate orchestration, scheduling, routing, and state management"
    command: "pytest tests/engine/ -v"
    timeout_seconds: 60
    expected_tests: 92
    pass_criteria: "92/92 passing"
    coverage_target: 80
    
    validates:
      - "Orchestrator creates and manages runs"
      - "ExecutionScheduler resolves dependencies and detects cycles"
      - "TaskRouter matches tasks to tools via capabilities"
      - "RunStateMachine enforces valid state transitions"
      - "ExecutionRequestBuilder validates Phase constraints"
    
    on_failure:
      - "Check core/engine/*.py for logic errors"
      - "Review execution_request.v1.json schema"
      - "Verify state machine transitions in state_machine.py"
    
    tags: ["engine", "orchestration", "scheduling", "routing", "domain"]
  
  # Tool adapter tests
  adapter_tests:
    name: "Tool Adapter Tests"
    description: "Validate tool integration and adapter registry"
    command: "pytest tests/adapters/ -v"
    timeout_seconds: 30
    expected_tests: 27
    pass_criteria: "27/27 passing"
    coverage_target: 80
    
    validates:
      - "AdapterRegistry loads router_config.json"
      - "SubprocessAdapter executes CLI tools correctly"
      - "Capability matching routes tasks to appropriate tools"
      - "Timeout and parallelism limits enforced"
    
    on_failure:
      - "Check core/adapters/*.py for logic errors"
      - "Verify router_config.v1.json schema"
      - "Review adapter base class in adapters/base.py"
    
    tags: ["adapters", "tools", "integration", "domain"]
  
  # Resilience pattern tests
  resilience_tests:
    name: "Resilience Pattern Tests"
    description: "Validate circuit breaker, retry logic, and fault tolerance"
    command: "pytest tests/resilience/ -v"
    timeout_seconds: 45
    expected_tests: 32
    pass_criteria: "32/32 passing"
    coverage_target: 85
    
    validates:
      - "CircuitBreaker transitions: CLOSED → OPEN → HALF_OPEN"
      - "RetryStrategy implements exponential backoff"
      - "ResilientExecutor combines circuit breaker + retry"
      - "Failure tracking per tool"
      - "Recovery timeout behavior"
    
    on_failure:
      - "Check core/engine/resilience/*.py for logic errors"
      - "Verify circuit breaker state transitions"
      - "Review retry backoff calculations"
    
    tags: ["resilience", "fault-tolerance", "circuit-breaker", "retry", "domain"]
  
  # Monitoring tests
  monitoring_tests:
    name: "Monitoring Tests"
    description: "Validate progress tracking and run monitoring"
    command: "pytest tests/monitoring/ -v"
    timeout_seconds: 20
    expected_tests: 15
    pass_criteria: "15/15 passing"
    coverage_target: 80
    
    validates:
      - "ProgressTracker calculates completion percentage"
      - "ETA estimation works correctly"
      - "Task timing and duration tracking"
      - "RunMonitor aggregates statistics"
    
    on_failure:
      - "Check core/engine/monitoring/*.py for logic errors"
      - "Verify progress calculation algorithms"
      - "Review ProgressSnapshot data structure"
    
    tags: ["monitoring", "progress", "tracking", "domain"]

# ==============================================================================
# AGGREGATE GATES
# Combined test suites for different scenarios
# ==============================================================================

aggregate_gates:
  # Quick validation (run frequently during development)
  quick_validation:
    name: "Quick Validation"
    description: "Fast subset of tests for rapid feedback"
    command: "pytest tests/schema/ tests/engine/ -v -k 'not slow'"
    timeout_seconds: 45
    expected_tests: "~100"
    pass_criteria: "All tests passing"
    
    includes:
      - "schema_tests"
      - "engine_tests (excluding slow tests)"
    
    use_cases:
      - "After small code changes"
      - "Before git commit"
      - "During active development"
    
    tags: ["quick", "pre-commit"]
  
  # Full test suite (run before push/merge)
  full_suite:
    name: "Full Test Suite"
    description: "Complete test coverage across all modules"
    command: "pytest tests/ -v"
    timeout_seconds: 120
    expected_tests: 196
    pass_criteria: "196/196 passing (100%)"
    
    includes:
      - "schema_tests"
      - "bootstrap_tests"
      - "engine_tests"
      - "adapter_tests"
      - "resilience_tests"
      - "monitoring_tests"
    
    use_cases:
      - "Before git push"
      - "Before merge to main"
      - "Pre-release validation"
      - "CI pipeline"
    
    tags: ["full", "pre-push", "ci"]
  
  # Coverage check (run before release)
  coverage_check:
    name: "Coverage Check"
    description: "Full test suite with coverage reporting"
    command: "pytest tests/ --cov=core --cov-report=html --cov-report=term"
    timeout_seconds: 150
    expected_tests: 196
    pass_criteria: "196/196 passing + ≥80% coverage"
    
    coverage_targets:
      "core/bootstrap/": 80
      "core/engine/": 80
      "core/adapters/": 80
      "core/engine/resilience/": 85
      "core/engine/monitoring/": 80
      "core/state/": 75
    
    outputs:
      - "htmlcov/index.html (HTML report)"
      - "Terminal coverage summary"
    
    use_cases:
      - "Pre-release validation"
      - "Quarterly quality audits"
      - "After major refactors"
    
    tags: ["coverage", "quality", "pre-release"]

# ==============================================================================
# STATIC ANALYSIS GATES
# Code quality and style checks
# ==============================================================================

static_analysis_gates:
  # Import path validation
  import_path_check:
    name: "Import Path Validation"
    description: "Ensure no deprecated import paths (src.*, MOD_*)"
    command: "grep -r 'from src\\.' core/ tests/ || grep -r 'import src' core/ tests/"
    timeout_seconds: 5
    pass_criteria: "No matches found (exit code 1 from grep)"
    
    forbidden_patterns:
      - "from src.pipeline.*"
      - "from src.*"
      - "import src.*"
      - "from MOD_ERROR_PIPELINE.*"
    
    correct_patterns:
      - "from core.bootstrap.*"
      - "from core.engine.*"
      - "from core.adapters.*"
    
    on_failure:
      - "Replace src.* with core.* imports"
      - "Review ai_policies.yaml for correct import standards"
    
    tags: ["static", "imports", "deprecation"]
  
  # Python syntax check
  syntax_check:
    name: "Python Syntax Check"
    description: "Validate Python syntax across all source files"
    command: "python -m py_compile core/**/*.py tests/**/*.py"
    timeout_seconds: 10
    pass_criteria: "No syntax errors"
    
    on_failure:
      - "Fix syntax errors in reported files"
      - "Run individual file: python -m py_compile <file>"
    
    tags: ["static", "syntax"]
  
  # YAML syntax check
  yaml_syntax_check:
    name: "YAML Syntax Check"
    description: "Validate YAML files (profiles, configs, this file)"
    command: "python -c 'import yaml, sys; [yaml.safe_load(open(f)) for f in sys.argv[1:]]' *.yaml profiles/**/*.yaml"
    timeout_seconds: 5
    pass_criteria: "No YAML parsing errors"
    
    on_failure:
      - "Fix YAML syntax errors"
      - "Check indentation (use spaces, not tabs)"
      - "Validate with online YAML parser"
    
    tags: ["static", "yaml", "syntax"]
  
  # JSON schema validation
  json_syntax_check:
    name: "JSON Syntax Check"
    description: "Validate JSON files (schemas, configs, router configs)"
    command: "python -c 'import json, sys; [json.load(open(f)) for f in sys.argv[1:]]' schema/*.json profiles/**/router_config.json"
    timeout_seconds: 5
    pass_criteria: "No JSON parsing errors"
    
    on_failure:
      - "Fix JSON syntax errors"
      - "Validate with online JSON parser"
      - "Check for trailing commas, missing quotes"
    
    tags: ["static", "json", "syntax"]

# ==============================================================================
# INTEGRATION GATES
# End-to-end workflow validation
# ==============================================================================

integration_gates:
  # Bootstrap workflow test
  bootstrap_workflow:
    name: "Bootstrap Workflow Integration Test"
    description: "Run full bootstrap on sample project"
    command: "python core/bootstrap/orchestrator.py tests/fixtures/sample_python_project"
    timeout_seconds: 30
    pass_criteria: "Bootstrap completes successfully, artifacts generated"
    
    validates:
      - "Discovery → Selection → Generation → Validation workflow"
      - "PROJECT_PROFILE.yaml created and valid"
      - "router_config.json created and valid"
      - ".framework_initialized marker created"
    
    outputs:
      - "PROJECT_PROFILE.yaml"
      - "router_config.json"
      - "bootstrap_report.json"
    
    on_failure:
      - "Check bootstrap logs for error details"
      - "Verify sample project structure in tests/fixtures/"
      - "Review bootstrap orchestrator code"
    
    tags: ["integration", "bootstrap", "e2e"]
  
  # Execution engine workflow test (when available)
  execution_workflow:
    name: "Execution Engine Integration Test"
    description: "Run orchestrator on sample phase/workstream/tasks"
    command: "pytest tests/integration/test_execution_workflow.py -v"
    timeout_seconds: 60
    pass_criteria: "Workflow completes, all tasks executed"
    
    validates:
      - "Create run → Schedule → Route → Execute → Monitor"
      - "State transitions: PENDING → RUNNING → COMPLETED"
      - "Progress tracking updates correctly"
    
    status: "TODO: Integration tests in Phase 4"
    
    tags: ["integration", "execution", "e2e"]

# ==============================================================================
# GATE EXECUTION ORDER
# Recommended order for running gates
# ==============================================================================

execution_order:
  development_workflow:
    description: "Gates to run during active development"
    gates:
      - step: 1
        gate: "syntax_check"
        reason: "Catch basic errors early"
      
      - step: 2
        gate: "quick_validation"
        reason: "Fast feedback on core functionality"
      
      - step: 3
        gate: "import_path_check"
        reason: "Ensure no deprecated imports"
    
    frequency: "After each code change"
  
  pre_commit_workflow:
    description: "Gates to run before git commit"
    gates:
      - step: 1
        gate: "syntax_check"
      
      - step: 2
        gate: "yaml_syntax_check"
      
      - step: 3
        gate: "json_syntax_check"
      
      - step: 4
        gate: "import_path_check"
      
      - step: 5
        gate: "quick_validation"
    
    frequency: "Before each commit"
  
  pre_push_workflow:
    description: "Gates to run before git push"
    gates:
      - step: 1
        gate: "full_suite"
        blocking: true
      
      - step: 2
        gate: "import_path_check"
        blocking: true
      
      - step: 3
        gate: "bootstrap_workflow"
        blocking: false
    
    frequency: "Before each push"
  
  pre_release_workflow:
    description: "Gates to run before releasing new version"
    gates:
      - step: 1
        gate: "coverage_check"
        blocking: true
      
      - step: 2
        gate: "bootstrap_workflow"
        blocking: true
      
      - step: 3
        gate: "execution_workflow"
        blocking: false
      
      - step: 4
        name: "Documentation review"
        description: "Verify specs/STATUS.md is up-to-date"
        blocking: true
    
    frequency: "Before each release"

# ==============================================================================
# GATE STATISTICS
# Current state of quality gates
# ==============================================================================

statistics:
  total_test_gates: 6
  total_static_gates: 4
  total_integration_gates: 2
  total_tests: 196
  current_pass_rate: "100%"
  
  test_breakdown:
    schema_tests: 22
    bootstrap_tests: 8
    engine_tests: 92
    adapter_tests: 27
    resilience_tests: 32
    monitoring_tests: 15
  
  last_full_run:
    date: "2025-11-23"
    result: "PASS"
    duration_seconds: 95
    tests_passed: 196
    tests_failed: 0

# ==============================================================================
# TROUBLESHOOTING GUIDE
# Common gate failures and solutions
# ==============================================================================

troubleshooting:
  "Tests fail after refactoring":
    symptom: "Previously passing tests now fail"
    diagnosis:
      - "Breaking changes to public APIs"
      - "Unexpected behavior changes"
      - "Import path changes"
    solution:
      - "Review test failures for specific assertions"
      - "Run tests in isolation: pytest tests/path/test_file.py::test_name -v"
      - "Revert changes and apply incrementally"
  
  "Import path check fails":
    symptom: "grep finds deprecated imports"
    diagnosis: "Code contains src.* or MOD_* imports"
    solution:
      - "Replace with core.* imports"
      - "Check ai_policies.yaml for correct patterns"
      - "Run: grep -r 'from src\\.' core/ tests/ to find all occurrences"
  
  "Schema validation fails":
    symptom: "Schema tests report validation errors"
    diagnosis:
      - "Invalid JSON schema syntax"
      - "Example artifacts don't conform to schema"
    solution:
      - "Validate schema with online JSON Schema validator"
      - "Check example artifacts in tests/fixtures/"
      - "Review schema/*.json for errors"
  
  "Coverage too low":
    symptom: "Coverage check reports <80% coverage"
    diagnosis: "Insufficient test coverage for new code"
    solution:
      - "Add tests for uncovered functions"
      - "Check coverage report in htmlcov/index.html"
      - "Focus on public APIs and critical paths"
  
  "Bootstrap workflow fails":
    symptom: "Integration test fails to generate artifacts"
    diagnosis:
      - "Sample project structure invalid"
      - "Profile templates broken"
      - "Schema validation issues"
    solution:
      - "Check tests/fixtures/sample_python_project/"
      - "Verify profiles/ templates are valid"
      - "Run bootstrap with --debug for detailed logs"

# ==============================================================================
# CI/CD INTEGRATION
# How quality gates integrate with CI/CD pipeline
# ==============================================================================

ci_integration:
  github_actions:
    workflow_file: ".github/workflows/quality_gates.yml"
    
    triggers:
      - "push to main"
      - "pull request"
      - "manual dispatch"
    
    jobs:
      quick_check:
        runs_on: "ubuntu-latest"
        steps:
          - "syntax_check"
          - "import_path_check"
          - "quick_validation"
        timeout_minutes: 5
      
      full_validation:
        runs_on: "ubuntu-latest"
        steps:
          - "full_suite"
          - "coverage_check"
          - "bootstrap_workflow"
        timeout_minutes: 10
    
    pass_criteria: "All jobs must succeed"
  
  local_pre_commit_hook:
    description: "Git pre-commit hook for local validation"
    file: ".git/hooks/pre-commit"
    script: |
      #!/bin/bash
      pytest tests/schema/ tests/engine/ -v -k 'not slow'
      if [ $? -ne 0 ]; then
        echo "Tests failed. Commit aborted."
        exit 1
      fi

# ==============================================================================
# END OF QUALITY GATES
# ==============================================================================
