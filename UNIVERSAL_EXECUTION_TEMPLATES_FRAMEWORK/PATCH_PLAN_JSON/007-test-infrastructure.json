[
  {
    "op": "add",
    "path": "/meta/patch_metadata",
    "value": {
      "patch_id": "007-test-infrastructure",
      "patch_ulid": "01JDKDXWQP8PATCH007TESTS001",
      "created_at": "2025-11-23T12:10:22.447Z",
      "description": "Integrate test infrastructure documentation (18 test files covering bootstrap, schemas, resilience, monitoring, engine, adapters)",
      "source_files": [
        "tests/bootstrap/test_validator.py",
        "tests/schema/test_all_schemas.py",
        "tests/resilience/test_circuit_breaker.py",
        "tests/resilience/test_retry.py",
        "tests/resilience/test_resilient_executor.py",
        "tests/monitoring/test_run_monitor.py",
        "tests/monitoring/test_progress_tracker.py",
        "tests/engine/test_run_lifecycle.py",
        "tests/engine/test_routing.py",
        "tests/engine/test_scheduling.py",
        "tests/adapters/test_base.py",
        "tests/adapters/test_registry.py",
        "tests/adapters/test_subprocess_adapter.py",
        "tests/schema/test_doc_meta.py"
      ],
      "operations_count": 12,
      "priority": "HIGH",
      "approved_by": "human",
      "applied": false
    }
  },
  {
    "op": "add",
    "path": "/meta/test_infrastructure",
    "value": {
      "test_framework": "pytest",
      "total_test_files": 18,
      "test_location": "tests/",
      "coverage_areas": {
        "bootstrap": ["validator"],
        "schema": ["all schemas validation", "doc-meta"],
        "resilience": ["circuit_breaker", "retry", "resilient_executor"],
        "monitoring": ["run_monitor", "progress_tracker"],
        "engine": ["run_lifecycle", "routing", "scheduling"],
        "adapters": ["base", "registry", "subprocess_adapter"]
      },
      "test_categories": {
        "unit_tests": "Component-level tests for individual modules",
        "integration_tests": "Tests for component interactions",
        "validation_tests": "Schema and constraint validation tests"
      },
      "pytest_features": [
        "Fixtures for test data setup",
        "Parametrized tests for multiple scenarios",
        "Mocking for isolation",
        "Temporary directories (tmp_path)"
      ]
    }
  },
  {
    "op": "add",
    "path": "/meta/test_coverage/bootstrap",
    "value": {
      "test_file": "tests/bootstrap/test_validator.py",
      "module_under_test": "core/bootstrap/validator.py",
      "component": "BootstrapValidator",
      "workstream_id": "WS-02-03A",
      "test_count": 10,
      "coverage": {
        "schema_validation": "Tests PROJECT_PROFILE.yaml and router_config.json schema compliance",
        "constraint_validation": "Tests patch_only, max_lines_changed constraints",
        "auto_fixes": "Tests path normalization and missing defaults auto-correction",
        "tool_validation": "Tests tool availability and router consistency",
        "error_detection": "Tests profile_id mismatch, invalid schema, relaxed constraints"
      },
      "key_validations": [
        "PROJECT_PROFILE.yaml schema compliance",
        "router_config.json schema compliance",
        "Constraint enforcement (patch_only, max_lines_changed)",
        "Path normalization (Windows backslash → forward slash)",
        "Tool availability vs router config consistency",
        "Profile ID consistency check"
      ],
      "auto_fix_capabilities": [
        "Path normalization (backslash to forward slash)",
        "Missing defaults addition (constraints, framework_paths)",
        "Tool mismatch warnings (router vs available_tools)"
      ]
    }
  },
  {
    "op": "add",
    "path": "/meta/test_coverage/schema",
    "value": {
      "test_file": "tests/schema/test_all_schemas.py",
      "schemas_tested": 17,
      "test_approach": "Parametrized tests for all schemas",
      "validations": [
        "Schema is valid JSON Schema (Draft 7)",
        "All expected schemas exist",
        "Schema count matches expected (17 schemas)"
      ],
      "schemas_list": [
        "doc-meta.v1.json",
        "run_record.v1.json",
        "step_attempt.v1.json",
        "run_event.v1.json",
        "patch_artifact.v1.json",
        "patch_ledger_entry.v1.json",
        "patch_policy.v1.json",
        "prompt_instance.v1.json",
        "execution_request.v1.json",
        "phase_spec.v1.json",
        "workstream_spec.v1.json",
        "task_spec.v1.json",
        "router_config.v1.json",
        "project_profile.v1.json",
        "profile_extension.v1.json",
        "bootstrap_discovery.v1.json",
        "bootstrap_report.v1.json"
      ]
    }
  },
  {
    "op": "add",
    "path": "/meta/test_coverage/resilience",
    "value": {
      "test_files": [
        "tests/resilience/test_circuit_breaker.py",
        "tests/resilience/test_retry.py",
        "tests/resilience/test_resilient_executor.py"
      ],
      "modules_under_test": [
        "core/engine/resilience/circuit_breaker.py",
        "core/engine/resilience/retry.py",
        "core/engine/resilience/resilient_executor.py"
      ],
      "workstream_id": "WS-03-03A",
      "circuit_breaker_tests": {
        "state_transitions": "Tests CLOSED → OPEN → HALF_OPEN → CLOSED",
        "failure_threshold": "Tests circuit opens after N failures",
        "blocking_behavior": "Tests open circuit blocks calls",
        "recovery_timeout": "Tests automatic recovery after timeout",
        "half_open_testing": "Tests recovery validation in HALF_OPEN state"
      },
      "retry_tests": {
        "simple_retry": "Tests fixed delay retry strategy",
        "exponential_backoff": "Tests exponential backoff with jitter",
        "max_attempts": "Tests retry exhaustion",
        "success_after_retry": "Tests eventual success after retries"
      }
    }
  },
  {
    "op": "add",
    "path": "/meta/test_coverage/monitoring",
    "value": {
      "test_files": [
        "tests/monitoring/test_run_monitor.py",
        "tests/monitoring/test_progress_tracker.py"
      ],
      "modules_under_test": [
        "core/engine/monitoring/run_monitor.py",
        "core/engine/monitoring/progress_tracker.py"
      ],
      "workstream_id": "WS-03-03B",
      "run_monitor_tests": {
        "metrics_aggregation": "Tests step count, event count aggregation",
        "active_runs": "Tests filtering active vs terminal runs",
        "summary_stats": "Tests system-wide summary generation",
        "duration_calculation": "Tests elapsed time calculation"
      }
    }
  },
  {
    "op": "add",
    "path": "/meta/test_coverage/engine",
    "value": {
      "test_files": [
        "tests/engine/test_run_lifecycle.py",
        "tests/engine/test_routing.py",
        "tests/engine/test_scheduling.py"
      ],
      "modules_under_test": [
        "core/engine/orchestrator.py",
        "core/engine/router.py",
        "core/engine/scheduler.py"
      ],
      "workstream_ids": ["WS-03-01A", "WS-03-01B", "WS-03-01C"],
      "run_lifecycle_tests": {
        "run_creation": "Tests create_run with state=pending",
        "state_transitions": "Tests pending → running → succeeded/failed",
        "step_attempts": "Tests step creation and completion",
        "event_emission": "Tests run_created, step_started events"
      },
      "routing_tests": {
        "task_routing": "Tests task_kind → tool_id mapping",
        "capability_matching": "Tests tool capability filtering",
        "strategy_selection": "Tests fixed/round_robin/auto strategies"
      },
      "scheduling_tests": {
        "dependency_graph": "Tests task dependency construction",
        "cycle_detection": "Tests circular dependency detection",
        "topological_ordering": "Tests DAG execution order",
        "parallel_batching": "Tests parallel task batching with max_parallel limit"
      }
    }
  },
  {
    "op": "add",
    "path": "/meta/test_coverage/adapters",
    "value": {
      "test_files": [
        "tests/adapters/test_base.py",
        "tests/adapters/test_registry.py",
        "tests/adapters/test_subprocess_adapter.py"
      ],
      "modules_under_test": [
        "core/adapters/base.py",
        "core/adapters/registry.py",
        "core/adapters/subprocess_adapter.py"
      ],
      "adapter_tests": {
        "base_adapter": "Tests abstract adapter interface",
        "adapter_registry": "Tests adapter registration and lookup",
        "subprocess_adapter": "Tests subprocess execution and output capture"
      }
    }
  },
  {
    "op": "add",
    "path": "/validation/quality_gates/test_execution",
    "value": {
      "pytest_all": {
        "command": "pytest tests/",
        "required": true,
        "description": "Run all tests"
      },
      "pytest_unit": {
        "command": "pytest tests/ -m 'not integration'",
        "required": true,
        "description": "Run unit tests only"
      },
      "pytest_coverage": {
        "command": "pytest tests/ --cov=core --cov-report=term-missing",
        "required": false,
        "description": "Run tests with coverage report"
      },
      "pytest_specific": {
        "bootstrap": "pytest tests/bootstrap/",
        "schema": "pytest tests/schema/",
        "resilience": "pytest tests/resilience/",
        "monitoring": "pytest tests/monitoring/",
        "engine": "pytest tests/engine/",
        "adapters": "pytest tests/adapters/"
      }
    }
  },
  {
    "op": "add",
    "path": "/phases/PH-000/workstreams/WS-000-010",
    "value": {
      "workstream_id": "WS-000-010",
      "workstream_ulid": "01JDKDXWQP8WS000010TESTINF",
      "name": "Test Infrastructure Documentation",
      "phase_id": "PH-000",
      "priority": "MEDIUM",
      "estimated_duration_hours": 1.0,
      "dependencies": [],
      "description": "Document test infrastructure and testing strategy",
      "tasks": {
        "TSK-000-010-001": {
          "task_id": "TSK-000-010-001",
          "task_ulid": "01JDKDXWQP8TSK000010001DOC",
          "name": "Create TESTING_GUIDE.md",
          "workstream_id": "WS-000-010",
          "executor": "file_create",
          "inputs": {
            "file_path": {"type": "string", "default": "docs/TESTING_GUIDE.md"}
          },
          "file_scope": {
            "create": ["docs/TESTING_GUIDE.md"],
            "modify": [],
            "read_only": ["tests/**/*.py", "pytest.ini"]
          },
          "acceptance_tests": {
            "powershell": "Test-Path docs/TESTING_GUIDE.md",
            "pytest": null
          },
          "max_runtime_seconds": 600,
          "idempotent": true
        }
      },
      "completion_criteria": {
        "all_tasks_complete": true,
        "documentation_exists": true
      }
    }
  },
  {
    "op": "replace",
    "path": "/phases/PH-000/estimated_duration_hours",
    "value": 10.0
  },
  {
    "op": "add",
    "path": "/meta/implementation_notes/testing",
    "value": {
      "test_framework": "pytest",
      "test_discovery": "pytest automatically discovers test_*.py files",
      "fixture_pattern": "Use @pytest.fixture for reusable test data",
      "parametrization": "Use @pytest.mark.parametrize for multiple test cases",
      "mocking": "Use unittest.mock or pytest-mock for isolation",
      "coverage_tool": "pytest-cov for coverage reports",
      "run_commands": {
        "all_tests": "pytest tests/",
        "specific_module": "pytest tests/engine/",
        "specific_test": "pytest tests/engine/test_scheduling.py::test_name",
        "with_coverage": "pytest tests/ --cov=core --cov-report=html",
        "verbose": "pytest tests/ -v",
        "fail_fast": "pytest tests/ -x"
      },
      "best_practices": [
        "Test one thing per test function",
        "Use descriptive test names (test_circuit_opens_after_threshold)",
        "Arrange-Act-Assert pattern",
        "Use fixtures for common setup",
        "Mock external dependencies",
        "Test both success and failure paths",
        "Test edge cases and boundary conditions"
      ]
    }
  },
  {
    "op": "add",
    "path": "/meta/existing_components/test_suite",
    "value": {
      "completion": 75,
      "location": "tests/",
      "description": "Comprehensive pytest test suite",
      "test_count": "50+ tests across 18 files",
      "coverage_estimate": "~75% of core modules",
      "test_categories": {
        "bootstrap_validation": "10 tests in test_validator.py",
        "schema_validation": "17 parametrized tests in test_all_schemas.py",
        "resilience_patterns": "20+ tests across circuit_breaker, retry",
        "monitoring": "10+ tests for run_monitor, progress_tracker",
        "engine": "15+ tests for orchestrator, router, scheduler",
        "adapters": "10+ tests for adapter infrastructure"
      },
      "missing_coverage": [
        "Integration tests for full run execution",
        "End-to-end tests for workstream completion",
        "Performance tests for large DAGs",
        "Stress tests for concurrent execution"
      ]
    }
  }
]
