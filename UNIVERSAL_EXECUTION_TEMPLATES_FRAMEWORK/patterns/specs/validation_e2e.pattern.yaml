# Pattern: End-to-End Validation
# Pattern ID: PAT-E2E-VALIDATION-001
# Version: 1.0.0
# Created: 2025-11-27
# Category: validation
# Use Case: Comprehensive system validation after changes
# Time Savings: 65% (45 min ‚Üí 16 min)

pattern_id: "PAT-E2E-VALIDATION-001"
name: "end_to_end_validation"
version: "1.0.0"
category: "validation"
status: "active"

metadata:
  created: "2025-11-27"
  last_updated: "2025-11-27"
  author: "UET Framework Team"
  proven_uses: 0
  time_savings_vs_manual: "65%"
  estimated_duration_seconds: 960  # 16 minutes

intent: |
  Run comprehensive validation suite to verify system works end-to-end
  after making changes. Tests integration points, validates data, and
  confirms functionality.
  
  This pattern enforces:
  - Systematic validation checklist
  - Automated test execution
  - Clear pass/fail criteria
  - Detailed reporting

applicability:
  when_to_use:
    - After completing integration
    - Before committing major changes
    - Validating activation of new features
    - Smoke testing after deployment
  when_not_to_use:
    - Unit testing individual functions (use test pattern)
    - Performance benchmarking (use perf pattern)
    - Security auditing (use security pattern)
  
  constraints:
    must_have_rollback: true
    clear_success_criteria: true
    comprehensive_coverage: true

inputs:
  validation_name:
    type: "string"
    required: true
    description: "Name of validation suite"
    example: "pattern_automation_activation"
  
  validation_type:
    type: "string"
    required: true
    enum: ["integration", "smoke", "regression", "activation"]
    default: "integration"
  
  test_script_path:
    type: "string"
    required: true
    description: "Path where validation script will be created"
    example: "automation/tests/test_activation.ps1"
  
  checks:
    type: "array"
    required: true
    description: "List of validation checks to perform"
    items:
      type: "object"
      required: ["name", "command"]
      properties:
        name:
          type: "string"
          description: "Check name"
          example: "Database tables exist"
        command:
          type: "string"
          description: "PowerShell/Bash command to run"
          example: "Test-Path database.db"
        expect:
          type: "string"
          description: "Expected output or condition"
          example: "True"
        critical:
          type: "boolean"
          default: true
          description: "Whether failure blocks continuation"
        setup:
          type: "string"
          description: "Optional setup command before check"
        cleanup:
          type: "string"
          description: "Optional cleanup after check"
  
  demo_script_path:
    type: "string"
    required: false
    description: "Optional demo script to create"
    example: "automation/tests/demo_pattern_detection.ps1"
  
  demo_scenario:
    type: "object"
    required: false
    description: "Demo scenario to validate"
    properties:
      description:
        type: "string"
      steps:
        type: "array"
        items:
          type: "string"
      expected_outcome:
        type: "string"
  
  report_path:
    type: "string"
    required: true
    description: "Path for validation report"
    example: "VALIDATION_REPORT_{{timestamp}}.md"

steps:
  - id: "create_test_script"
    description: "Generate validation test script"
    operation_kind: "CREATE_TESTS"
    
    actions:
      - tool: "create"
        path: "{{test_script_path}}"
        content: |
          # {{validation_name | titleCase}} Validation
          # Type: {{validation_type}}
          # Created: {{timestamp}}
          
          $ErrorActionPreference = "Stop"
          
          Write-Host "üß™ {{validation_name | titleCase}}" -ForegroundColor Cyan
          Write-Host "Type: {{validation_type}}" -ForegroundColor Gray
          Write-Host ""
          
          $results = @()
          $passCount = 0
          $failCount = 0
          $checkCount = {{checks.length}}
          
          {{#each checks}}
          # Check {{@index | add 1}}: {{name}}
          Write-Host "[{{@index | add 1}}/{{../checks.length}}] {{name}}..." -NoNewline
          
          try {
              {{#if setup}}
              # Setup
              {{setup}}
              {{/if}}
              
              # Execute check
              $output = {{command}}
              
              {{#if expect}}
              # Validate output
              if ($output -match "{{expect}}") {
                  Write-Host " ‚úì" -ForegroundColor Green
                  $results += @{
                      name = "{{name}}"
                      status = "PASS"
                      output = $output
                  }
                  $passCount++
              } else {
                  Write-Host " ‚úó" -ForegroundColor Red
                  Write-Host "  Expected: {{expect}}" -ForegroundColor Yellow
                  Write-Host "  Got: $output" -ForegroundColor Yellow
                  $results += @{
                      name = "{{name}}"
                      status = "FAIL"
                      expected = "{{expect}}"
                      actual = $output
                  }
                  $failCount++
                  {{#if critical}}
                  throw "Critical check failed: {{name}}"
                  {{/if}}
              }
              {{else}}
              # No specific expectation - check exit code
              if ($LASTEXITCODE -eq 0 -or $output) {
                  Write-Host " ‚úì" -ForegroundColor Green
                  $passCount++
                  $results += @{
                      name = "{{name}}"
                      status = "PASS"
                  }
              } else {
                  Write-Host " ‚úó" -ForegroundColor Red
                  $failCount++
                  $results += @{
                      name = "{{name}}"
                      status = "FAIL"
                  }
                  {{#if critical}}
                  throw "Critical check failed: {{name}}"
                  {{/if}}
              }
              {{/if}}
              
              {{#if cleanup}}
              # Cleanup
              {{cleanup}}
              {{/if}}
          }
          catch {
              Write-Host " ‚úó" -ForegroundColor Red
              Write-Host "  Error: $_" -ForegroundColor Red
              $results += @{
                  name = "{{name}}"
                  status = "ERROR"
                  error = $_.Exception.Message
              }
              $failCount++
              {{#if critical}}
              throw
              {{/if}}
          }
          
          {{/each}}
          
          # Summary
          Write-Host ""
          Write-Host "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ" -ForegroundColor Gray
          Write-Host "Summary:" -ForegroundColor Cyan
          Write-Host "  Total checks: $checkCount" -ForegroundColor White
          Write-Host "  Passed: $passCount" -ForegroundColor Green
          Write-Host "  Failed: $failCount" -ForegroundColor $(if ($failCount -eq 0) { "Green" } else { "Red" })
          Write-Host "  Success rate: $([math]::Round($passCount / $checkCount * 100, 1))%" -ForegroundColor White
          
          if ($failCount -eq 0) {
              Write-Host ""
              Write-Host "‚úÖ All validation checks passed!" -ForegroundColor Green
              exit 0
          } else {
              Write-Host ""
              Write-Host "‚ùå Validation failed with $failCount error(s)" -ForegroundColor Red
              exit 1
          }
    
    outputs:
      - name: "test_script_created"
        type: "boolean"
  
  - id: "create_demo_script"
    description: "Create demo script for manual validation"
    operation_kind: "CREATE_DEMO"
    
    condition: "demo_script_path is defined"
    
    actions:
      - tool: "create"
        path: "{{demo_script_path}}"
        content: |
          # Demo: {{validation_name}}
          # {{demo_scenario.description}}
          # Created: {{timestamp}}
          
          Write-Host "üéØ Demo: {{validation_name | titleCase}}" -ForegroundColor Cyan
          Write-Host "{{demo_scenario.description}}" -ForegroundColor Yellow
          Write-Host ""
          
          {{#each demo_scenario.steps}}
          Write-Host "Step {{@index | add 1}}: {{this}}" -ForegroundColor White
          {{this}}
          
          {{/each}}
          
          Write-Host ""
          Write-Host "Expected Outcome:" -ForegroundColor Cyan
          Write-Host "{{demo_scenario.expected_outcome}}" -ForegroundColor Green
    
    outputs:
      - name: "demo_script_created"
        type: "boolean"
  
  - id: "run_validation"
    description: "Execute validation script"
    operation_kind: "RUN_TESTS"
    
    actions:
      - tool: "powershell"
        command: "& {{test_script_path}}"
        capture_output: true
    
    outputs:
      - name: "validation_results"
        type: "object"
        description: "Test execution results"
  
  - id: "generate_report"
    description: "Create validation report"
    operation_kind: "CREATE_REPORT"
    
    actions:
      - tool: "create"
        path: "{{report_path}}"
        content: |
          # Validation Report: {{validation_name}}
          
          **Date:** {{timestamp}}  
          **Type:** {{validation_type}}  
          **Status:** {{#if all_passed}}‚úÖ PASSED{{else}}‚ùå FAILED{{/if}}
          
          ## Summary
          
          - **Total Checks:** {{checks.length}}
          - **Passed:** {{pass_count}}
          - **Failed:** {{fail_count}}
          - **Success Rate:** {{success_rate}}%
          
          ## Validation Checks
          
          {{#each checks}}
          ### {{@index | add 1}}. {{name}}
          
          **Status:** {{#if passed}}‚úÖ PASS{{else}}‚ùå FAIL{{/if}}  
          **Command:** `{{command}}`
          {{#if expect}}
          **Expected:** `{{expect}}`
          {{/if}}
          {{#if critical}}
          **Critical:** Yes
          {{/if}}
          
          {{#if output}}
          **Output:**
          ```
          {{output}}
          ```
          {{/if}}
          
          {{#if error}}
          **Error:**
          ```
          {{error}}
          ```
          {{/if}}
          
          ---
          
          {{/each}}
          
          ## Next Steps
          
          {{#if all_passed}}
          ‚úÖ All validation checks passed. System is ready.
          
          {{#if demo_script_path}}
          ### Optional Demo
          
          Run the demo script to see the system in action:
          
          ```powershell
          & {{demo_script_path}}
          ```
          {{/if}}
          {{else}}
          ‚ùå Validation failed. Review errors above and:
          
          1. Fix failing checks
          2. Re-run validation
          3. Check rollback plan if needed
          
          ### Failed Checks
          
          {{#each checks}}
          {{#unless passed}}
          - {{name}}: {{error}}
          {{/unless}}
          {{/each}}
          {{/if}}
          
          ## Validation Script
          
          **Location:** `{{test_script_path}}`
          
          Re-run validation:
          ```powershell
          & {{test_script_path}}
          ```

outputs:
  test_script:
    type: "file"
    path: "{{test_script_path}}"
    description: "Automated validation script"
  
  demo_script:
    type: "file"
    path: "{{demo_script_path}}"
    condition: "demo_script_path is defined"
    description: "Demo script for manual validation"
  
  validation_report:
    type: "file"
    path: "{{report_path}}"
    description: "Detailed validation report"
  
  all_passed:
    type: "boolean"
    description: "Whether all checks passed"
  
  success_rate:
    type: "number"
    description: "Percentage of checks that passed"

verification:
  ground_truth:
    - condition: "test script exists and is executable"
      description: "Test script created"
    - condition: "validation report exists"
      description: "Report generated"
    - condition: "all_passed == true OR failures documented"
      description: "Validation completed"
  
  validation_commands:
    - name: "check_test_script"
      command: "Test-Path {{test_script_path}}"
      expect: "True"
    
    - name: "check_report"
      command: "Test-Path {{report_path}}"
      expect: "True"
    
    - name: "run_validation"
      command: "& {{test_script_path}}"
      expect: "exit code 0 (if all passed)"

anti_patterns:
  avoid:
    - pattern: "Manual validation without script"
      reason: "Not repeatable, error-prone"
    - pattern: "No clear success criteria"
      reason: "Cannot determine if validation passed"
    - pattern: "Skipping validation"
      reason: "Integration issues discovered later"
    - pattern: "No validation report"
      reason: "Cannot review what was tested"

example_usage: |
  # Validate pattern automation activation
  
  Input:
    validation_name: "pattern_automation_activation"
    validation_type: "activation"
    test_script_path: "automation/tests/test_activation.ps1"
    checks:
      - name: "Database tables exist"
        command: "sqlite3 patterns/metrics/pattern_automation.db \".tables\""
        expect: "execution_logs.*pattern_candidates.*anti_patterns"
        critical: true
      
      - name: "Configuration loads"
        command: "Test-Path automation/config/detection_config.yaml"
        expect: "True"
        critical: true
      
      - name: "Integration module imports"
        command: "python -c 'from automation.integration.orchestrator_hooks import get_hooks; print(\"OK\")'"
        expect: "OK"
        critical: true
      
      - name: "Execution logging works"
        command: "python -c '...' "
        expect: "OK"
        critical: false
    
    demo_script_path: "automation/tests/demo_pattern_detection.ps1"
    demo_scenario:
      description: "Trigger pattern auto-detection with 3 similar tasks"
      steps:
        - "Run file creation task 1"
        - "Run file creation task 2"
        - "Run file creation task 3"
        - "Check for auto-generated pattern"
      expected_outcome: "AUTO-*.yaml file created in patterns/drafts/"
    
    report_path: "ACTIVATION_VALIDATION_REPORT.md"
  
  Output:
    - automation/tests/test_activation.ps1 created
    - automation/tests/demo_pattern_detection.ps1 created
    - ACTIVATION_VALIDATION_REPORT.md created
    - all_passed: true
    - success_rate: 100

related_patterns:
  - pattern_id: "PAT-DATABASE-SETUP-001"
    relationship: "validates"
    description: "Validates database setup completed correctly"
  
  - pattern_id: "PAT-INTEGRATION-HOOK-001"
    relationship: "validates"
    description: "Validates integration hooks working"

tool_targets:
  - "github_copilot_cli"
  - "claude_code"
  - "cursor"

operation_kinds:
  - "CREATE_TESTS"
  - "CREATE_DEMO"
  - "RUN_TESTS"
  - "CREATE_REPORT"
