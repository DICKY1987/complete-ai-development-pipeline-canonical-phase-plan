[
  {
    "op": "add",
    "path": "/meta/patch_metadata/011-ai-optimization",
    "value": {
      "patch_id": "011-ai-codebase-optimization",
      "patch_ulid": "01JDKB8ZX12PATCH011AIOPTIM",
      "created_at": "2025-11-23T22:18:00.000Z",
      "description": "Optimize codebase for AI agent efficiency: fix test infrastructure, create AI guidance, standardize module manifests, add visual aids",
      "source_files": [
        "uet-execution-acceleration-guide.md",
        "ecision Elimination Through Pattern Recognition6.md",
        "ai_policies.yaml",
        "CODEBASE_INDEX.yaml"
      ],
      "operations_count": 18,
      "priority": "HIGH",
      "approved_by": "human",
      "applied": false,
      "optimization_notes": "Decision elimination principles applied: templates for manifests, batch validation, parallel test fixes, ground truth verification"
    }
  },
  {
    "op": "add",
    "path": "/phases/PH-011",
    "value": {
      "phase_id": "PH-011",
      "phase_ulid": "01JDKB8ZX12PH011AIOPTIM01",
      "name": "AI Codebase Optimization",
      "category": "infrastructure",
      "description": "Make codebase best-in-class for AI agents: fix broken tests, add AI guidance docs, standardize module manifests, enable visual navigation",
      "priority": "HIGH",
      "status": "ready",
      "estimated_duration_hours": 6,
      "dependencies": [],
      "spec_id": "AI-OPTIMIZATION-SPEC-v1.0",
      "execution_strategy": {
        "parallelism_enabled": true,
        "max_parallel_workers": 3,
        "worker_distribution": "by_workstream",
        "conflict_resolution": "worktree_isolation",
        "merge_strategy": "sequential_with_tests"
      },
      "resource_scope": {
        "type": "files",
        "read": [
          "tests/**/*.py",
          "**/.ai-module-manifest",
          "docs/**/*.md",
          "ai_policies.yaml",
          "CODEBASE_INDEX.yaml"
        ],
        "write": [
          "tests/**/*.py",
          "**/.ai-module-manifest",
          ".meta/AI_GUIDANCE.md",
          "schema/ai_module_manifest.schema.json",
          "docs/diagrams/**"
        ],
        "create": [
          ".meta/",
          ".meta/AI_GUIDANCE.md",
          "schema/ai_module_manifest.schema.json",
          "scripts/validate_module_manifests.py",
          "scripts/generate_module_manifests.py",
          "scripts/generate_architecture_diagrams.py",
          "scripts/validate_doc_links.py",
          "docs/diagrams/",
          "examples/"
        ],
        "forbidden": [
          "legacy/**",
          "src/pipeline/**",
          "MOD_ERROR_PIPELINE/**"
        ]
      },
      "constraints": {
        "patch": {
          "patch_required": false,
          "max_lines_changed": 5000,
          "max_files_changed": 100
        },
        "tests": {
          "tests_must_run": true,
          "tests_must_pass": true,
          "test_command": "pytest tests -q --tb=short"
        },
        "behavior": {
          "atomic_commits_per_workstream": true,
          "worktree_isolation": true,
          "no_breaking_changes": true
        }
      },
      "acceptance": {
        "mode": "all",
        "checks": [
          {
            "id": "ac-011-001",
            "type": "test_infrastructure",
            "description": "All pytest collection errors fixed",
            "command": "pytest --collect-only tests 2>&1 | grep -c 'error'",
            "required": true,
            "severity": "critical",
            "expected_output": "0"
          },
          {
            "id": "ac-011-002",
            "type": "file_exists",
            "description": "AI_GUIDANCE.md created and comprehensive",
            "command": "Test-Path .meta/AI_GUIDANCE.md && (Get-Content .meta/AI_GUIDANCE.md | Measure-Object -Line).Lines -gt 100",
            "required": true,
            "severity": "critical"
          },
          {
            "id": "ac-011-003",
            "type": "validation",
            "description": "All module manifests valid against schema",
            "command": "python scripts/validate_module_manifests.py --strict",
            "required": true,
            "severity": "critical"
          },
          {
            "id": "ac-011-004",
            "type": "file_exists",
            "description": "Architecture diagrams generated",
            "command": "Test-Path docs/diagrams/module_dependencies.png && Test-Path docs/diagrams/execution_flow.png",
            "required": true,
            "severity": "warning"
          },
          {
            "id": "ac-011-005",
            "type": "test_pass",
            "description": "Full test suite passes",
            "command": "pytest tests -q",
            "required": true,
            "severity": "critical"
          }
        ],
        "post_actions": [
          "git add tests/ .meta/ schema/ scripts/ docs/ **/.ai-module-manifest",
          "git commit -m 'PH-011: AI codebase optimization complete'",
          "python scripts/validate_acs_conformance.py",
          "python scripts/validate_workstreams.py"
        ]
      },
      "workstreams": {}
    }
  },
  {
    "op": "add",
    "path": "/phases/PH-011/workstreams/WS-011-TESTS",
    "value": {
      "workstream_id": "WS-011-TESTS",
      "workstream_ulid": "01JDKB8ZX12WS011TESTS0001",
      "project_id": "PRJ-COMPLETE_AI_DEVELOPMENT_PIPELINE",
      "phase_id": "PH-011",
      "name": "Test Infrastructure Fixes (CRITICAL)",
      "objective": "Fix all 28 pytest collection errors to enable reliable validation",
      "tags": ["testing", "infrastructure", "critical"],
      "execution": {
        "worktree": {
          "enabled": true,
          "path": ".worktrees/ws-011-tests",
          "branch": "ws/PH-011/test-fixes",
          "base_branch": "main"
        },
        "worker_affinity": "cli_worker_1",
        "conflict_group": "test_infrastructure",
        "allow_parallel": true
      },
      "concurrency": {
        "max_parallel": 3
      },
      "error_handling": {
        "strategy": "continue"
      },
      "tasks": [
        {
          "task_id": "t-011-tests-001",
          "name": "Audit test collection errors",
          "kind": "analysis",
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "testing",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 60
          },
          "execution_request_template": {
            "tool": "pytest",
            "action": "collect_only",
            "params": {
              "args": ["--collect-only", "tests", "-q"],
              "capture_errors": true,
              "parse_output": true
            }
          },
          "output_artifacts": [
            ".state/test_collection_errors.json"
          ]
        },
        {
          "task_id": "t-011-tests-002",
          "name": "Remove/fix test files for missing modules",
          "kind": "file_edit",
          "depends_on": ["t-011-tests-001"],
          "allow_parallel": false,
          "classification": {
            "complexity": "moderate",
            "risk_tier": "medium",
            "domain": "testing",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 300
          },
          "prompt_template_ref": "batch_fix_test_imports",
          "prompt_overrides": {
            "instructions": "Based on collection errors: (1) Comment out or remove test files for non-existent modules (test_queue_manager.py, test_worker_pool.py if modules don't exist), (2) Fix import paths using deprecated modules (src.pipeline.* → core.*, MOD_ERROR_PIPELINE.* → error.*), (3) Add skip decorators with reason for incomplete tests."
          },
          "files_scope": {
            "reads": ["tests/**/*.py", ".state/test_collection_errors.json"],
            "writes": ["tests/**/*.py"]
          }
        },
        {
          "task_id": "t-011-tests-003",
          "name": "Add pytest.ini with exclusions",
          "kind": "file_create",
          "depends_on": ["t-011-tests-002"],
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "testing",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 60
          },
          "prompt_template_ref": "create_config_file",
          "prompt_overrides": {
            "instructions": "Create pytest.ini: [pytest] section with testpaths=tests, python_files=test_*.py, python_classes=Test*, python_functions=test_*, addopts=--tb=short -v, norecursedirs=.git .worktrees .state .venv __pycache__, markers for slow/integration tests. Add coverage config: --cov=core --cov=error --cov=aim --cov=pm --cov-report=html --cov-report=term."
          },
          "files_scope": {
            "creates": ["pytest.ini"]
          }
        },
        {
          "task_id": "t-011-tests-004",
          "name": "Update conftest.py with graceful handling",
          "kind": "file_edit",
          "depends_on": ["t-011-tests-002"],
          "allow_parallel": true,
          "classification": {
            "complexity": "moderate",
            "risk_tier": "low",
            "domain": "testing",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 180
          },
          "prompt_template_ref": "edit_test_fixture",
          "prompt_overrides": {
            "instructions": "Update tests/conftest.py: Add fixtures for tmp_path, db_path (tmp worktree db), mock_project_root. Add import error handling: try/except ImportError with pytest.skip for optional modules. Add session-scoped fixture for test database setup/teardown."
          },
          "files_scope": {
            "writes": ["tests/conftest.py"]
          }
        },
        {
          "task_id": "t-011-tests-005",
          "name": "Verify all tests collectible",
          "kind": "validation",
          "depends_on": ["t-011-tests-002", "t-011-tests-003", "t-011-tests-004"],
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "testing",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 60
          },
          "execution_request_template": {
            "tool": "pytest",
            "action": "collect_verify",
            "params": {
              "args": ["--collect-only", "tests", "-q"],
              "assert_zero_errors": true
            }
          }
        }
      ]
    }
  },
  {
    "op": "add",
    "path": "/phases/PH-011/workstreams/WS-011-GUIDANCE",
    "value": {
      "workstream_id": "WS-011-GUIDANCE",
      "workstream_ulid": "01JDKB8ZX12WS011GUIDE0001",
      "project_id": "PRJ-COMPLETE_AI_DEVELOPMENT_PIPELINE",
      "phase_id": "PH-011",
      "name": "AI Guidance Documentation",
      "objective": "Create comprehensive .meta/AI_GUIDANCE.md to eliminate 25 min onboarding per session",
      "tags": ["documentation", "ai", "onboarding"],
      "execution": {
        "worktree": {
          "enabled": true,
          "path": ".worktrees/ws-011-guidance",
          "branch": "ws/PH-011/ai-guidance",
          "base_branch": "main"
        },
        "worker_affinity": "cli_worker_2",
        "conflict_group": "documentation",
        "allow_parallel": true
      },
      "concurrency": {
        "max_parallel": 1
      },
      "error_handling": {
        "strategy": "fail_fast"
      },
      "tasks": [
        {
          "task_id": "t-011-guide-001",
          "name": "Create .meta/AI_GUIDANCE.md from template",
          "kind": "file_create",
          "allow_parallel": false,
          "classification": {
            "complexity": "moderate",
            "risk_tier": "low",
            "domain": "documentation",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 300
          },
          "prompt_template_ref": "create_ai_guidance",
          "prompt_overrides": {
            "instructions": "Create .meta/AI_GUIDANCE.md with sections: (1) This Codebase in 60 Seconds - UET pipeline, section architecture, db location, entry point, (2) Common AI Gotchas - forbidden imports, db path resolution, DAG validation, plugin discovery, (3) Typical Task Patterns - add error plugin, add script, modify db schema, (4) Quick Commands - test + validate, (5) Speed Demon Principles - ground truth, atomic execution, no planning overhead, template-driven, (6) Module Navigation Quick Reference - table of module → purpose → entry point, (7) Common Edit Patterns - what files safe to edit vs review-required. Reference ai_policies.yaml and CODEBASE_INDEX.yaml. Target: 150-200 lines, scannable in 2 minutes."
          },
          "files_scope": {
            "reads": ["ai_policies.yaml", "CODEBASE_INDEX.yaml", "CLAUDE.md", "**/.ai-module-manifest"],
            "creates": [".meta/", ".meta/AI_GUIDANCE.md"]
          }
        },
        {
          "task_id": "t-011-guide-002",
          "name": "Add decision elimination cheatsheet",
          "kind": "file_edit",
          "depends_on": ["t-011-guide-001"],
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "documentation",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 120
          },
          "prompt_template_ref": "append_section",
          "prompt_overrides": {
            "instructions": "Add section 'Decision Elimination Cheatsheet' to AI_GUIDANCE.md: Common decisions → pre-made answers table. Examples: 'What structure?' → Use template, 'How detailed?' → Good enough, 'How to verify?' → Ground truth command, 'Is it complete?' → Tests pass, 'Should I continue?' → Yes until acceptance checks green. Reference Decision Elimination Through Pattern Recognition principles."
          },
          "files_scope": {
            "writes": [".meta/AI_GUIDANCE.md"]
          }
        },
        {
          "task_id": "t-011-guide-003",
          "name": "Update CLAUDE.md to reference AI_GUIDANCE.md",
          "kind": "file_edit",
          "depends_on": ["t-011-guide-001"],
          "allow_parallel": true,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "documentation",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 60
          },
          "prompt_template_ref": "update_reference",
          "prompt_overrides": {
            "instructions": "Update CLAUDE.md: Change broken reference to AI_GUIDANCE.md → actual file path .meta/AI_GUIDANCE.md. Add note: 'Read this FIRST before any session - saves 25 min onboarding time.'"
          },
          "files_scope": {
            "writes": ["CLAUDE.md"]
          }
        }
      ]
    }
  },
  {
    "op": "add",
    "path": "/phases/PH-011/workstreams/WS-011-MANIFESTS",
    "value": {
      "workstream_id": "WS-011-MANIFESTS",
      "workstream_ulid": "01JDKB8ZX12WS011MANIF001",
      "project_id": "PRJ-COMPLETE_AI_DEVELOPMENT_PIPELINE",
      "phase_id": "PH-011",
      "name": "Module Manifest Standardization",
      "objective": "Create schema, validate/fix existing manifests, auto-generate missing ones",
      "tags": ["schema", "validation", "automation"],
      "execution": {
        "worktree": {
          "enabled": true,
          "path": ".worktrees/ws-011-manifests",
          "branch": "ws/PH-011/manifests",
          "base_branch": "main"
        },
        "worker_affinity": "cli_worker_3",
        "conflict_group": "manifests",
        "allow_parallel": true
      },
      "concurrency": {
        "max_parallel": 2
      },
      "error_handling": {
        "strategy": "continue"
      },
      "tasks": [
        {
          "task_id": "t-011-manifest-001",
          "name": "Create ai_module_manifest.schema.json",
          "kind": "file_create",
          "allow_parallel": false,
          "classification": {
            "complexity": "moderate",
            "risk_tier": "low",
            "domain": "schema",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 180
          },
          "prompt_template_ref": "create_json_schema",
          "prompt_overrides": {
            "instructions": "Create schema/ai_module_manifest.schema.json: JSON Schema Draft 7. Required: module (string), purpose (string), layer (enum: infra/domain/api/ui), entry_points (array of objects with file/function/description). Optional: key_patterns (array), common_tasks (array), gotchas (array), dependencies (object with external/internal arrays), status (object with maturity/test_coverage/production_ready), ai_quick_reference (object). Use existing .ai-module-manifest files as examples. Target: strict schema that catches incomplete manifests."
          },
          "files_scope": {
            "reads": ["**/.ai-module-manifest"],
            "creates": ["schema/ai_module_manifest.schema.json"]
          }
        },
        {
          "task_id": "t-011-manifest-002",
          "name": "Create validate_module_manifests.py",
          "kind": "code_create",
          "depends_on": ["t-011-manifest-001"],
          "allow_parallel": false,
          "classification": {
            "complexity": "moderate",
            "risk_tier": "low",
            "domain": "tooling",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 240
          },
          "prompt_template_ref": "create_python_tool",
          "prompt_overrides": {
            "instructions": "Create scripts/validate_module_manifests.py: (1) Find all .ai-module-manifest files, (2) Parse as YAML, (3) Validate against schema/ai_module_manifest.schema.json using jsonschema, (4) Check entry_points files exist, (5) Verify layer matches CODEBASE_INDEX.yaml, (6) Report validation errors with file:line. CLI: --strict (exit 1 on any error), --fix (auto-fix common issues), --report-only (default). Exit 0 if all valid."
          },
          "files_scope": {
            "reads": ["schema/ai_module_manifest.schema.json", "**/.ai-module-manifest", "CODEBASE_INDEX.yaml"],
            "creates": ["scripts/validate_module_manifests.py"]
          }
        },
        {
          "task_id": "t-011-manifest-003",
          "name": "Validate existing manifests and report",
          "kind": "script_execution",
          "depends_on": ["t-011-manifest-002"],
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "validation",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 60
          },
          "execution_request_template": {
            "tool": "python",
            "action": "run",
            "params": {
              "script": "scripts/validate_module_manifests.py",
              "args": ["--report-only"],
              "capture_output": true
            }
          },
          "output_artifacts": [
            ".state/manifest_validation_report.txt"
          ]
        },
        {
          "task_id": "t-011-manifest-004",
          "name": "Fix validation errors in existing manifests",
          "kind": "file_edit",
          "depends_on": ["t-011-manifest-003"],
          "allow_parallel": false,
          "classification": {
            "complexity": "moderate",
            "risk_tier": "medium",
            "domain": "documentation",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 300
          },
          "prompt_template_ref": "batch_fix_manifests",
          "prompt_overrides": {
            "instructions": "Based on validation report: Fix all manifests to comply with schema. Add missing required fields (module/purpose/layer/entry_points). Standardize layer values. Ensure entry_points have file/function/description. Add status.maturity for all. Keep existing content, just make schema-compliant."
          },
          "files_scope": {
            "reads": [".state/manifest_validation_report.txt", "**/.ai-module-manifest"],
            "writes": ["**/.ai-module-manifest"]
          }
        },
        {
          "task_id": "t-011-manifest-005",
          "name": "Create generate_module_manifests.py",
          "kind": "code_create",
          "depends_on": ["t-011-manifest-001"],
          "allow_parallel": true,
          "classification": {
            "complexity": "complex",
            "risk_tier": "low",
            "domain": "tooling",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 300
          },
          "prompt_template_ref": "create_python_tool",
          "prompt_overrides": {
            "instructions": "Create scripts/generate_module_manifests.py: (1) Scan CODEBASE_INDEX.yaml for modules, (2) Check if .ai-module-manifest exists, (3) If missing, generate from template using module metadata (name, layer, files), (4) Use AST to find entry points (top-level functions/classes), (5) Generate placeholder purpose/gotchas, (6) Write to module/.ai-module-manifest. CLI: --fill-missing (generate only missing), --regenerate-all (overwrite all), --dry-run. Use template from existing manifests."
          },
          "files_scope": {
            "reads": ["CODEBASE_INDEX.yaml", "**/*.py", "**/.ai-module-manifest"],
            "creates": ["scripts/generate_module_manifests.py", "**/.ai-module-manifest"]
          }
        },
        {
          "task_id": "t-011-manifest-006",
          "name": "Generate missing manifests",
          "kind": "script_execution",
          "depends_on": ["t-011-manifest-005"],
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "automation",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 120
          },
          "execution_request_template": {
            "tool": "python",
            "action": "run",
            "params": {
              "script": "scripts/generate_module_manifests.py",
              "args": ["--fill-missing", "--dry-run"]
            }
          }
        },
        {
          "task_id": "t-011-manifest-007",
          "name": "Final validation: all manifests valid",
          "kind": "validation",
          "depends_on": ["t-011-manifest-004", "t-011-manifest-006"],
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "validation",
            "priority": "P0"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 60
          },
          "execution_request_template": {
            "tool": "python",
            "action": "run",
            "params": {
              "script": "scripts/validate_module_manifests.py",
              "args": ["--strict"],
              "assert_exit_code": 0
            }
          }
        }
      ]
    }
  },
  {
    "op": "add",
    "path": "/phases/PH-011/workstreams/WS-011-VISUAL",
    "value": {
      "workstream_id": "WS-011-VISUAL",
      "workstream_ulid": "01JDKB8ZX12WS011VISUAL01",
      "project_id": "PRJ-COMPLETE_AI_DEVELOPMENT_PIPELINE",
      "phase_id": "PH-011",
      "name": "Visual Aids & Doc Validation",
      "objective": "Generate architecture diagrams, validate doc links, improve visual navigation",
      "tags": ["visualization", "documentation", "validation"],
      "execution": {
        "worktree": {
          "enabled": true,
          "path": ".worktrees/ws-011-visual",
          "branch": "ws/PH-011/visual-aids",
          "base_branch": "main"
        },
        "worker_affinity": "cli_worker_1",
        "conflict_group": "documentation",
        "allow_parallel": true
      },
      "concurrency": {
        "max_parallel": 2
      },
      "error_handling": {
        "strategy": "continue"
      },
      "dependencies": {
        "workstreams": ["WS-011-MANIFESTS"],
        "reason": "Diagrams depend on validated manifests"
      },
      "tasks": [
        {
          "task_id": "t-011-visual-001",
          "name": "Create generate_architecture_diagrams.py",
          "kind": "code_create",
          "allow_parallel": false,
          "classification": {
            "complexity": "complex",
            "risk_tier": "low",
            "domain": "tooling",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 300
          },
          "prompt_template_ref": "create_python_tool",
          "prompt_overrides": {
            "instructions": "Create scripts/generate_architecture_diagrams.py: (1) Read CODEBASE_INDEX.yaml, (2) Generate Graphviz DOT format for module dependencies, (3) Render to PNG/SVG using graphviz library (or subprocess to dot command), (4) Create 2 diagrams: module_dependencies (full DAG), execution_flow (core → error → aim → pm flow). Output to docs/diagrams/. CLI: --format png/svg, --output-dir. Fallback to Mermaid markdown if graphviz unavailable."
          },
          "files_scope": {
            "reads": ["CODEBASE_INDEX.yaml", "**/.ai-module-manifest"],
            "creates": ["scripts/generate_architecture_diagrams.py", "docs/diagrams/", "docs/diagrams/*.png", "docs/diagrams/*.svg"]
          }
        },
        {
          "task_id": "t-011-visual-002",
          "name": "Generate diagrams",
          "kind": "script_execution",
          "depends_on": ["t-011-visual-001"],
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "automation",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 120
          },
          "execution_request_template": {
            "tool": "python",
            "action": "run",
            "params": {
              "script": "scripts/generate_architecture_diagrams.py",
              "args": ["--format", "png", "--output-dir", "docs/diagrams"]
            }
          }
        },
        {
          "task_id": "t-011-visual-003",
          "name": "Create validate_doc_links.py",
          "kind": "code_create",
          "allow_parallel": true,
          "classification": {
            "complexity": "moderate",
            "risk_tier": "low",
            "domain": "tooling",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 240
          },
          "prompt_template_ref": "create_python_tool",
          "prompt_overrides": {
            "instructions": "Create scripts/validate_doc_links.py: (1) Find all .md files, (2) Parse markdown links [text](path) using regex or markdown parser, (3) Check if target files exist (relative path resolution), (4) Report broken links with source:line, (5) Ignore external URLs (http/https). CLI: --fix (convert broken to plaintext), --report-only (default). Exit 1 if broken links found."
          },
          "files_scope": {
            "reads": ["**/*.md"],
            "creates": ["scripts/validate_doc_links.py"]
          }
        },
        {
          "task_id": "t-011-visual-004",
          "name": "Audit and report broken links",
          "kind": "script_execution",
          "depends_on": ["t-011-visual-003"],
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "validation",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 60
          },
          "execution_request_template": {
            "tool": "python",
            "action": "run",
            "params": {
              "script": "scripts/validate_doc_links.py",
              "args": ["--report-only"],
              "capture_output": true
            }
          },
          "output_artifacts": [
            ".state/broken_links_report.txt"
          ]
        },
        {
          "task_id": "t-011-visual-005",
          "name": "Fix broken documentation links",
          "kind": "file_edit",
          "depends_on": ["t-011-visual-004"],
          "allow_parallel": false,
          "classification": {
            "complexity": "moderate",
            "risk_tier": "low",
            "domain": "documentation",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 180
          },
          "prompt_template_ref": "batch_fix_links",
          "prompt_overrides": {
            "instructions": "Based on broken links report: Fix or remove broken markdown links. Options: (1) Update path if file moved, (2) Convert to plaintext if file deleted, (3) Add TODO comment if uncertain. Prioritize fixing links in ARCHITECTURE.md, README.md, CLAUDE.md."
          },
          "files_scope": {
            "reads": [".state/broken_links_report.txt"],
            "writes": ["**/*.md"]
          }
        }
      ]
    }
  },
  {
    "op": "add",
    "path": "/phases/PH-011/workstreams/WS-011-POLISH",
    "value": {
      "workstream_id": "WS-011-POLISH",
      "workstream_ulid": "01JDKB8ZX12WS011POLISH01",
      "project_id": "PRJ-COMPLETE_AI_DEVELOPMENT_PIPELINE",
      "phase_id": "PH-011",
      "name": "Polish & Examples (Optional)",
      "objective": "Add test coverage reporting, refine policies, create examples directory",
      "tags": ["polish", "examples", "optional"],
      "execution": {
        "worktree": {
          "enabled": false,
          "comment": "Low-priority enhancements, work in main"
        },
        "worker_affinity": "cli_worker_2",
        "conflict_group": "polish",
        "allow_parallel": true
      },
      "concurrency": {
        "max_parallel": 2
      },
      "error_handling": {
        "strategy": "continue"
      },
      "dependencies": {
        "workstreams": ["WS-011-TESTS"],
        "reason": "Coverage requires working tests"
      },
      "tasks": [
        {
          "task_id": "t-011-polish-001",
          "name": "Add pytest coverage configuration",
          "kind": "file_edit",
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "testing",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 60
          },
          "prompt_template_ref": "edit_config_file",
          "prompt_overrides": {
            "instructions": "Update pytest.ini: Add to [pytest] section: addopts = --cov=core --cov=error --cov=aim --cov=pm --cov-report=html --cov-report=term --cov-fail-under=70. Create .coveragerc with [run] omit = */tests/*,*/__pycache__/*,*/venv/*."
          },
          "files_scope": {
            "writes": ["pytest.ini"],
            "creates": [".coveragerc"]
          }
        },
        {
          "task_id": "t-011-polish-002",
          "name": "Add coverage gate to QUALITY_GATE.yaml",
          "kind": "file_edit",
          "depends_on": ["t-011-polish-001"],
          "allow_parallel": false,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "quality",
            "priority": "P1"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 60
          },
          "prompt_template_ref": "edit_yaml_config",
          "prompt_overrides": {
            "instructions": "Add to QUALITY_GATE.yaml: gate-test-coverage with id: qc-test-coverage, description: 'Minimum test coverage threshold', command: 'pytest --cov=core --cov=error --cov-fail-under=70 -q', required: false (warning only initially)."
          },
          "files_scope": {
            "writes": ["QUALITY_GATE.yaml"]
          }
        },
        {
          "task_id": "t-011-polish-003",
          "name": "Refine ai_policies.yaml with coordination zone",
          "kind": "file_edit",
          "allow_parallel": true,
          "classification": {
            "complexity": "simple",
            "risk_tier": "low",
            "domain": "policy",
            "priority": "P2"
          },
          "execution": {
            "max_attempts": 1,
            "timeout_seconds": 120
          },
          "prompt_template_ref": "edit_yaml_config",
          "prompt_overrides": {
            "instructions": "Add to ai_policies.yaml edit-zones: new zone 'safe-with-coordination' between safe and review-required. Include: core/state/db_schema.py (coordinate with migrations), config/adapters.yaml (affects all tools), schema/*.json (validate downstream). Policy: 'Can edit directly, but notify team in commit message and run full validation.'"
          },
          "files_scope": {
            "writes": ["ai_policies.yaml"]
          }
        },
        {
          "task_id": "t-011-polish-004",
          "name": "Create examples/ directory with working code",
          "kind": "code_create",
          "allow_parallel": true,
          "classification": {
            "complexity": "moderate",
            "risk_tier": "low",
            "domain": "documentation",
            "priority": "P2"
          },
          "execution": {
            "max_attempts": 2,
            "timeout_seconds": 300
          },
          "prompt_template_ref": "create_example_files",
          "prompt_overrides": {
            "instructions": "Create examples/ with: (1) 01_create_workstream.py - minimal working example of creating workstream with tasks, (2) 02_add_error_plugin.py - skeleton error plugin with parse() method, (3) 03_custom_adapter.py - minimal tool adapter, (4) README.md - index of examples with when to use each. Use real imports from core/error/aim. Keep each example < 100 lines, well-commented."
          },
          "files_scope": {
            "reads": ["core/**/*.py", "error/**/*.py", "aim/**/*.py"],
            "creates": ["examples/", "examples/01_create_workstream.py", "examples/02_add_error_plugin.py", "examples/03_custom_adapter.py", "examples/README.md"]
          }
        }
      ]
    }
  },
  {
    "op": "add",
    "path": "/meta/optimization_rationale",
    "value": {
      "problem_statement": "Codebase is 8.5/10 AI-friendly but has operational bottlenecks that slow AI agents by 25-50 min per session",
      "root_causes": [
        "28 pytest collection errors prevent reliable validation",
        "Missing AI_GUIDANCE.md forces 25 min discovery phase each session",
        "Inconsistent module manifests make navigation slow",
        "No visual aids require mental model reconstruction",
        "Broken doc links create navigation dead-ends"
      ],
      "optimization_approach": "Decision Elimination via Templates",
      "key_optimizations": [
        "Batch test fixes: 28 errors → 0 in one pass (not one-by-one)",
        "Template-based AI_GUIDANCE.md: 150 lines covers 95% of questions",
        "Schema-validated manifests: eliminate 'is this complete?' decision",
        "Auto-generated diagrams: visual understanding in 30 sec vs 5 min",
        "Automated link validation: trust docs vs verify each link"
      ],
      "speedup_calculation": {
        "current_onboarding_time_minutes": 25,
        "with_ai_guidance_minutes": 2,
        "savings_per_session_minutes": 23,
        "sessions_per_week": 10,
        "weekly_savings_hours": 3.8,
        "roi_after_weeks": 2
      },
      "acceptance_criteria_philosophy": "Ground truth over vibes - tests pass, schema valid, files exist. No subjective 'looks good' checks."
    }
  },
  {
    "op": "add",
    "path": "/meta/execution_strategy",
    "value": {
      "parallelism_analysis": {
        "parallel_eligible_workstreams": [
          "WS-011-TESTS (cli_worker_1)",
          "WS-011-GUIDANCE (cli_worker_2)",
          "WS-011-MANIFESTS (cli_worker_3)"
        ],
        "sequential_dependencies": [
          "WS-011-VISUAL depends on WS-011-MANIFESTS (needs validated manifests)",
          "WS-011-POLISH depends on WS-011-TESTS (needs working tests)"
        ],
        "estimated_timeline": {
          "sequential_hours": 12,
          "parallel_hours": 6,
          "speedup": "2x"
        }
      },
      "worktree_isolation_benefits": [
        "Tests, guidance, manifests can work simultaneously without conflicts",
        "Each worker has clean state",
        "Rollback is per-workstream (surgical)",
        "Merge validation catches integration issues"
      ],
      "merge_strategy": "Sequential with test gates at each step",
      "rollback_plan": "If any workstream fails validation, remove its worktree, keep others"
    }
  }
]
