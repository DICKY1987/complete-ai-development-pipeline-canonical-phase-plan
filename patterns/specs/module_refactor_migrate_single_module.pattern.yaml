doc_id: DOC-PAT-MODULE-REFACTOR-MIGRATE-SINGLE-MODULE-052
pattern_id: PAT-MODULE-REFACTOR-MIGRATE-003
name: module_refactor_migrate_single_module
version: 1.0.0
status: active
category: module_refactor
operation_kind: MIGRATE_MODULE

metadata:
  created: "2025-11-28"
  author: "Module Refactor Team"
  purpose: "Safely migrate a single module from legacy paths to modules/<module_id>/"
  time_savings_vs_manual: "92%"
  proven_uses: 0

summary: |
  Migrate one module from legacy locations to the new module-centric structure.
  Includes file moves, registry updates, validation, and rollback capability.

description: |
  This pattern orchestrates the complete migration of a single module:
  1. Create module skeleton (src/, docs/, schemas/, tests/, config/)
  2. Move files from legacy paths
  3. Update registry paths for all affected artifacts
  4. Validate file existence and registry consistency
  5. Run module tests
  6. Commit or rollback
  
  Uses PowerShell recovery points for safe rollback.
  Should be executed once per module in dependency order.

inputs:
  - name: module_id
    type: string
    required: true
    description: "Module ID to migrate (e.g. 'aim_tools', 'execution')"
    
  - name: module_root
    type: path
    required: false
    description: "Target module root directory"
    default: "modules/{{module_id}}"
    
  - name: inventory_path
    type: path
    required: false
    description: "Path to MODULES_INVENTORY.yaml"
    default: "modules/MODULES_INVENTORY.yaml"
    
  - name: docs_inventory_path
    type: path
    required: false
    description: "Path to docs_inventory.jsonl"
    default: ".state/docs_inventory.jsonl"
    
  - name: registry_db_path
    type: path
    required: false
    description: "Path to registry database"
    default: "registry/registry.db"
    
  - name: dry_run
    type: boolean
    required: false
    description: "Perform dry run without actual file moves"
    default: false
    
  - name: run_tests
    type: boolean
    required: false
    description: "Run module tests after migration"
    default: true
    
  - name: auto_commit
    type: boolean
    required: false
    description: "Automatically commit if validation passes"
    default: false

outputs:
  - name: migration_report
    type: path
    description: "Detailed migration report"
    default: "reports/module_migrations/{{module_id}}_migration_{{timestamp}}.md"
    
  - name: recovery_point_id
    type: string
    description: "Recovery point ID for rollback"
    
  - name: files_moved
    type: integer
    description: "Number of files successfully moved"
    
  - name: registry_rows_updated
    type: integer
    description: "Number of registry rows updated"

execution_steps:
  - step: 1
    operation_kind: VALIDATE_PREREQUISITES
    description: "Verify module is defined in inventory and dependencies are met"
    validation:
      - check: file_exists
        path: "{{inventory_path}}"
        error: "MODULES_INVENTORY.yaml not found. Run PAT-MODULE-REFACTOR-INVENTORY-002 first."
      - check: yaml_contains_key
        path: "{{inventory_path}}"
        key: "{{module_id}}"
        error: "Module '{{module_id}}' not defined in inventory"
      - check: dependencies_migrated
        module_id: "{{module_id}}"
        inventory_path: "{{inventory_path}}"
        
  - step: 2
    operation_kind: CREATE_RECOVERY_POINT
    description: "Create PowerShell recovery point before any changes"
    script_language: powershell
    script: |
      $recoveryId = New-RecoveryPoint `
        -ModulePath "." `
        -Description "Pre-migration snapshot for module {{module_id}}"
      
      Write-Output "RECOVERY_POINT_ID=$recoveryId"
      $recoveryId | Set-Content -Path ".state/current_recovery_point.txt"
    capture_output: recovery_point_id
    
  - step: 3
    operation_kind: CREATE_MODULE_SKELETON
    description: "Create directory structure for target module"
    script: |
      import pathlib
      
      module_root = pathlib.Path("{{module_root}}")
      subdirs = ["src", "docs", "schemas", "tests", "config", "examples"]
      
      for subdir in subdirs:
          (module_root / subdir).mkdir(parents=True, exist_ok=True)
      
      # Create README
      readme_path = module_root / "README.md"
      readme_path.write_text(f"""# Module: {{module_id}}
      
      **Module Kind**: {{module_kind}}
      **Migrated**: {{timestamp}}
      
      ## Purpose
      {{module_description}}
      
      ## Structure
      - `src/` - Source code
      - `docs/` - Module documentation
      - `schemas/` - JSON/YAML schemas
      - `tests/` - Unit and integration tests
      - `config/` - Configuration files
      - `examples/` - Usage examples
      
      ## Dependencies
      {{module_dependencies}}
      """, encoding="utf-8")
      
  - step: 4
    operation_kind: BUILD_FILE_MOVE_PLAN
    description: "Determine which files to move based on legacy paths and docs inventory"
    script: |
      import json
      import yaml
      from pathlib import Path
      
      # Load module definition
      with open("{{inventory_path}}", "r", encoding="utf-8") as f:
          inventory = yaml.safe_load(f)
      
      module_def = None
      for category in ["pipeline_stage_modules", "feature_service_modules", "interface_modules"]:
          for mod in inventory.get(category, []):
              if mod["module_id"] == "{{module_id}}":
                  module_def = mod
                  break
      
      if not module_def:
          raise ValueError(f"Module {{module_id}} not found in inventory")
      
      legacy_paths = module_def.get("legacy_paths", [])
      
      # Build move plan from docs inventory
      move_plan = []
      with open("{{docs_inventory_path}}", "r", encoding="utf-8") as f:
          for line in f:
              if not line.strip():
                  continue
              record = json.loads(line)
              
              # Check if this file belongs to this module
              if record.get("module_name") == "{{module_id}}" or \
                 any(record.get("rel_path", "").startswith(lp) for lp in legacy_paths):
                  
                  source_path = record["rel_path"]
                  
                  # Determine target subdir based on file type
                  if source_path.endswith((".md", ".txt")):
                      target_subdir = "docs"
                  elif "schema" in source_path.lower() or source_path.endswith((".yaml", ".json", ".schema")):
                      target_subdir = "schemas"
                  elif "test" in source_path.lower() or source_path.startswith("tests/"):
                      target_subdir = "tests"
                  elif "config" in source_path.lower():
                      target_subdir = "config"
                  else:
                      target_subdir = "src"
                  
                  filename = Path(source_path).name
                  target_path = f"{{module_root}}/{target_subdir}/{filename}"
                  
                  move_plan.append({
                      "source": source_path,
                      "target": target_path,
                      "doc_id": record.get("artifact_id"),
                      "size": record.get("size_bytes")
                  })
      
      # Save move plan
      with open(".state/move_plan_{{module_id}}.json", "w", encoding="utf-8") as out:
          json.dump({"module_id": "{{module_id}}", "moves": move_plan}, out, indent=2)
      
      print(f"Move plan created: {len(move_plan)} files to migrate")
      
  - step: 5
    operation_kind: EXECUTE_FILE_MOVES
    description: "Move files using git mv (preserves history)"
    condition: "not {{dry_run}}"
    script: |
      import json
      import subprocess
      from pathlib import Path
      
      with open(".state/move_plan_{{module_id}}.json", "r", encoding="utf-8") as f:
          plan = json.load(f)
      
      moved_count = 0
      failed_moves = []
      
      for move in plan["moves"]:
          source = Path(move["source"])
          target = Path(move["target"])
          
          if not source.exists():
              print(f"SKIP: {source} does not exist")
              continue
          
          # Ensure target directory exists
          target.parent.mkdir(parents=True, exist_ok=True)
          
          # Use git mv to preserve history
          try:
              result = subprocess.run(
                  ["git", "mv", str(source), str(target)],
                  capture_output=True,
                  text=True,
                  timeout=10
              )
              
              if result.returncode == 0:
                  moved_count += 1
                  print(f"MOVED: {source} → {target}")
              else:
                  failed_moves.append({
                      "source": str(source),
                      "target": str(target),
                      "error": result.stderr
                  })
                  print(f"FAILED: {source} → {target}: {result.stderr}")
          except Exception as e:
              failed_moves.append({
                  "source": str(source),
                  "target": str(target),
                  "error": str(e)
              })
              print(f"ERROR: {source} → {target}: {e}")
      
      result = {
          "moved": moved_count,
          "failed": len(failed_moves),
          "failures": failed_moves
      }
      
      with open(".state/move_result_{{module_id}}.json", "w", encoding="utf-8") as out:
          json.dump(result, out, indent=2)
      
      if failed_moves:
          print(f"\nWARNING: {len(failed_moves)} moves failed")
      
      print(f"\nTotal moved: {moved_count} files")
      
  - step: 6
    operation_kind: UPDATE_REGISTRY_PATHS
    description: "Update registry to point to new module paths"
    condition: "not {{dry_run}}"
    script: |
      import json
      import sqlite3
      from pathlib import Path
      
      # Load move plan
      with open(".state/move_plan_{{module_id}}.json", "r", encoding="utf-8") as f:
          plan = json.load(f)
      
      # Build path mapping
      path_mapping = {move["source"]: move["target"] for move in plan["moves"]}
      
      # Update registry database
      conn = sqlite3.connect("{{registry_db_path}}")
      cursor = conn.cursor()
      
      updated_count = 0
      for old_path, new_path in path_mapping.items():
          cursor.execute("""
              UPDATE registry 
              SET path = ?, 
                  module_id = ?,
                  last_updated = datetime('now')
              WHERE path = ?
          """, (new_path, "{{module_id}}", old_path))
          
          if cursor.rowcount > 0:
              updated_count += cursor.rowcount
      
      conn.commit()
      conn.close()
      
      print(f"Registry updated: {updated_count} rows")
      
      with open(".state/registry_update_{{module_id}}.json", "w", encoding="utf-8") as out:
          json.dump({"updated_rows": updated_count}, out, indent=2)
      
  - step: 7
    operation_kind: VALIDATE_MIGRATION
    description: "Verify all files exist at new locations and registry is consistent"
    validation:
      - check: files_exist_at_targets
        move_plan_path: ".state/move_plan_{{module_id}}.json"
      - check: registry_paths_valid
        module_id: "{{module_id}}"
        registry_db: "{{registry_db_path}}"
      - check: no_orphaned_files
        module_root: "{{module_root}}"
        
  - step: 8
    operation_kind: RUN_TESTS
    description: "Run module tests to verify nothing broke"
    condition: "{{run_tests}}"
    script_language: powershell
    script: |
      $testPath = "{{module_root}}/tests"
      if (Test-Path $testPath) {
          pytest $testPath -v --tb=short
          if ($LASTEXITCODE -ne 0) {
              throw "Module tests failed for {{module_id}}"
          }
      } else {
          Write-Output "No tests found for {{module_id}}, skipping"
      }
      
  - step: 9
    operation_kind: GENERATE_MIGRATION_REPORT
    description: "Create detailed migration report"
    script: |
      import json
      from datetime import datetime
      from pathlib import Path
      
      # Load results
      move_result = json.loads(Path(".state/move_result_{{module_id}}.json").read_text())
      registry_update = json.loads(Path(".state/registry_update_{{module_id}}.json").read_text())
      
      report = f"""# Module Migration Report: {{module_id}}
      
      **Timestamp**: {datetime.now().isoformat()}
      **Module ID**: {{module_id}}
      **Module Root**: {{module_root}}
      
      ## Summary
      - **Files Moved**: {move_result['moved']}
      - **Move Failures**: {move_result['failed']}
      - **Registry Rows Updated**: {registry_update['updated_rows']}
      
      ## Status
      {"✅ MIGRATION SUCCESSFUL" if move_result['failed'] == 0 else "⚠️  MIGRATION COMPLETED WITH WARNINGS"}
      
      ## Files Moved
      {move_result['moved']} files successfully relocated to `{{module_root}}/`
      
      ## Registry Updates
      {registry_update['updated_rows']} registry entries updated with new paths and module_id
      
      ## Next Steps
      1. Review moved files in `{{module_root}}/`
      2. Run `pytest {{module_root}}/tests/` to verify tests
      3. Commit changes: `git commit -m "refactor: migrate {{module_id}} module"`
      
      ## Rollback Instructions
      If issues are found:
      ```powershell
      Restore-RecoveryPoint -RecoveryId {{recovery_point_id}}
      ```
      
      ---
      Generated by PAT-MODULE-REFACTOR-MIGRATE-003
      """
      
      Path("{{migration_report}}").parent.mkdir(parents=True, exist_ok=True)
      Path("{{migration_report}}").write_text(report, encoding="utf-8")
      
      print(report)
      
  - step: 10
    operation_kind: GIT_COMMIT
    description: "Commit migration if auto_commit is enabled"
    condition: "{{auto_commit}}"
    script_language: powershell
    script: |
      git add -A
      git commit -m "refactor: migrate {{module_id}} module to modules/{{module_id}}/"
      Write-Output "✅ Changes committed"

validation_gates:
  - gate: module_skeleton_created
    description: "Module directory structure created"
    check_type: directory_exists
    path: "{{module_root}}"
    
  - gate: files_moved_successfully
    description: "All files moved without errors"
    check_type: json_field_equals
    path: ".state/move_result_{{module_id}}.json"
    field: "failed"
    expected: 0
    
  - gate: registry_updated
    description: "Registry paths updated"
    check_type: json_field_greater_than
    path: ".state/registry_update_{{module_id}}.json"
    field: "updated_rows"
    min_value: 1

success_criteria:
  - "Module skeleton created at {{module_root}}"
  - "All files moved successfully"
  - "Registry paths updated"
  - "Module tests pass (if run_tests=true)"
  - "Migration report generated"

rollback_steps:
  - operation_kind: RESTORE_RECOVERY_POINT
    description: "Restore from recovery point"
    script_language: powershell
    script: |
      $recoveryId = Get-Content ".state/current_recovery_point.txt" -Raw
      Restore-RecoveryPoint -RecoveryId $recoveryId.Trim()

tool_targets:
  - claude_code
  - github_copilot_cli
  - cursor

tags:
  - module_refactor
  - migration
  - file_operations
  - registry_update
  - safety
  
dependencies:
  patterns:
    - PAT-MODULE-REFACTOR-SCAN-001
    - PAT-MODULE-REFACTOR-INVENTORY-002
  scripts:
    - PowerShell recovery point functions
  tools:
    - git
    - python3
    - pytest

notes: |
  CRITICAL SAFETY FEATURES:
  - Creates recovery point before any changes
  - Uses git mv to preserve history
  - Validates at each step
  - Can rollback on failure
  - Dry-run mode available
  
  MIGRATION ORDER:
  Migrate modules in dependency order from MODULE_DEPENDENCIES.yaml:
  1. Independent modules first (registry_core, state_lifecycle)
  2. Feature services (aim_tools, patterns_engine, spec_bridge)
  3. Pipeline stages (intake → planning → scheduling → execution → error → reporting)
  4. Interfaces last (gui_shell)
  
  PREREQUISITES:
  - docs_inventory.jsonl must exist (run PAT-MODULE-REFACTOR-SCAN-001)
  - MODULES_INVENTORY.yaml must exist (run PAT-MODULE-REFACTOR-INVENTORY-002)
  - Module dependencies must be migrated first
  
examples:
  - name: "Dry run migration"
    inputs:
      module_id: "aim_tools"
      dry_run: true
      
  - name: "Full migration with tests"
    inputs:
      module_id: "aim_tools"
      dry_run: false
      run_tests: true
      auto_commit: false
      
  - name: "Automated migration"
    inputs:
      module_id: "registry_core"
      dry_run: false
      run_tests: true
      auto_commit: true
