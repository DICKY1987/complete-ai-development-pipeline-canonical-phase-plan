---
doc_id: DOC-PAT-TERMINAL-CHAT-BEST-PLAN-12-730
---

Here’s a complete first version of `doc_inventory_scan_and_enrich.py`:

````python
#!/usr/bin/env python3
"""
doc_inventory_scan_and_enrich.py

Scan a repository for Markdown / text documents and build an enriched
docs_inventory.jsonl index suitable for AI-aware tooling.

Key features:
- Walks the tree from a given root and finds all .md / .txt files.
- Computes basic file metadata (size, mtime, hash).
- Classifies each document by module, folder role, and lifecycle layer.
- Extracts lightweight semantic hints:
  - title (front matter title or first H1 heading or filename)
  - first few Markdown headings
  - YAML front matter keys (if present)
  - ID candidates (doc_id, pattern_id, module_id, workstream_id, etc.)
  - keyword hits (pattern, workstream, architecture, spec, error, aim, ccpm, openspec, etc.)
  - whether the file contains Mermaid diagrams
  - whether the file appears to contain ULID-like identifiers
- Writes one JSON object per line to docs_inventory.jsonl

Intended usage:
    python doc_inventory_scan_and_enrich.py
    python doc_inventory_scan_and_enrich.py /path/to/repo
    python doc_inventory_scan_and_enrich.py --output .state/docs_inventory.jsonl
"""

from __future__ import annotations

import argparse
import datetime as _dt
import hashlib
import json
import os
from pathlib import Path
import re
import sys
from typing import Dict, Iterable, Iterator, List, Optional, Sequence, Set, Tuple


DEFAULT_EXTENSIONS: Tuple[str, ...] = (".md", ".markdown", ".txt")
DEFAULT_EXCLUDE_DIRS: Set[str] = {
    ".git",
    ".hg",
    ".svn",
    ".idea",
    ".vscode",
    ".venv",
    "venv",
    "env",
    "__pycache__",
    "node_modules",
    "dist",
    "build",
    ".mypy_cache",
    ".pytest_cache",
    ".tox",
    ".sync",
}

# Top-level folder → semantic kind label
TOP_LEVEL_KIND: Dict[str, str] = {
    # Input / entry
    "workstreams": "workstream-spec",
    "pm": "project-management",
    "aim": "aim-tools",
    # Config
    "schema": "schema-contracts",
    "config": "config",
    "templates": "templates",
    "docs": "docs",
    "registry": "registry",
    # Core / domain / execution
    "core": "core-domain",
    "engine": "execution-engine",
    "error": "error-engine",
    "specifications": "specifications",
    # Infra
    "scripts": "automation-scripts",
    "infra": "ci-cd-infra",
    "state": "runtime-state",
    "logs": "logs",
    "tools": "tools",
    # Output / interface
    "gui": "gui",
    "modules": "modules-output",
    "archive": "archive",
    # GitHub / meta
    ".github": "github-meta",
}

# Top-level folder → lifecycle layer, based on PIPELINE FOLDER LIFECYCLE
LIFECYCLE_LAYER: Dict[str, str] = {
    # Input
    "workstreams": "input",
    "pm": "input",
    "aim": "input",
    # Config
    "schema": "config",
    "config": "config",
    "templates": "config",
    "docs": "config",
    "registry": "config",
    # Core
    "core": "core-domain",
    "error": "core-domain",
    "specifications": "core-domain",
    # Execution
    "engine": "execution",
    # Infra
    "scripts": "infra",
    "infra": "infra",
    "state": "infra",
    "logs": "infra",
    "tools": "infra",
    ".github": "infra",
    # Output
    "gui": "output",
    "modules": "output",
    "archive": "output",
}


# Keyword detectors used for enrichment
KEYWORD_PATTERNS: Dict[str, re.Pattern] = {
    "pattern": re.compile(r"\bpattern(s)?\b", re.IGNORECASE),
    "workstream": re.compile(r"\bworkstream(s)?\b", re.IGNORECASE),
    "uet": re.compile(r"\bUET\b", re.IGNORECASE),
    "spec": re.compile(r"\b(spec|specification|OpenSpec)\b", re.IGNORECASE),
    "architecture": re.compile(r"\b(architecture|diagram)\b", re.IGNORECASE),
    "error": re.compile(r"\berror(s)?\b", re.IGNORECASE),
    "aim": re.compile(r"\bAIM\b", re.IGNORECASE),
    "ccpm": re.compile(r"\bCCPM\b", re.IGNORECASE),
    "openspec": re.compile(r"\bOpenSpec\b", re.IGNORECASE),
}

ULID_PATTERN = re.compile(r"\b[0-9A-HJKMNP-TV-Z]{26}\b")


def iter_documents(
    root: Path,
    include_extensions: Sequence[str],
    exclude_dirs: Set[str],
    follow_symlinks: bool = False,
) -> Iterator[Path]:
    """Yield all document paths under root matching the include_extensions."""
    include_set = {ext.lower() for ext in include_extensions}

    for dirpath, dirnames, filenames in os.walk(root, followlinks=follow_symlinks):
        # Prune excluded directories in-place
        dirnames[:] = [d for d in dirnames if d not in exclude_dirs]

        base = Path(dirpath)
        for name in filenames:
            path = base / name
            if not path.is_file():
                continue
            ext = path.suffix.lower()
            if ext in include_set:
                yield path


def compute_sha256(path: Path, chunk_size: int = 1024 * 1024) -> str:
    """Compute SHA256 hash of a file incrementally."""
    h = hashlib.sha256()
    with path.open("rb") as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()


def read_preview(
    path: Path,
    max_chars: int,
) -> str:
    """
    Read up to max_chars of text from file using UTF-8 with replacement.

    This is intentionally forgiving and lossy: we care more about getting
    enough signal than preserving exact bytes.
    """
    try:
        with path.open("r", encoding="utf-8", errors="replace") as f:
            return f.read(max_chars)
    except OSError as exc:
        sys.stderr.write(f"[WARN] Failed to read preview for {path}: {exc}\n")
        return ""


def extract_frontmatter(preview: str) -> Tuple[Optional[Dict[str, str]], str]:
    """
    Extract simple YAML front matter if present.

    We don't attempt full YAML parsing; we only capture key names for enrichment.
    Returns (frontmatter_dict_or_none, remaining_text).
    """
    if not preview.startswith("---"):
        return None, preview

    lines = preview.splitlines()
    fm_lines: List[str] = []
    body_lines: List[str] = []
    in_fm = False
    done = False

    for line in lines:
        if not in_fm:
            if line.strip() == "---":
                in_fm = True
            else:
                # No front matter header at very top
                return None, preview
        else:
            if line.strip() == "---":
                done = True
                continue
            if not done:
                fm_lines.append(line)
            else:
                body_lines.append(line)

    if not in_fm or not done:
        # Unterminated front matter; treat as normal text
        return None, preview

    frontmatter: Dict[str, str] = {}
    key_re = re.compile(r"^([A-Za-z0-9_.-]+)\s*:\s*(.*)$")
    for raw in fm_lines:
        m = key_re.match(raw)
        if not m:
            continue
        key, value = m.group(1), m.group(2)
        frontmatter[key] = value.strip()

    body = "\n".join(body_lines) if body_lines else ""
    return frontmatter or None, body


def extract_headings(
    text: str,
    max_headings: int = 5,
) -> List[str]:
    """Return the first few Markdown headings (#, ##, ### ...) found in text."""
    headings: List[str] = []
    for line in text.splitlines():
        stripped = line.lstrip()
        if stripped.startswith("#"):
            # Normalize: collapse multiple leading # and whitespace
            title = stripped.lstrip("#").strip()
            if title:
                headings.append(title)
                if len(headings) >= max_headings:
                    break
    return headings


def detect_ids(text: str) -> List[str]:
    """Find candidate IDs from common ID keys and ULID-like tokens."""
    ids: List[str] = []

    # YAML-ish id: doc_id: ..., pattern_id: ...
    id_key_re = re.compile(
        r"^(doc_id|pattern_id|module_id|workstream_id|spec_id)\s*:\s*(.+)$",
        re.MULTILINE,
    )
    for m in id_key_re.finditer(text):
        key, value = m.group(1), m.group(2).strip()
        if value:
            ids.append(f"{key}:{value}")

    # ULID-like tokens
    for m in ULID_PATTERN.finditer(text):
        ids.append(f"ulid:{m.group(0)}")

    return ids


def detect_keywords(text: str, path: Path) -> List[str]:
    """Return a list of keyword labels that appear in either the text or the path."""
    hits: Set[str] = set()
    lowered_path = str(path).lower()

    # Path-based hints
    if "diagram" in lowered_path or "architecture" in lowered_path:
        hits.add("diagram")
        hits.add("architecture")
    if "pattern" in lowered_path:
        hits.add("pattern")
    if "workstream" in lowered_path:
        hits.add("workstream")
    if "error" in lowered_path:
        hits.add("error")
    if "spec" in lowered_path:
        hits.add("spec")
    if "aim" in lowered_path:
        hits.add("aim")
    if "ccpm" in lowered_path:
        hits.add("ccpm")
    if "openspec" in lowered_path:
        hits.add("openspec")

    # Content-based hits
    for label, pattern in KEYWORD_PATTERNS.items():
        if pattern.search(text):
            hits.add(label)

    return sorted(hits)


def classify_path(
    repo_root: Path,
    path: Path,
) -> Dict[str, Optional[str]]:
    """
    Classify a document path into:
      - rel_path (POSIX)
      - top_level (first directory)
      - module_kind (semantic label)
      - lifecycle_layer (input/config/core/execution/infra/output)
      - module_root (for modules/, e.g. modules/pipeline)
      - module_name (e.g. 'pipeline', 'core', 'engine')
    """
    try:
        rel_path = path.relative_to(repo_root)
    except ValueError:
        # Shouldn't happen if we always pass files under repo_root
        rel_path = path

    parts = list(rel_path.parts)
    rel_posix = rel_path.as_posix()

    top_level = parts[0] if parts else None
    module_kind = TOP_LEVEL_KIND.get(top_level or "", "unknown")
    lifecycle_layer = LIFECYCLE_LAYER.get(top_level or "", "unknown")

    module_root: Optional[str] = None
    module_name: Optional[str] = None

    if not parts:
        module_root = None
        module_name = None
    elif top_level == "modules" and len(parts) >= 2:
        module_name = parts[1]
        module_root = Path(*parts[:2]).as_posix()
    else:
        module_name = top_level
        module_root = top_level

    return {
        "rel_path": rel_posix,
        "top_level": top_level,
        "module_kind": module_kind,
        "lifecycle_layer": lifecycle_layer,
        "module_root": module_root,
        "module_name": module_name,
    }


def derive_title(
    path: Path,
    headings: Sequence[str],
    frontmatter: Optional[Dict[str, str]],
) -> str:
    """Select the best-guess title for the document."""
    # 1) Front matter title
    if frontmatter and "title" in frontmatter and frontmatter["title"].strip():
        return frontmatter["title"].strip()

    # 2) First heading
    if headings:
        return headings[0]

    # 3) Fallback to filename without extension
    return path.stem


def bool_flag(value: bool) -> bool:
    """Tiny helper to make intent explicit in dataclass-like construction."""
    return bool(value)


def build_record(
    repo_root: Path,
    path: Path,
    max_preview_chars: int,
    max_headings: int,
    compute_hash: bool,
) -> Dict[str, object]:
    """
    Build a single inventory record for a path.

    This function is intentionally side-effect-free besides file IO.
    """
    stat = path.stat()
    preview = read_preview(path, max_preview_chars)
    frontmatter, body = extract_frontmatter(preview)
    headings = extract_headings(body or preview, max_headings=max_headings)
    ids = detect_ids(preview)
    keywords = detect_keywords(preview, path)
    has_mermaid = "```mermaid" in preview
    has_ulid = bool(ULID_PATTERN.search(preview))

    classification = classify_path(repo_root, path)
    title = derive_title(path, headings, frontmatter)

    record: Dict[str, object] = {
        # Identity / location
        "abs_path": str(path.resolve()),
        "rel_path": classification["rel_path"],
        "top_level": classification["top_level"],
        "module_root": classification["module_root"],
        "module_name": classification["module_name"],
        "module_kind": classification["module_kind"],
        "lifecycle_layer": classification["lifecycle_layer"],
        # File stats
        "name": path.name,
        "ext": path.suffix.lower(),
        "size_bytes": stat.st_size,
        "mtime_iso": _dt.datetime.fromtimestamp(stat.st_mtime).isoformat(),
        # Content hints
        "title": title,
        "headings": headings,
        "frontmatter_keys": sorted(list(frontmatter.keys())) if frontmatter else [],
        "has_frontmatter": bool_flag(frontmatter is not None),
        "id_candidates": ids,
        "has_ids": bool_flag(bool(ids)),
        "has_mermaid": bool_flag(has_mermaid),
        "has_ulid": bool_flag(has_ulid),
        "keyword_hits": keywords,
        # Small content preview for AI routing / classification
        "preview": preview,
    }

    if compute_hash:
        record["sha256"] = compute_sha256(path)

    return record


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Scan repository documents and build an enriched docs_inventory.jsonl",
    )
    parser.add_argument(
        "root",
        nargs="?",
        default=".",
        help="Repository root to scan (default: current directory)",
    )
    parser.add_argument(
        "-o",
        "--output",
        default=".state/docs_inventory.jsonl",
        help="Output JSONL file path (default: .state/docs_inventory.jsonl)",
    )
    parser.add_argument(
        "--include-ext",
        nargs="*",
        default=list(DEFAULT_EXTENSIONS),
        help="File extensions to include (default: .md .markdown .txt)",
    )
    parser.add_argument(
        "--max-preview-chars",
        type=int,
        default=4000,
        help="Maximum number of characters to read per file for preview/enrichment (default: 4000)",
    )
    parser.add_argument(
        "--max-headings",
        type=int,
        default=5,
        help="Maximum number of Markdown headings to capture per file (default: 5)",
    )
    parser.add_argument(
        "--no-hash",
        action="store_true",
        help="Do not compute SHA256 hashes (faster on large repositories)",
    )
    parser.add_argument(
        "--follow-symlinks",
        action="store_true",
        help="Follow symlinks when walking the tree (default: do not follow)",
    )
    parser.add_argument(
        "--exclude-dir",
        nargs="*",
        default=[],
        help="Additional directory names to exclude from the scan",
    )
    return parser.parse_args(argv)


def ensure_parent_dir(path: Path) -> None:
    parent = path.parent
    if not parent.exists():
        parent.mkdir(parents=True, exist_ok=True)


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)

    repo_root = Path(args.root).resolve()
    if not repo_root.exists() or not repo_root.is_dir():
        sys.stderr.write(f"[ERROR] Root path does not exist or is not a directory: {repo_root}\n")
        return 1

    output_path = Path(args.output)
    ensure_parent_dir(output_path)

    include_exts = [ext if ext.startswith(".") else f".{ext}" for ext in args.include_ext]
    exclude_dirs = set(DEFAULT_EXCLUDE_DIRS)
    for name in args.exclude_dir:
        if name:
            exclude_dirs.add(name)

    compute_hash = not args.no_hash

    sys.stderr.write(
        f"[INFO] Scanning documents under {repo_root} -> {output_path}\n"
        f"[INFO] Include extensions: {', '.join(include_exts)}\n"
        f"[INFO] Exclude directories: {', '.join(sorted(exclude_dirs))}\n"
    )

    count = 0
    with output_path.open("w", encoding="utf-8", newline="\n") as out_f:
        for doc_path in iter_documents(
            repo_root,
            include_extensions=include_exts,
            exclude_dirs=exclude_dirs,
            follow_symlinks=args.follow_symlinks,
        ):
            record = build_record(
                repo_root=repo_root,
                path=doc_path,
                max_preview_chars=args.max_preview_chars,
                max_headings=args.max_headings,
                compute_hash=compute_hash,
            )
            out_f.write(json.dumps(record, ensure_ascii=False, sort_keys=True))
            out_f.write("\n")
            count += 1

    sys.stderr.write(f"[INFO] Wrote {count} records to {output_path}\n")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
````

This script’s classification (module_kind and lifecycle_layer) matches your **pipeline folder lifecycle** (input/config/core/execution/infra/output) and your **module-centric** “everything for a module lives inside the module folder” architecture, so the inventory naturally lines up with the refactor you’re doing.
