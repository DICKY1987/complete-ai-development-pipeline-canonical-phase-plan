---
doc_id: DOC-PAT-WHERE-ABSTRACTION-FITS-696
---


 **where abstraction fits** in *your* pipeline and then give a **concrete “abstraction plan”** you can implement.

---

## 1. Where you’re *already* using abstraction

From the README alone you can see you’ve already done a big abstraction move: the **section-based, module-centric architecture**:

* `core/state/` – DB, CRUD, bundles, worktree mgmt
* `core/engine/` – orchestrator, scheduler, executor, tools adapter, recovery
* `error/engine/`, `error/plugins/` – error detection/analysis
* `aim/`, `pm/`, `openspec/`, `aider/` – domain-specific integration bridges
* `config/`, `schema/`, `workstreams/`, `workstreams_uet/` – declarative configs and inputs ([GitHub][1])

Plus:

* A **job-based execution engine** with a clear orchestrator entry point and 4 adapters (Aider, Codex, Tests, Git). ([GitHub][1])
* The **UET framework** as a pattern/phase abstraction layer over “how” execution actually happens. ([GitHub][1])
* A strong “path abstraction” via CI path standards that forbid legacy imports and enforce `core.*` / `error.*` ([GitHub][1])

So the *structural* abstraction is solid. Where you can go further is: **turn those conceptual boundaries into explicit *interfaces*** that code (and AI agents) must go through, instead of letting everyone poke the internals.

---

## 2. Key abstraction targets in this repo

### 2.1 Tool adapters: one clean `ToolAdapter` interface

**Today**

* You have 4 adapters: Aider, Codex, Tests, Git, wired into the engine as “adapters available.” ([GitHub][1])
* Config for tool profiles & breakers lives in `config/`. ([GitHub][1])

Likely, the orchestrator knows *too much* about each specific tool (CLI args, env vars, quirks, etc.).

**Abstraction to add**

Define a **single adapter protocol** in `core/engine/tools.py`, something like:

```python
from typing import Protocol, Any, Dict

class ToolAdapter(Protocol):
    name: str

    def supports(self, capabilities: set[str]) -> bool:
        ...

    def prepare_job(self, job: dict) -> dict:
        """Transform generic job spec into tool-specific payload."""
        ...

    def run(self, payload: dict, *, dry_run: bool = False) -> dict:
        """Execute the tool and return a normalized result."""
        ...

    def normalize_result(self, raw: Any) -> dict:
        """Map tool-specific output → standard execution result schema."""
        ...
```

Then:

* Each concrete adapter (`AiderAdapter`, `CodexAdapter`, `GitAdapter`, `TestRunnerAdapter`) implements this.
* The **orchestrator only talks to `ToolAdapter`**, not to `subprocess` calls or tool-specific flags.
* A `ToolRegistry` in `core/engine/tools_registry.py` reads `config/tool_profiles.yaml` and returns the right adapter instance.

**Why this improves the system**

* Adding a new tool = “implement `ToolAdapter` + config entry”, no orchestrator surgery.
* All logging/metrics can be attached at the `ToolAdapter` layer.
* AI agents can be told: *“Never call Aider directly; always request a TOOL_ADAPTER invocation with these fields.”* That’s a *behavioral abstraction* that protects you from prompt drift.

---

### 2.2 State & DB: a `StateStore` abstraction instead of raw DB calls

**Today**

* `core/state/` owns DB, CRUD operations, bundles, worktree management. ([GitHub][1])
* Scripts like `test_state_store.py` validate behavior. ([GitHub][1])

But other modules (engine, error, UET, tools) may still know file paths, table names, or SQL details.

**Abstraction to add**

Create a **single `StateStore` interface** that hides the DB/filesystem details:

```python
class StateStore(Protocol):
    def get_workstream(self, ws_id: str) -> dict: ...
    def save_workstream(self, ws: dict) -> None: ...

    def record_execution(self, record: dict) -> None: ...
    def list_executions(self, filters: dict) -> list[dict]: ...

    def get_repo_snapshot(self, snapshot_id: str) -> dict: ...
    def save_repo_snapshot(self, snapshot: dict) -> None: ...
```

* Engine, error engine, pattern automation, GUI panels all depend on `StateStore`.
* Actual implementation (`SQLiteStateStore`, `FileStateStore`) lives in `core/state/impl_*.py`.

**Why this improves the system**

* You can change the DB layout, add indexes, or move from SQLite → Postgres without touching engine/error/pattern code.
* All invariants (e.g., “workstream IDs are unique”, “execution logs are append-only”) live behind this choke point.
* AI tools get a clean mental model: *“State lives behind `StateStore`; you never talk to the DB directly.”*

---

### 2.3 Workstreams & jobs: a `WorkstreamRepository` abstraction

**Today**

* Workstreams & job specs live under `schema/`, `workstreams/`, `workstreams_uet/`. ([GitHub][1])
* Scripts like `spec_to_workstream.py`, `validate_workstreams.py`, `run_workstream.py` know about paths, IDs, and JSON shapes. ([GitHub][1])

That means the *file layout* and *schema details* are leaking into multiple places.

**Abstraction to add**

Define a **repository-style abstraction**:

```python
class WorkstreamRepository(Protocol):
    def list_ids(self) -> list[str]: ...
    def load(self, ws_id: str) -> dict: ...
    def save(self, ws_id: str, data: dict) -> None: ...
    def archive(self, ws_id: str) -> None: ...
```

Implementation:

* `FileSystemWorkstreamRepo` knows whether it’s `.json` or `.yaml`, what directory, how to handle archived vs active.
* `UETWorkstreamRepo` wraps the UET-specific layout in `workstreams_uet/`.

**Why this improves the system**

* Changing the storage structure is a repo-only refactor; no changes to engine/GUI/CLI.
* You can add higher-level operations (like “duplicate workstream with new ID” or “apply scaffold pattern”) right here.
* AI agents get a stable mental API: *“To get a workstream, call `WorkstreamRepository.load(ws-id)`,”* not “go find this JSON file under path X with naming convention Y.”

---

### 2.4 Patterns & phase plans: a `PatternEngine` abstraction

**Today**

* You have the **UET framework** for execution templates + integration docs under `UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/docs/integration/` ([GitHub][1])
* `PATTERN_FILES.md`, `PRODUCTION_VALIDATION_SUITE.md`, and various completion/automation docs describe patterns for merge safety, validation, etc. ([GitHub][1])

Right now, patterns live mostly as **documents + scripts**, not a single code-level “pattern engine”.

**Abstraction to add**

Create a `PatternEngine` or `ExecutionPatternRunner` in something like `core/planning/pattern_engine.py`:

```python
class PatternEngine(Protocol):
    def list_patterns(self, kind: str | None = None) -> list[str]: ...
    def render(self, pattern_id: str, context: dict) -> str:
        """Fill in a pattern template with a context dict."""
    def execute(self, pattern_id: str, context: dict) -> dict:
        """High-level: render + send to appropriate tool + log execution."""
```

Under the hood:

* It knows how to find UET patterns, classic text patterns, YAML pattern files, etc.
* It calls the **ToolAdapter** + **StateStore** abstractions instead of directly running tools.

**Why this improves the system**

* Engine doesn’t need to know *how* patterns are stored (YAML, MD, JSON) or which tool should run them.
* All pattern-based decisions (e.g., which “safe merge pattern” to use) live in one place.
* You can switch from “prompt-based pattern” to “symbolic action pattern” without rewriting orchestrator calls.

---

### 2.5 Doc IDs & registries: a `DocRegistry` abstraction

**Today**

* You have a `doc_id/` section and docs like `PLAN_DOC_ID_COMPLETION_001.md` and `PLAN_DOC_ID_COMPLETION_001_FILE_CHANGES.md` that describe how doc IDs are used and the changes required to apply them repo-wide. ([GitHub][1])

But doc ID logic is probably scattered across scripts / AI prompts / mental rules.

**Abstraction to add**

Define a `DocRegistry` (or `DocIdService`) that is the *only* API for:

* Generating new IDs
* Validating uniqueness
* Mapping ID ↔ path
* Reporting missing IDs

Example:

```python
class DocRegistry(Protocol):
    def get_by_id(self, doc_id: str) -> "DocMeta": ...
    def find_by_path(self, path: str) -> "DocMeta | None": ...
    def allocate(self, kind: str, path: str) -> "DocMeta": ...
    def validate_all(self) -> list["DocIssue"]:
        """Returns missing IDs, duplicates, stale mappings, etc."""
```

Internally, it manages your `docs_inventory.jsonl`/YAML/whatever you end up with.

**Why this improves the system**

* “Do all files have IDs?” becomes a single call to `validate_all()`, not a custom script each time.
* AI assistants can be told: *“To create a new doc, call the DOC_REGISTRY pattern, never invent IDs.”* That’s abstraction protecting the integrity of the ID system.
* If you change your ID scheme, you only rework `DocRegistry` and ID migration tools, not every script.

---

### 2.6 Frontends (CLI / TUI / GUI): an `ExecutionService` facade

**Today**

* You have a hybrid GUI/Terminal architecture planned, a TUI under `tui_app/`, and multiple scripts in `scripts/` as the current CLI surface. ([GitHub][1])
* GUI docs: `HYBRID_GUI_PLAN.md`, `GUI_PLAN_EXECUTION_PATTERNS.md`, `GUI_MODULE_ANALYSIS_SUMMARY.md`, etc., describe panels and behavior. ([GitHub][1])

Each frontend risks knowing too much about the orchestrator, state files, pattern layout, etc.

**Abstraction to add**

Create a single **application service layer** (call it `ExecutionService`) in e.g. `core/app/service.py`:

```python
class ExecutionService(Protocol):
    def run_workstream(self, ws_id: str) -> str: ...
    def run_phase_plan(self, plan_id: str) -> str: ...
    def get_execution_report(self, run_id: str) -> dict: ...
    def list_recent_runs(self, limit: int = 20) -> list[dict]: ...
```

Then:

* CLI scripts import and use `ExecutionService`.
* TUI & GUI call the same interface.
* ExecutionService internally uses `PatternEngine`, `ToolAdapter`s, `StateStore`, `WorkstreamRepository`.

**Why this improves the system**

* You get **one abstraction that defines “what the system does”**: run workstreams, run phase plans, show results.
* You can rewrite the engine internals without touching GUI/TUI/CLI, as long as `ExecutionService` stays stable.
* AI tools can be oriented to this layer: *“Your job is to assemble calls to ExecutionService, not to modify orchestrator code directly.”*

---

### 2.7 Git/worktree & safe merge: a `RepoSyncService` abstraction

You already have:

* `SAFE_MERGE_STRATEGY.md`, `MERGE_QUICKSTART.md`, `MERGE_COMPLETION_REPORT.md`, `LATEST_MERGE_COMPLETION.md`, etc., describing your safe merge automation and worktree usage. ([GitHub][1])

Right now, the logic is partly in docs, partly in scripts, partly in CLI AI conversations.

**Abstraction to add**

Create `RepoSyncService` under `core/state/repo_sync.py`:

```python
class RepoSyncService(Protocol):
    def snapshot(self, reason: str) -> str: ...
    def safe_merge(self, source: str, target: str) -> "MergeResult": ...
    def sync_worktree(self, worktree: str) -> None: ...
```

* It encapsulates:

  * worktree creation
  * snapshot (tags/branches)
  * merge heuristics (timestamps, allowed file types)
  * rollback rules

**Why this improves the system**

* All your “bulletproof safe merge” rules live in *one* choke point.
* Engine / tools / AI scripts ask this service to “sync repo”, instead of manually running git plumbing.
* You can evolve the merge strategy (e.g., more file-level heuristics) without touching the orchestrator or AI prompts.

---

## 3. How this concretely improves your system

Tie this back to the abstraction definition:

> “Define a stable **what**, hide the volatile **how**.”

These changes give you:

1. **Stable contracts** between:

   * Patterns ↔ Tools
   * Engine ↔ State/DB
   * Frontends (CLI/TUI/GUI) ↔ Execution core
   * Docs/workstreams ↔ registries

2. **Freedom to experiment behind the scenes**

   * Swap CLIs, change DB schema, tweak safe-merge heuristics, refine pattern formats — without rewriting every consumer.

3. **Much safer AI automation**

   * You can tell any AI agent:

     * “Use `ToolAdapter` APIs, not raw shell commands.”
     * “Use `DocRegistry` to allocate IDs, never invent them.”
     * “Use `ExecutionService` instead of touching orchestrator internals.”
   * That’s exactly the kind of abstraction that protects the system from “helpful but chaotic AI activity.”

---

## 4. Suggested next steps (small, concrete)

If you want to make this actionable without a full rewrite:

1. **Pick one layer to abstract first**

   * My vote: `ToolAdapter` or `StateStore`, because they’re central and give immediate safety.

2. **Write a 1-page “Interface Contract” doc for it**

   * E.g. `specifications/ABSTRACTION_TOOL_ADAPTER_CONTRACT.md`
   * Include:

     * Methods
     * Input/output schemas
     * Error behavior
     * Logging/metrics hooks

3. **Create a thin Python interface/protocol and update 1–2 call sites**

   * Don’t refactor everything at once; prove the abstraction on a small path.

4. **Add an AI-facing prompt snippet**

   * E.g. in `.ai-context.md` / NAV docs:

     * “When you need to run a tool, request a ToolAdapter call of this shape…”

That’s enough to start turning your current *conceptual* abstraction into **hard, enforceable boundaries** — which is where your whole pattern-first, module-centric pipeline really shines.



[1]: https://github.com/DICKY1987/complete-ai-development-pipeline-canonical-phase-plan "GitHub - DICKY1987/complete-ai-development-pipeline-canonical-phase-plan: Complete AI Development Pipeline – Canonical Phase Plan"
