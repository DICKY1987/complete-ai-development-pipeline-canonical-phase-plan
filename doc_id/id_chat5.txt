You’re remembering exactly right — this *is* the “Phase 0: auto-assign IDs to everything” we designed in the ID framework exploration. 

Right now you already have:

* A solid ID framework + registry + CLI. 
* A **scanner** that finds all eligible files and marks which ones have / don’t have `doc_id`, and writes `docs_inventory.jsonl`. 
* A coverage snapshot showing ~2,514 eligible files, ~6.1% coverage, and ~2,360 still missing IDs. 

What you’re asking now (“add id to every file and register them”) is exactly the **Phase-0 auto-assigner** we scoped in the roadmap. 

Below is a concrete, drop-in plan **plus** a starter `scripts/doc_id_assigner.py` you can paste into your repo.

---

## 1. High-level game plan

1. **Re-scan** the repo to refresh `docs_inventory.jsonl`:

```bash
python scripts/doc_id_scanner.py scan
python scripts/doc_id_scanner.py stats
```

2. **Add** `scripts/doc_id_assigner.py` (code below).

3. **Dry-run** the auto-assigner on a small subset to sanity-check:

```bash
python scripts/doc_id_assigner.py auto-assign --dry-run --limit 25 --types py md yaml json ps1 sh txt
```

4. Inspect the printed preview + `git diff` to make sure the injections look right.

5. **Run for real** (no `--dry-run`) to assign IDs and update the registry:

```bash
python scripts/doc_id_assigner.py auto-assign --types py md yaml json ps1 sh txt
```

6. **Re-scan and validate**:

```bash
python scripts/doc_id_scanner.py scan
python scripts/doc_id_scanner.py stats        # expect ~100% coverage
python doc_id/doc_id_registry_cli.py validate
python doc_id/doc_id_registry_cli.py stats
```

That’s the “Phase 0: 6% → 100% coverage in one sweep” that the analysis docs recommended before module refactors. 

---

## 2. Auto-assigner script (drop into `scripts/doc_id_assigner.py`)

This script:

* Reads `docs_inventory.jsonl` (from the scanner). 
* Finds entries with `status == "missing"`.
* Infers a **category** from the path (core / spec / config / script / test / guide / aim / pm / etc.) based on the registry’s categories.
* Infers a **name** and **title** for the registry entry.
* Uses the existing `DocIDRegistry` from `doc_id/doc_id_registry_cli.py` to mint the `doc_id` and update `DOC_ID_REGISTRY.yaml`. 
* Injects the new `doc_id` into the file in a type-specific way:

  * `.py`: adds `DOC_ID:` in the module docstring or a `# DOC_LINK:` header.
  * `.md`: adds or updates YAML frontmatter with `doc_id: ...`.
  * `.yaml`/`.yml`: prepends a `doc_id:` line.
  * `.json`: adds `"doc_id": "..."` to the top-level object.
  * `.ps1`/`.sh`: adds `# DOC_LINK: ...` near the top.
  * `.txt`: uses a tiny YAML frontmatter block.

> **Important:** this is a **starting point**. You can tighten the heuristics (categories, naming, injection rules) as you see the actual diffs.

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Doc ID Auto-Assigner

PURPOSE:
    Use docs_inventory.jsonl + DOC_ID_REGISTRY.yaml to assign doc_ids to all
    eligible files that are currently missing them, and inject the IDs into
    the files in-place.

PATTERN: PAT-DOC-ID-AUTOASSIGN-002

USAGE:
    python scripts/doc_id_assigner.py auto-assign --dry-run
    python scripts/doc_id_assigner.py auto-assign --limit 50 --dry-run
    python scripts/doc_id_assigner.py auto-assign
"""

import argparse
import json
import sys
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# Assuming this file lives in scripts/ at repo root
REPO_ROOT = Path(__file__).parent.parent
INVENTORY_PATH = REPO_ROOT / "docs_inventory.jsonl"


# --- Registry module loader -------------------------------------------------


def _load_registry_module():
    """
    Load doc_id_registry_cli.py as a module so we can reuse DocIDRegistry
    without spawning a subprocess for every file.

    This expects:
        repo_root/doc_id/doc_id_registry_cli.py
    """
    registry_path = REPO_ROOT / "doc_id" / "doc_id_registry_cli.py"
    if not registry_path.exists():
        print(f"[ERROR] Could not find registry CLI at {registry_path}", file=sys.stderr)
        sys.exit(1)

    import importlib.util

    spec = importlib.util.spec_from_file_location("doc_id_registry_cli", registry_path)
    if spec is None or spec.loader is None:
        print("[ERROR] Failed to load doc_id_registry_cli module", file=sys.stderr)
        sys.exit(1)

    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)  # type: ignore[union-attr]
    return module


_registry_module = _load_registry_module()
DocIDRegistry = _registry_module.DocIDRegistry  # type: ignore[attr-defined]


# --- Inventory model --------------------------------------------------------


@dataclass
class InventoryEntry:
    path: str
    doc_id: Optional[str]
    status: str
    file_type: str
    last_modified: str = ""
    scanned_at: str = ""

    @classmethod
    def from_dict(cls, d: Dict) -> "InventoryEntry":
        return cls(
            path=d["path"],
            doc_id=d.get("doc_id"),
            status=d.get("status", "missing"),
            file_type=d.get("file_type", "unknown"),
            last_modified=d.get("last_modified", ""),
            scanned_at=d.get("scanned_at", ""),
        )


def load_inventory(path: Path = INVENTORY_PATH) -> List[InventoryEntry]:
    """Load docs_inventory.jsonl into memory."""
    if not path.exists():
        print(f"[ERROR] Inventory file not found: {path}", file=sys.stderr)
        print("        Run: python scripts/doc_id_scanner.py scan", file=sys.stderr)
        sys.exit(1)

    entries: List[InventoryEntry] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            data = json.loads(line)
            entries.append(InventoryEntry.from_dict(data))

    return entries


# --- Inference helpers ------------------------------------------------------


def infer_category(path: str, available_categories: List[str]) -> str:
    """
    Infer registry category from file path.

    This is heuristic and intentionally simple; it favors existing categories
    and falls back to 'legacy' or the first available category if needed.
    """
    normalized = path.replace("\\", "/")

    heuristics: List[Tuple[str, str]] = [
        ("patterns", "/patterns/"),
        ("core", "/core/"),
        ("error", "/error/"),
        ("spec", "/schema/"),
        ("spec", "/spec/"),
        ("config", "/config/"),
        ("script", "/scripts/"),
        ("test", "/tests/"),
        ("guide", "/docs/"),
        ("aim", "/aim/"),
        ("pm", "/pm/"),
        ("guide", ".md"),
        ("script", ".ps1"),
        ("script", ".sh"),
        ("config", ".yaml"),
        ("config", ".yml"),
        ("config", ".json"),
    ]

    candidates: List[str] = []
    for candidate, marker in heuristics:
        if marker in normalized and candidate in available_categories:
            candidates.append(candidate)

    if candidates:
        return candidates[0]

    # Prefer docs/legacy if they exist
    for fallback in ("docs", "legacy"):
        if fallback in available_categories:
            return fallback

    # Absolute fallback: first category defined in registry
    if available_categories:
        return available_categories[0]

    print("[ERROR] No categories available in DOC_ID_REGISTRY.yaml", file=sys.stderr)
    sys.exit(1)


def infer_name_and_title(path: str, file_type: str) -> Tuple[str, str]:
    """
    Infer logical 'name' and human-readable 'title' for the registry.

    name: short machine-friendly identifier (used in doc_id)
    title: human-readable description shown in registry
    """
    rel = Path(path)
    stem = rel.stem
    parent = rel.parent.name

    if file_type == "py":
        if stem.startswith("test_"):
            name = f"{parent}-{stem}".replace("_", "-")
            title = f"Tests for {parent}.{stem.replace('test_', '')}"
        else:
            name = f"{parent}-{stem}".replace("_", "-")
            title = f"{parent} module: {stem}"
    elif file_type in ("ps1", "sh"):
        name = stem.replace("_", "-")
        title = f"Script: {stem}"
    elif file_type in ("yaml", "yml"):
        name = stem.replace("_", "-")
        title = f"Config: {stem}"
    elif file_type == "json":
        name = stem.replace("_", "-")
        title = f"JSON spec: {stem}"
    elif file_type == "md":
        name = stem.replace("_", "-")
        title = stem.replace("-", " ").title()
    else:
        name = stem.replace("_", "-")
        title = stem

    return name, title


# --- Injection helpers ------------------------------------------------------


def inject_doc_id_into_content(content: str, file_type: str, doc_id: str) -> str:
    """
    Inject doc_id into file content based on type.

    Simple and idempotent: if the doc_id is already present, content is
    returned unchanged.
    """
    if doc_id in content:
        return content

    # Python: module docstring or header comment
    if file_type == "py":
        lines = content.splitlines()
        new_lines: List[str] = []

        idx = 0
        # Preserve shebang
        if lines and lines[0].startswith("#!"):
            new_lines.append(lines[0])
            idx = 1

        inserted = False

        # Look for a top-level docstring
        if idx < len(lines) and (
            lines[idx].lstrip().startswith('"""') or lines[idx].lstrip().startswith("'''")
        ):
            quote = lines[idx].lstrip()[:3]
            new_lines.append(lines[idx])
            i = idx + 1
            while i < len(lines):
                new_lines.append(lines[i])
                if lines[i].rstrip().endswith(quote):
                    new_lines.append(f"DOC_ID: {doc_id}")
                    inserted = True
                    i += 1
                    break
                i += 1
            new_lines.extend(lines[i:])
        else:
            # No obvious docstring – insert comment near top
            new_lines.append(f"# DOC_LINK: {doc_id}")
            new_lines.extend(lines[idx:])
            inserted = True
            if idx > 0:
                # We already added shebang above if it existed
                pass

        if not inserted:
            new_lines.append(f"# DOC_LINK: {doc_id}")

        result = "\n".join(new_lines)
        if content.endswith("\n"):
            result += "\n"
        return result

    # Markdown: YAML frontmatter
    if file_type == "md":
        if content.startswith("---\n"):
            lines = content.splitlines()
            end_idx = None
            for i in range(1, len(lines)):
                if lines[i].strip() == "---":
                    end_idx = i
                    break
            if end_idx is not None:
                fm = lines[1:end_idx]
                if any(l.strip().startswith("doc_id:") for l in fm):
                    return content
                new_fm = ["doc_id: " + doc_id] + fm
                new_lines = ["---", *new_fm, "---", *lines[end_idx + 1 :]]
                result = "\n".join(new_lines)
                if content.endswith("\n"):
                    result += "\n"
                return result
        # No frontmatter
        fm = f"---\ndoc_id: {doc_id}\n---\n\n"
        return fm + content

    # YAML
    if file_type in ("yaml", "yml"):
        lines = content.splitlines()
        if any(l.strip().startswith("doc_id:") for l in lines[:20]):
            return content
        result = "doc_id: " + doc_id + "\n" + content
        return result

    # JSON
    if file_type == "json":
        try:
            data = json.loads(content)
            if isinstance(data, dict):
                if "doc_id" in data:
                    return content
                data["doc_id"] = doc_id
                return json.dumps(data, indent=2) + "\n"
        except json.JSONDecodeError:
            # Fall back to a header comment
            pass
        return f"/* DOC_ID: {doc_id} */\n" + content

    # PowerShell / Shell
    if file_type in ("ps1", "sh"):
        lines = content.splitlines()
        new_lines: List[str] = []
        idx = 0
        if lines and lines[0].startswith("#!"):
            new_lines.append(lines[0])
            idx = 1
        new_lines.append(f"# DOC_LINK: {doc_id}")
        new_lines.extend(lines[idx:])
        result = "\n".join(new_lines)
        if content.endswith("\n"):
            result += "\n"
        return result

    # TXT: treat like light markdown
    if file_type == "txt":
        if content.startswith("---\n"):
            lines = content.splitlines()
            end_idx = None
            for i in range(1, len(lines)):
                if lines[i].strip() == "---":
                    end_idx = i
                    break
            if end_idx is not None:
                fm = lines[1:end_idx]
                if any(l.strip().startswith("doc_id:") for l in fm):
                    return content
                new_fm = ["doc_id: " + doc_id] + fm
                new_lines = ["---", *new_fm, "---", *lines[end_idx + 1 :]]
                result = "\n".join(new_lines)
                if content.endswith("\n"):
                    result += "\n"
                return result
        fm = f"---\ndoc_id: {doc_id}\n---\n\n"
        return fm + content

    # Unknown / other: leave unchanged
    return content


# --- Assignment core --------------------------------------------------------


@dataclass
class AssignmentResult:
    path: str
    doc_id: str
    category: str
    name: str
    skipped: bool
    reason: Optional[str] = None


def auto_assign(
    dry_run: bool = True,
    limit: Optional[int] = None,
    include_types: Optional[List[str]] = None,
) -> Dict:
    """
    Assign doc_ids to all inventory entries marked as 'missing'.

    Returns a dict with summary + per-file assignment details.
    """
    inventory = load_inventory()
    registry = DocIDRegistry()
    available_categories = list(registry.data["categories"].keys())

    missing = [e for e in inventory if e.status == "missing"]
    total_missing = len(missing)

    if include_types:
        include_set = set(include_types)
        missing = [e for e in missing if e.file_type in include_set]

    if limit is not None:
        missing = missing[:limit]

    assignments: List[AssignmentResult] = []
    skipped: List[AssignmentResult] = []

    for idx, entry in enumerate(missing, start=1):
        rel_path = entry.path
        full_path = REPO_ROOT / rel_path

        if not full_path.exists():
            skipped.append(
                AssignmentResult(
                    path=rel_path,
                    doc_id="",
                    category="",
                    name="",
                    skipped=True,
                    reason="File does not exist",
                )
            )
            continue

        category = infer_category(rel_path, available_categories)
        name, title = infer_name_and_title(rel_path, entry.file_type)

        if dry_run:
            # We just preview what *would* happen
            preview_id = f"DOC-{category.upper()}-{name.upper().replace('_', '-')}-XXX"
            assignments.append(
                AssignmentResult(
                    path=rel_path,
                    doc_id=preview_id,
                    category=category,
                    name=name,
                    skipped=False,
                )
            )
        else:
            artifacts = [{"type": "source", "path": rel_path}]
            new_doc_id = registry.mint_doc_id(
                category=category,
                name=name,
                title=title,
                artifacts=artifacts,
                tags=[entry.file_type],
            )

            content = full_path.read_text(encoding="utf-8", errors="ignore")
            new_content = inject_doc_id_into_content(content, entry.file_type, new_doc_id)
            full_path.write_text(new_content, encoding="utf-8")

            assignments.append(
                AssignmentResult(
                    path=rel_path,
                    doc_id=new_doc_id,
                    category=category,
                    name=name,
                    skipped=False,
                )
            )

        if idx % 50 == 0:
            print(f"[INFO] Processed {idx}/{len(missing)} files...")

    summary = {
        "total_missing_in_inventory": total_missing,
        "processed": len(missing),
        "assigned": len([a for a in assignments if not a.skipped]),
        "skipped": len(skipped),
        "dry_run": dry_run,
        "timestamp": datetime.now().isoformat(),
    }

    return {
        "summary": summary,
        "assignments": [asdict(a) for a in assignments],
        "skipped": [asdict(s) for s in skipped],
    }


# --- CLI --------------------------------------------------------------------


def main() -> int:
    parser = argparse.ArgumentParser(
        description="Auto-assign doc_ids to files based on docs_inventory.jsonl"
    )
    subparsers = parser.add_subparsers(dest="command")

    assign_parser = subparsers.add_parser(
        "auto-assign",
        help="Assign doc_ids to files that are missing them",
    )
    assign_parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview assignments without modifying files or registry",
    )
    assign_parser.add_argument(
        "--limit",
        type=int,
        help="Limit the number of files processed (for testing)",
    )
    assign_parser.add_argument(
        "--types",
        nargs="+",
        help="Limit to specific file types (e.g. py md yaml json ps1 sh txt)",
    )
    assign_parser.add_argument(
        "--report",
        type=Path,
        help="Optional JSON file to write a detailed report",
    )

    args = parser.parse_args()

    if args.command == "auto-assign":
        result = auto_assign(
            dry_run=args.dry_run,
            limit=args.limit,
            include_types=args.types,
        )
        summary = result["summary"]

        print("\n=== DOC_ID AUTO-ASSIGN REPORT ===")
        print(f"Total missing in inventory: {summary['total_missing_in_inventory']}")
        print(f"Processed in this run:      {summary['processed']}")
        print(f"Assigned:                   {summary['assigned']}")
        print(f"Skipped:                    {summary['skipped']}")
        print(f"Dry run:                    {summary['dry_run']}")
        print(f"Timestamp:                  {summary['timestamp']}")

        if args.report:
            args.report.write_text(json.dumps(result, indent=2), encoding="utf-8")
            print(f"\n[OK] Detailed report written to {args.report}")

        # Non-zero exit if anything was skipped so you see it in CI if you choose
        return 0 if summary["skipped"] == 0 else 1

    parser.print_help()
    return 1


if __name__ == "__main__":
    raise SystemExit(main())
```

---

## 3. Suggested first run sequence

From a **clean** working tree:

```bash
# 0. Make sure everything is committed first
git status

# 1. Refresh the inventory and coverage
python scripts/doc_id_scanner.py scan
python scripts/doc_id_scanner.py stats

# 2. Dry-run on a small subset to eyeball behavior
python scripts/doc_id_assigner.py auto-assign --dry-run --limit 25 --types py md yaml json ps1 sh txt

git diff   # nothing should have changed (dry-run), just prints preview in stdout

# 3. Real run across all missing files
python scripts/doc_id_assigner.py auto-assign --types py md yaml json ps1 sh txt

# 4. Re-scan & validate
python scripts/doc_id_scanner.py scan
python scripts/doc_id_scanner.py stats
python doc_id/doc_id_registry_cli.py validate
python doc_id/doc_id_registry_cli.py stats

# 5. Commit
git add .
git commit -m "chore: Phase 0 doc_id auto-assign (coverage → 100%)"
```

That gets you to: **every eligible file has a `doc_id` embedded + a corresponding entry in `DOC_ID_REGISTRY.yaml`**, which is exactly the “seatbelt everywhere before refactor” target from the exploration docs. 

---


Short version: the categorical structure is the *shape* of the ID (`DOC-CORE-ORCHESTRATOR-001`), and the **key** is the thing the system uses as the *join key* everywhere (the full `doc_id`, and for patterns also `PAT-...`).

Let me walk it through using your actual files.

---

## 1. Categorical `doc_id` structure

From `DOC_ID_FRAMEWORK.md` and the registry CLI, every `doc_id` has this shape:

```text
DOC-<CATEGORY-PREFIX>-<NAME-SEGMENTS>-<NNN>
```

* `DOC` – fixed namespace for the doc registry
* `<CATEGORY-PREFIX>` – comes from the **category definition** (`PAT`, `CORE`, `ERROR`, `SPEC`, etc.)
* `<NAME-SEGMENTS>` – uppercase, dash-separated name (`ORCHESTRATOR`, `SAVE-FILE`, `QUALITY-GATE`)
* `<NNN>` – 3-digit sequence per category (`001–999`)

The **categories** are defined centrally in `DOC_ID_REGISTRY.yaml`:

```yaml
categories:
  patterns:
    prefix: PAT
    description: UET patterns in UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/patterns/
    count: 4
    next_id: 5
    index_file: UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/patterns/registry/PATTERN_INDEX.yaml
  core:
    prefix: CORE
    description: Core modules in core/
    ...
  script:
    prefix: SCRIPT
    description: Automation scripts in scripts/
    ...
```

So there are three closely related pieces:

1. **Category key (lowercase)** – e.g. `patterns`, `core`, `script`
2. **Category prefix (uppercase)** – e.g. `PAT`, `CORE`, `SCRIPT`
3. **doc_id string** – e.g. `DOC-CORE-ORCHESTRATOR-005`

The CLI uses (1) and (2) to build (3).

---

## 2. How the registry mints IDs (the “categorical” bit)

From `doc_id_registry_cli.py`, `mint_doc_id` does this:

```python
cat_data = self.data['categories'][category_lower]
prefix = cat_data['prefix']
next_num = cat_data['next_id']

name_upper = name.upper().replace('_', '-').replace(' ', '-')

doc_id = f"DOC-{prefix}-{name_upper}-{next_num:03d}"
```

So when you call:

```bash
python doc_id/doc_id_registry_cli.py mint \
  --category core \
  --name orchestrator \
  --title "Workstream Orchestrator"
```

it:

1. Looks up category `core` → prefix `CORE`, `next_id` (say `5`)
2. Normalizes name → `ORCHESTRATOR`
3. Builds `DOC-CORE-ORCHESTRATOR-005`
4. Appends a new entry under `docs:` in `DOC_ID_REGISTRY.yaml`
5. Increments `categories.core.next_id` to `6`

Each entry looks like:

```yaml
- doc_id: DOC-CORE-ORCHESTRATOR-005
  category: core
  name: orchestrator
  title: Workstream Orchestration Engine
  status: active
  artifacts:
    - type: source
      path: core/engine/orchestrator.py
    - type: doc
      path: core/engine/docs/orchestrator.md
  tags:
    - core
    - engine
    - high-priority
```

So:

* **Category key** (`core`) tells you *which bucket* it belongs to
* **Prefix** (`CORE`) determines the second segment of the `doc_id`
* **Sequence** (`005`) is tracked per category in `next_id`

That’s the categorical structure.

---

## 3. What is the “ID key” in practice?

There are really **two key flavors** in your system:

### 3.1 Primary ID key: the `doc_id`

This is the **canonical join key** across everything:

* `DOC_ID_REGISTRY.yaml` → `docs[].doc_id`
* `docs_inventory.jsonl` → `{"path": "...", "doc_id": "DOC-CORE-ORCHESTRATOR-005", ...}`
* File contents:

  * Markdown frontmatter: `doc_id: DOC-GUIDE-DOC-ID-FRAMEWORK-001`
  * Python docstring/header: `DOC_ID: DOC-CORE-ORCHESTRATOR-001` or `# DOC_LINK: ...`
  * YAML/JSON top-level: `doc_id: ...` / `"doc_id": "..."`

Everything is wired to treat `doc_id` as the **primary key** for “this logical documentation unit”.

### 3.2 Pattern / categorical key: `PAT-...-NNN`

For UET patterns you also have a **pattern key** (what you’ve sometimes called the categorical ID key):

* Pattern IDs look like: `PAT-MODULE-CREATION-001`, `PAT-WORKTREE-LIFECYCLE-001`, etc.
* `doc_id_mapping.json` maps these to the doc registry:

  ```json
  {
    "PAT-MODULE-CREATION-001": "DOC-PAT-MODULE-CREATION-001",
    "PAT-WORKTREE-LIFECYCLE-001": "DOC-PAT-WORKTREE-LIFECYCLE-001"
  }
  ```

Notice the structure:

* Pattern key: `PAT-MODULE-CREATION-001`
* Doc key:     `DOC-PAT-MODULE-CREATION-001`

So for patterns:

* The **categorical key** is effectively the `PAT-...-NNN` portion
* The **doc_id** just wraps that with the `DOC-` namespace in front

This gives you:

* Pattern registry talks in **pattern keys** (`PAT-...`)
* Doc registry + inventory talk in **doc_ids** (`DOC-PAT-...`)
* `doc_id_mapping.json` is the **bridge** between the two namespaces

Your auto-assigner I wrote earlier already assumes this model: for category `patterns`, it uses the registry’s `prefix: PAT` and mints `DOC-PAT-...-NNN`.

---

## 4. How it all works together in the pipeline

Putting it together:

1. **Category selection**

   * The scanner / auto-assigner infers a **category key** (`core`, `patterns`, `script`, `guide`, etc.) from the path.
   * CLI uses that to choose prefix (`CORE`, `PAT`, `SCRIPT`, …) and `next_id`.

2. **Key generation**

   * It builds a **name** from filename (`orchestrator.py` → `ORCHESTRATOR`, `save_file.pattern.yaml` → `SAVE-FILE`).
   * Minted `doc_id` is your **primary key**: `DOC-CORE-ORCHESTRATOR-005`.

3. **Registration**

   * Entry is added into `DOC_ID_REGISTRY.yaml` under the right **category bucket**, with all artifacts.
   * Category’s `next_id` increments so the next doc in that category gets `006`.

4. **Embedding**

   * That `doc_id` is injected into all related artifacts:

     * Specs, schemas, executors, scripts, tests, guides…

5. **Cross-registry mapping (patterns)**

   * For patterns, you *also* maintain `PAT-...-NNN` as the pattern-level key.
   * `doc_id_mapping.json` links `PAT-*` → `DOC-PAT-*`, so the UET layer and DOC layer can both treat IDs as first-class without guessing.

So when you say “add id to every file and register in the registry”:

* **Categorical structure** decides which bucket and prefix to use.
* **ID key** (`doc_id`, and for patterns also `PAT-*`) is what every tool uses as the stable join handle across registries, inventory, and file contents.

---

Here’s a complete draft you can drop in as **`doc_id/ID_KEY_CHEATSHEET.md`**:

````markdown
---
title: ID Key & Categorical Structure Cheatsheet
status: draft
audience: humans_and_ai
# NOTE: Run doc_id_registry_cli.py mint and replace this once registered
doc_id: DOC-TBD-ID-KEY-CHEATSHEET-001
---

# ID Key & Categorical Structure Cheatsheet

This cheatsheet explains **how IDs work in this repo**:

- The **primary key**: `doc_id`
- The **categorical structure**: category + prefix + name + sequence
- The relationship between **`doc_id`** and **domain IDs** like `pattern_id`
- Where and how IDs MUST be embedded in files

Use this whenever you mint new IDs, wire tools, or reason about registry / inventory state.

---

## 1. Quick Glossary

- **`doc_id`**  
  The **single primary identifier** that links all artifacts (docs, code, configs, tests, patterns).  
  Example: `DOC-CORE-STATE-DB-001`

- **Category (registry category key)**  
  Lowercase bucket in `DOC_ID_REGISTRY.yaml` that determines **where** something lives conceptually.  
  Example: `core`, `patterns`, `script`, `test`, `guide`.

- **Category prefix**  
  Uppercase token used inside the `doc_id`, defined per category.  
  Example: `core → CORE`, `patterns → PAT`.

- **Domain IDs (e.g., `pattern_id`)**  
  Domain-specific identifiers (like `PAT-ATOMIC-CREATE-001`) used inside UET or other subsystems.  
  They are **secondary IDs** and MUST NOT replace `doc_id` as the cross-artifact join key.

- **Registry** (`DOC_ID_REGISTRY.yaml`)  
  The **source of truth** that stores:
  - All categories and their prefixes
  - All registered `doc_id` entries
  - Artifact paths + tags for each ID

- **Inventory** (`docs_inventory.jsonl`)  
  A **scan of real files** in the repo and whether they have a `doc_id` embedded.  
  Used to measure coverage and drive auto-assignment.

---

## 2. `doc_id` – Primary Key (Single Linking ID)

### 2.1 Role

- `doc_id` is the **only cross-artifact join key** for this system.
- Every logical “documentation unit” (spec, pattern, module, script, guide, etc.) **MUST** have exactly one `doc_id`.
- All artifacts that belong to that unit **MUST** embed the same `doc_id`.

> **Rule (MUST):** If two files describe the same logical thing (e.g., `orchestrator.py`, its spec, and tests), they **MUST** share one `doc_id`.

### 2.2 Format

There are two active layers of spec:

1. **Base ID format** (from `ID_SYSTEM_SPEC`):

   ```text
   [A-Z0-9]+(-[A-Z0-9]+)*
````

2. **Concrete repo format** (from `DOC_ID_FRAMEWORK` + registry):

   ```text
   DOC-<CATEGORY_PREFIX>-<NAME_SEGMENTS>-<NNN>
   ```

Where:

* `DOC` – fixed namespace for doc registry
* `<CATEGORY_PREFIX>` – one of the prefixes from the **categories table** (below)
* `<NAME_SEGMENTS>` – uppercase, dash-separated name (e.g., `STATE-DB`, `SAVE-FILE`)
* `<NNN>` – 3-digit sequence **per category** (`001–999`), managed by the registry

---

## 3. Categories & Prefixes (Categorical Structure)

Categories live in `DOC_ID_REGISTRY.yaml` under `categories:`.

Each category defines:

* A **key** (lowercase) – used by tools / CLI
* A **prefix** (uppercase) – used in the `doc_id`
* A **description**
* An `index_file` for category-specific lists

### 3.1 Current categories (summary)

> **Source of truth:** `DOC_ID_REGISTRY.yaml → categories`

| Category key | Prefix   | Typical scope / directory                                              | Example `doc_id`                     |
| ------------ | -------- | ---------------------------------------------------------------------- | ------------------------------------ |
| `patterns`   | `PAT`    | UET patterns under `UNIVERSAL_EXECUTION_TEMPLATES_FRAMEWORK/patterns/` | `DOC-PAT-SAVE-FILE-001`              |
| `core`       | `CORE`   | Core modules in `core/`                                                | `DOC-CORE-STATE-DB-001`              |
| `error`      | `ERROR`  | Error system modules and plugins in `error/`                           | `DOC-ERROR-HANDLER-001`              |
| `spec`       | `SPEC`   | Specs & schemas in `specifications/`                                   | `DOC-SPEC-WORKFLOW-SCHEMA-001`       |
| `arch`       | `ARCH`   | ADRs & design docs in `adr/`                                           | `DOC-ARCH-ADR-010-ULID-IDENTITY-001` |
| `aim`        | `AIM`    | AIM env manager modules in `aim/`                                      | `DOC-AIM-PROFILE-LOADER-001`         |
| `pm`         | `PM`     | Project management docs / plans                                        | `DOC-PM-PHASE-PLAN-001`              |
| `infra`      | `INFRA`  | Infra / deployment in `infra/`                                         | `DOC-INFRA-PIPELINE-SETUP-001`       |
| `config`     | `CONFIG` | Configuration files in `config/`                                       | `DOC-CONFIG-QUALITY-GATE-001`        |
| `script`     | `SCRIPT` | Automation scripts in `scripts/`                                       | `DOC-SCRIPT-DOC-ID-REGISTRY-CLI-001` |
| `test`       | `TEST`   | Test suites in `tests/`                                                | `DOC-TEST-CORE-ENGINE-001`           |
| `guide`      | `GUIDE`  | User-facing docs in `docs/`                                            | `DOC-GUIDE-DOC-ID-FRAMEWORK-001`     |

> **Rule (MUST):** Every `doc_id` **MUST** belong to exactly one category in `DOC_ID_REGISTRY.yaml`.

> **Rule (SHOULD):** New files **SHOULD** pick the category that matches where the file lives in the repo and how it’s used (code vs spec vs script vs guide).

---

## 4. How IDs Are Minted (Categorical Minting Logic)

Minting is handled by `DocIDRegistry` in `doc_id/doc_id_registry_cli.py`.

### 4.1 Inputs to mint

When you call `mint`, you provide:

* `category` – **category key** (e.g., `core`, `patterns`)
* `name` – name segment(s) (e.g., `state_db`, `save_file`)
* `title` – human-readable title
* `artifacts` – list of `{type, path}` entries
* `tags` – free-form tags (e.g., `["core", "db"]`)

### 4.2 Minting steps (simplified)

1. Lookup category:

   ```python
   cat_data = self.data["categories"][category_lower]
   prefix = cat_data["prefix"]        # e.g., "CORE"
   next_num = cat_data["next_id"]     # e.g., 5
   ```

2. Normalize name:

   ```python
   name_upper = name.upper().replace("_", "-").replace(" ", "-")
   # "state_db" → "STATE-DB"
   ```

3. Construct `doc_id`:

   ```python
   doc_id = f"DOC-{prefix}-{name_upper}-{next_num:03d}"
   # "DOC-CORE-STATE-DB-005"
   ```

4. Validate format + duplicates, append to `docs:` list, and bump:

   ```yaml
   categories:
     core:
       next_id: 6  # incremented

   docs:
     - doc_id: DOC-CORE-STATE-DB-005
       category: core
       name: state_db
       title: Database Initialization and Connection Management
       artifacts:
         - type: source
           path: core/state/db.py
         - type: doc
           path: core/state/db.md
       tags:
         - core
         - db
   ```

> **Rule (MUST):** Only the registry’s `mint` logic may generate new `doc_id` values. IDs MUST NOT be hand-crafted.

---

## 5. Domain IDs vs `doc_id` (Patterns & Mapping)

### 5.1 Pattern IDs (`pattern_id` / `PAT-*`)

For UET patterns you also have a **domain-level ID** like:

* `pattern_id: PAT-SAVE-FILE-001`
* `pattern_id: PAT-ATOMIC-CREATE-001`
* Legacy “migrated” forms like `PAT-MIGRATED-CORE-010`

These live primarily in the **pattern registry** and pattern specs.

### 5.2 Mapping file (`doc_id_mapping.json`)

To keep the system coherent, you have:

```json
{
  "PAT-ATOMIC-CREATE-001": "DOC-ATOMIC-CREATE-001",
  "PAT-REFACTOR-PATCH-001": "DOC-REFACTOR-PATCH-001"
}
```

This means:

* **Domain key:** `PAT-ATOMIC-CREATE-001`
* **Doc key:** `DOC-ATOMIC-CREATE-001` (or `DOC-PAT-ATOMIC-CREATE-001` in the fully category-prefixed world)

> **Rule (MUST):** Domain-specific IDs like `PAT-*` MAY exist and SHOULD be kept, but `doc_id` remains the **primary join key** for cross-artifact linkage and validation.

> **Rule (SHOULD):** For new work, keep **pattern IDs** and **doc IDs** aligned in naming to reduce cognitive load (e.g., `PAT-SAVE-FILE-001` ↔ `DOC-PAT-SAVE-FILE-001`).

---

## 6. Where IDs Live in Files (Embedding Rules)

These rules are already defined in the framework; this is the **cheatsheet view**:

| File type    | Placement                          | Example                                              |
| ------------ | ---------------------------------- | ---------------------------------------------------- |
| Markdown     | YAML frontmatter                   | `doc_id: DOC-GUIDE-DOC-ID-FRAMEWORK-001`             |
| Python       | Module docstring or header comment | `DOC_ID: DOC-CORE-ORCHESTRATOR-001` or `# DOC_LINK:` |
| YAML         | Top-level field                    | `doc_id: DOC-CONFIG-QUALITY-GATE-001`                |
| JSON         | Top-level field                    | `"doc_id": "DOC-SPEC-WORKSTREAM-SCHEMA-001"`         |
| PowerShell   | Header comment                     | `# DOC_LINK: DOC-SCRIPT-VALIDATE-001`                |
| Shell script | Header comment                     | `# DOC_LINK: DOC-SCRIPT-DEPLOY-PIPELINE-001`         |
| Text         | Optional frontmatter               | `doc_id: DOC-GUIDE-NOTES-001`                        |

Your scanner + auto-assigner (`doc_id_scanner.py`, `doc_id_assigner.py`) enforce this:

* Scanner reads **from files → inventory**
* Registry and auto-assigner write **from inventory → registry + files**

---

## 7. Practical Rules for Humans & AI

### 7.1 When creating a new unit

1. Decide **what it is**:

   * Core module, config, script, guide, pattern, test, etc.

2. Pick the right **category key** from the table (e.g., `core`, `patterns`, `script`).

3. Pick a **name**:

   * Short, stable noun phrase: `state_db`, `save_file`, `doc_id_preflight`.

4. Call the registry:

   ```bash
   python doc_id/doc_id_registry_cli.py mint \
     --category core \
     --name state_db \
     --title "Database Initialization and Connection Management"
   ```

5. Embed the `doc_id` in:

   * All related spec/doc files
   * Code files
   * Configs/tests

### 7.2 When wiring tools or prompts

* **Always pass `doc_id`** (not just filenames) when describing artifacts.
* For patterns, include both:

  * `pattern_id`: `PAT-ATOMIC-CREATE-001`
  * `doc_id`: `DOC-PAT-ATOMIC-CREATE-001` (or mapped legacy form)
* Use `category` + `prefix` if you need to **mint**, **search**, or **filter**:

  * Filter core modules: `category == "core"`
  * Filter scripts: `category == "script"`

### 7.3 Do / Don’t

* ✅ **Do** treat `doc_id` as the primary key everywhere.

* ✅ **Do** use categories to keep `doc_id` space organized and human-readable.

* ✅ **Do** keep pattern IDs and doc IDs aligned and mapped.

* ❌ **Don’t** hand-craft `doc_id` strings. Always use `mint`.

* ❌ **Don’t** use `pattern_id` or filenames as the primary join key across systems.

* ❌ **Don’t** create multiple `doc_id` values for the same logical unit.

---

## 8. TL;DR for Agents

> * Use **`doc_id`** as your **only** cross-file join key.
> * Use **categories** (core / patterns / script / config / guide / etc.) to pick the right namespace.
> * Use the registry to **mint**, and ensure file contents + registry + inventory stay in sync.
> * For patterns, treat `pattern_id` as a **domain label** and `doc_id` as the **canonical key**.

```


