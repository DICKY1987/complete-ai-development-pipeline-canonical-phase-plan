
---

> `TARGET_DIR = "C:\Users\richg\ALL_AI\Complete AI Development Pipeline – Canonical Phase Plan\glossary"`
>
> You are a senior **software architect, automation engineer, and codebase auditor**.
> Your mission is to perform a **complete, direct, and deeply detailed analysis** of the repository at `TARGET_DIR`.
>
> You must:
>
> * Reconstruct the **end-to-end automation chain** (build, test, deploy, data, ops, CLI workflows).
> * Identify **all automation gaps and chain breaks**.
> * Detect **overlapping implementations, deprecated code, unused/dead code, and redundant configuration**.
> * Surface **all other issues**: correctness, security, performance, maintainability, testing, documentation, DevOps.
> * Provide **specific, actionable recommendations** with clear ROI and implementation steps.
>
> Do **not** be high-level or generic. Go as deep and concrete as possible, always tying claims to code, structure, or config.

---

### 1. First: Understand the Repository as a Whole

1. Examine `TARGET_DIR` to understand:

> * Overall purpose and main responsibilities
> * Key services/modules and how they interact
> * Main entry points (CLIs, APIs, jobs, workflows)
> * Tech stack and major dependencies

2. Summarize in a short **High-Level Overview** before going into details.

---

### 2. Automation Chain & Gap Analysis

Reconstruct the automation chain(s) across **Build, Test, Deploy, Data/ETL, Ops, and CLI workflows**.

#### 2.1 Build a Chain Map

Treat each of these as a **step/node**:

* CLI commands / scripts (e.g., `scripts/`, `bin/`, `cli.*`, `.ps1`, `.sh`)
* CI jobs and workflow steps (`.github/workflows/`, other pipeline configs)
* Schedulers/cron, file/state watchers, ETL/data jobs
* Tests/validation, deployment steps, monitoring/alert pipelines

For each step, assign:

* `Step ID: STEP-XXX`
* `automation_class: [FULLY_AUTOMATED | SEMI_MANUAL | MANUAL]`
* `chain_role: [ENTRY_POINT | INTERNAL_STEP | TERMINAL_STEP]`
* `trigger: [CI | cron | file_watcher | webhook | CLI_manual | other]`
* `state_integration: [central_state | logs_only | none]`
* `error_handling: [retry+escalation | log_only | none]`

Then map **edges/hand-offs**: `STEP-AAA → STEP-BBB` with trigger and notes.

#### 2.2 Mark Automation Chain Breaks

A **chain break** exists when:

* The next step requires a human to start it, approve it, or manually move outputs.
* Outputs are not machine-consumed (only docs/README instructions).
* A CLI is run interactively instead of via a standard orchestrator/wrapper.
* Failures/timeouts don’t propagate to central state/logging/monitoring.
* There is no automated retry/error pipeline.

For each break, record:

```text
Chain Break ID: BREAK-XXX
From Step: STEP-AAA
To Step: STEP-BBB
Break Type: [Manual Start | Manual Approval | Missing Handoff | No Error Propagation | Patternless CLI Use]
Description: [Short explanation]
```

#### 2.3 Automation Gaps per Step

Scan code, scripts, configs, and docs for:

* **Manual processes** (prompts, TTY assumptions, “run X after Y”, “verify by eye”)
* **Repetitive patterns** (same sequences manually repeated)
* **Missing validations & workflows** (no pre-commit/CI gates, no error pipelines, no monitoring)
* **Incomplete workflows** (start automated, end manual; logs never read; state never consumed)
* **Error-prone operations** (manual data transforms, config edits, releases)

For each gap or break, evaluate:

* Frequency (Daily/Weekly/Monthly/Rare)
* Time cost (person-hours per execution)
* Error risk (High/Medium/Low)
* Complexity (manual steps count & description)
* Automation feasibility (Trivial/Moderate/Complex)
* ROI = (Time saved × Frequency) − Implementation cost
* Chain impact (critical vs secondary pipeline)
* Pattern compliance (uses standard orchestrator/state/logging vs patternless ad-hoc)

Document each as:

```text
Gap ID: GAP-XXX
Chain Break ID(s): [BREAK-YYY, BREAK-ZZZ]    # if applicable
Location: [file path(s) or process name]
Pipeline: [Build | Test | Deploy | Data | Ops | Other]
Type: [Manual Workflow | Chain Break | Repetitive Code | Missing Validation | Incomplete Automation | Patternless Execution]

Current State:
  [What exists today, including how it’s run]

Problem:
  [Why this is inefficient, fragile, or risky]

Impact:
  [Time, risk, quality, and which pipeline(s) are affected]

Evidence:
  - [Code snippets, file paths, log examples, docs]
```

---

### 3. Overlap, Deprecated Code, and Dead Code

Use a second pass to detect **overlaps and deprecations**:

#### 3.1 Overlapping Implementations

Look for:

* Duplicate or near-duplicate functions and helpers
* Parallel modules providing same behavior (legacy vs new, v1 vs v2)
* Redundant configuration (duplicate/conflicting settings, env-specific copies)

Group into **Overlap Groups**:

```text
Overlap Group: OG-XXX

Implementations:
1. [Location 1] - [Description] - [Status: Active/Deprecated/Unknown]
   - LOC: XXX
   - First commit: YYYY-MM-DD
   - Last modified: YYYY-MM-DD
   - Call sites: XXX

2. [Location 2] - [Description] - [Status: Active/Deprecated/Unknown]
   - LOC: XXX
   - First commit: YYYY-MM-DD
   - Last modified: YYYY-MM-DD
   - Call sites: XXX

Recommended Keeper: [Implementation #X]
Reason: [Why – more complete, better tested, actively maintained, etc.]

Items to Deprecate/Remove: [Implementation #Y, #Z]
Reason: [Why – older, less complete, untested, etc.]
```

#### 3.2 Deprecation & Dead Code

Find both **explicit** and **implicit** deprecation:

* Explicit markers: `@deprecated`, “DEPRECATED”, “TODO: remove”, warnings, docs saying “use X instead”.
* Implicit:

  * Unused functions/classes (no call sites/imports)
  * Modules never imported
  * Old branches/modules left after migrations
  * Files untouched for long periods with newer alternatives
  * Docs/ADRs/changelogs marking things as deprecated

For each finding:

```text
Finding ID: [OVLP-XXX | DEPR-XXX | DEAD-XXX]
Type: [Duplicate Function | Overlapping Module | Deprecated API | Unused Code | Dead Code | Redundant Config]

Current Usage:
  - Active call sites: [count + locations]
  - Import references: [count + locations]
  - Test coverage: [count + locations]
  - External dependencies: [other code depending on this]

Risk Level: [Critical | High | Medium | Low | Safe]
```

---

### 4. General Code & System Issues

While doing all of the above, also flag **non-automation issues** that matter:

* **Correctness & Bugs** – logic errors, bad edge-case handling, missing error handling.
* **Security** – secrets in repo, injection risk, insecure defaults, missing auth/permissions checks.
* **Performance & Scalability** – obvious inefficiencies, N+1 queries, unbounded loops.
* **Maintainability** – confusing structure, duplication, magic numbers, poor naming.
* **Testing & Reliability** – missing tests on critical paths, flaky/brittle tests, CI gaps.
* **Documentation & Developer Experience** – setup friction, missing/incorrect docs, manual runbooks.

For each issue, use:

```text
Category: [Correctness | Security | Performance | Maintainability | Testing | Docs | DevOps | Other]
Severity: [High | Medium | Low]
Location: [file path + function/section]
Issue:
  [Clear description]

Impact:
  [What can go wrong, in what scenarios]

Recommended Fix:
  [Concrete changes or refactor, not generic advice]
```

---

### 5. Output Structure

Organize your final answer into these sections:

1. **High-Level Overview**

   * What the repo does, main components, main pipelines.

2. **Automation Chain Map**

   * Node list: `STEP-IDs` with `automation_class`, `trigger`, `chain_role`.
   * Edge list: `STEP-AAA → STEP-BBB` with any `BREAK-IDs`.

3. **Automation Gaps & Chain Breaks (Priority-Sorted)**

   * Summary table/list of all `GAP-XXX` with `Type`, `Priority`, `Pipeline`, `Time Savings`, `Effort`, `Chain Impact`.

4. **Overlap / Deprecated / Dead Code Findings**

   * Overlap Groups (`OG-XXX`), `OVLP-XXX`, `DEPR-XXX`, `DEAD-XXX` with risk and recommended keeper/removals.

5. **Other Technical Issues**

   * Group by category (Correctness, Security, Performance, etc.) with severity and fixes.

6. **Prioritized Action Plan & Roadmap**

   * **Phase 1 – Quick Wins (Week 1–2)**: low-effort, high-ROI fixes and safe deletions.
   * **Phase 2 – High Impact (Month 1)**: critical chain breaks and high-value consolidations.
   * **Phase 3 – Long-Term (Quarter)**: complex refactors and large workflow redesigns.

---

### 6. Style Requirements

* Be **direct, explicit, and code-referential** – avoid vague advice.
* Do **not invent** files, modules, or workflows that don’t exist; if you infer, mark it as an **assumption**.
* Prefer **depth over breadth**: better to analyze real pipelines/files thoroughly than stay generic.

---


